import { defineStore } from 'pinia'
import { p2lApi } from '@/utils/api'
import { requestRacer } from '@/utils/requestRacer'

export const useP2LStore = defineStore('p2l', {
  state: () => ({
    // ç³»ç»ŸçŠ¶æ€
    backendHealth: false,
    loading: false,
    
    // é…ç½®ç‰ˆæœ¬ï¼ˆç”¨äºæ£€æµ‹é…ç½®æ›´æ–°ï¼‰
    configVersion: '2.1.0', // æ›´æ–°ç‰ˆæœ¬å·ä»¥å¼ºåˆ¶åˆ·æ–°é…ç½® - æ”¯æŒæƒé‡æ’åº
    
    // P2Låˆ†æç»“æœ
    currentAnalysis: null,
    recommendations: [],
    
    // èŠå¤©å†å²
    chatHistory: [],
    
    // æ¨¡å‹ä¿¡æ¯ - ä»åç«¯APIåŠ¨æ€è·å–
    availableModels: [],
    
    // å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    enabledModels: [],
    
    // ä¼˜å…ˆæ¨¡å¼
    priorityMode: 'balanced'
  }),

  getters: {
    isBackendReady: (state) => state.backendHealth,
    
    getModelByName: (state) => (name) => {
      return state.availableModels.find(model => model.name === name)
    },
    
    // è¿‡æ»¤åçš„æ¨èç»“æœï¼ˆåªæ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹ï¼‰
    sortedRecommendations: (state) => {
      return [...state.recommendations]
        .filter(rec => state.enabledModels.includes(rec.model))
        .sort((a, b) => b.score - a.score)
    },
    
    // å¯ç”¨çš„æ¨¡å‹ä¿¡æ¯
    enabledModelInfos: (state) => {
      return state.availableModels.filter(model => 
        state.enabledModels.includes(model.name)
      )
    }
  },

  actions: {
    // æ£€æŸ¥åç«¯å¥åº·çŠ¶æ€ - ä½¿ç”¨ç«é€Ÿè¯·æ±‚
    async checkBackendHealth() {
      try {
        console.log('ğŸ¥ [Health Check] å¼€å§‹ç«é€Ÿå¥åº·æ£€æŸ¥...')
        const response = await requestRacer.raceHealthCheck()
        this.backendHealth = response.status === 200
        console.log('âœ… [Health Check] ç«é€Ÿå¥åº·æ£€æŸ¥æˆåŠŸ')
        return this.backendHealth
      } catch (error) {
        console.error('âŒ [Health Check] ç«é€Ÿå¥åº·æ£€æŸ¥å¤±è´¥:', error)
        this.backendHealth = false
        return false
      }
    },

    // ä»åç«¯è·å–æ¨¡å‹åˆ—è¡¨
    async loadModelsFromBackend() {
      try {
        const response = await p2lApi.get('/models')
        const backendData = response.data
        
        // å¤„ç†åç«¯è¿”å›çš„æ•°æ®æ ¼å¼ {models: [...], total: 42}
        const modelNames = backendData.models || []
        
        // è½¬æ¢æ¨¡å‹åç§°ä¸ºå‰ç«¯æ ¼å¼
        this.availableModels = modelNames.map(name => ({
          name,
          provider: this.getProviderDisplayName(this.getProviderFromModelName(name)),
          type: this.getModelType(name),
          cost: this.getCostLevel(this.estimateCostFromModelName(name)),
          speed: this.getSpeedLevel(this.estimateSpeedFromModelName(name)),
          hasApiKey: true
        }))
        
        console.log(`âœ… ä»åç«¯åŠ è½½äº† ${this.availableModels.length} ä¸ªæ¨¡å‹`)
        return this.availableModels
      } catch (error) {
        console.error('ä»åç«¯è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error)
        // å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çš„å‡ ä¸ªä¸»è¦æ¨¡å‹
        this.availableModels = [
          { name: 'gpt-4o', provider: 'OpenAI', type: 'GPT', cost: 'é«˜', speed: 'ä¸­', hasApiKey: true },
          { name: 'claude-3-5-sonnet-20241022', provider: 'Anthropic', type: 'Claude', cost: 'é«˜', speed: 'ä¸­', hasApiKey: true },
          { name: 'gemini-1.5-pro', provider: 'Google', type: 'Gemini', cost: 'ä¸­', speed: 'ä¸­', hasApiKey: true }
        ]
        return this.availableModels
      }
    },

    // è¾…åŠ©æ–¹æ³•ï¼šä»æ¨¡å‹åç§°æ¨æ–­æä¾›å•†
    getProviderFromModelName(modelName) {
      if (modelName.includes('gpt') || modelName.includes('o1')) return 'openai'
      if (modelName.includes('claude')) return 'anthropic'
      if (modelName.includes('gemini')) return 'google'
      if (modelName.includes('deepseek')) return 'deepseek'
      if (modelName.includes('qwen')) return 'qwen'
      if (modelName.includes('llama')) return 'meta'
      if (modelName.includes('yi')) return 'yi'
      return 'unknown'
    },

    // è¾…åŠ©æ–¹æ³•ï¼šè·å–æä¾›å•†æ˜¾ç¤ºåç§°
    getProviderDisplayName(provider) {
      const providerMap = {
        'openai': 'OpenAI',
        'anthropic': 'Anthropic', 
        'google': 'Google',
        'deepseek': 'DeepSeek',
        'qwen': 'åƒé—®',
        'meta': 'Meta',
        'yi': 'é›¶ä¸€ä¸‡ç‰©',
        'yinli': 'yinliä»£ç†',
        'probex': 'ProbeXä»£ç†',
        'unknown': 'æœªçŸ¥'
      }
      return providerMap[provider] || provider
    },

    // è¾…åŠ©æ–¹æ³•ï¼šä»æ¨¡å‹åç§°ä¼°ç®—æˆæœ¬
    estimateCostFromModelName(modelName) {
      if (modelName.includes('o1') || modelName.includes('gpt-4')) return 0.015
      if (modelName.includes('claude-3-opus')) return 0.015
      if (modelName.includes('claude') || modelName.includes('gemini-1.5-pro')) return 0.01
      if (modelName.includes('gpt-4o-mini') || modelName.includes('deepseek') || modelName.includes('qwen')) return 0.002
      if (modelName.includes('gemini-1.5-flash')) return 0.001
      return 0.005
    },

    // è¾…åŠ©æ–¹æ³•ï¼šä»æ¨¡å‹åç§°ä¼°ç®—é€Ÿåº¦
    estimateSpeedFromModelName(modelName) {
      if (modelName.includes('turbo') || modelName.includes('flash') || modelName.includes('mini')) return 0.8
      if (modelName.includes('deepseek') || modelName.includes('qwen')) return 1.0
      if (modelName.includes('o1') || modelName.includes('opus')) return 3.0
      return 1.5
    },

    // è¾…åŠ©æ–¹æ³•ï¼šè·å–æ¨¡å‹ç±»å‹
    getModelType(modelName) {
      if (modelName.includes('gpt')) return 'GPT'
      if (modelName.includes('claude')) return 'Claude'
      if (modelName.includes('gemini')) return 'Gemini'
      if (modelName.includes('deepseek')) return 'DeepSeek'
      if (modelName.includes('qwen')) return 'Qwen'
      return 'LLM'
    },

    // è¾…åŠ©æ–¹æ³•ï¼šè·å–æˆæœ¬ç­‰çº§
    getCostLevel(costPer1k) {
      if (costPer1k <= 0.001) return 'æä½'
      if (costPer1k <= 0.005) return 'ä½'
      if (costPer1k <= 0.015) return 'ä¸­'
      return 'é«˜'
    },

    // è¾…åŠ©æ–¹æ³•ï¼šè·å–é€Ÿåº¦ç­‰çº§
    getSpeedLevel(responseTime) {
      if (responseTime <= 1.0) return 'å¿«'
      if (responseTime <= 2.0) return 'ä¸­'
      return 'æ…¢'
    },

    // P2Læ™ºèƒ½åˆ†æ - ä½¿ç”¨ç«é€Ÿè¯·æ±‚æœºåˆ¶
    async analyzeWithP2L(prompt, mode = 'balanced') {
      this.loading = true
      
      try {
        console.log('ğŸ [P2L Store] å¼€å§‹ç«é€ŸP2Låˆ†æ:', { 
          prompt: prompt.substring(0, 50), 
          priority: mode 
        })
        
        const enabledModels = this.enabledModels.length > 0 
          ? this.enabledModels 
          : this.availableModels.map(m => m.name)
        
        // ä½¿ç”¨ç«é€Ÿè¯·æ±‚
        const response = await requestRacer.raceP2LAnalysis(prompt, mode, enabledModels)
        
        console.log('ğŸ† [P2L Store] ç«é€ŸP2Låˆ†ææˆåŠŸ:', {
          routing_info: response.data.routing_info,
          strategy: response.data.routing_info?.strategy
        })
        
        this.currentAnalysis = response.data
        this.recommendations = response.data.recommendations || []
        
        return response.data
      } catch (error) {
        console.error('âŒ [P2L Store] ç«é€ŸP2Låˆ†æå¤±è´¥:', error)
        
        // å¦‚æœç«é€Ÿè¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿé‡è¯•æœºåˆ¶
        if (error.allErrors) {
          console.log('ğŸ”„ [P2L Store] ç«é€Ÿå¤±è´¥ï¼Œå°è¯•ä¼ ç»Ÿé‡è¯•...')
          return this._fallbackAnalyzeWithP2L(prompt, mode)
        }
        
        throw new Error('P2Låˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•')
      } finally {
        this.loading = false
      }
    },

    // ä¼ ç»ŸP2Låˆ†æä½œä¸ºç«é€Ÿå¤±è´¥çš„å¤‡ç”¨æ–¹æ¡ˆ
    async _fallbackAnalyzeWithP2L(prompt, mode = 'balanced', retryCount = 0) {
      const maxRetries = 1
      
      try {
        console.log(`ğŸ”„ [P2L Fallback] ä¼ ç»Ÿé‡è¯• (${retryCount + 1}/${maxRetries + 1})`)
        
        const response = await p2lApi.post('/p2l/analyze', {
          prompt,
          priority: mode,
          enabled_models: this.enabledModels.length > 0 ? this.enabledModels : this.availableModels.map(m => m.name)
        })
        
        this.currentAnalysis = response.data
        this.recommendations = response.data.recommendations || []
        
        return response.data
      } catch (error) {
        if (retryCount < maxRetries) {
          await this.delay(3000)
          return this._fallbackAnalyzeWithP2L(prompt, mode, retryCount + 1)
        }
        throw error
      }
    },

    // é”™è¯¯åˆ†æå’Œåˆ†ç±»
    analyzeError(error) {
      // ç½‘ç»œè¶…æ—¶é”™è¯¯
      if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {
        return {
          canRetry: true,
          retryDelay: 3000,
          userMessage: 'P2Låˆ†æè¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•...'
        }
      }
      
      // ç½‘ç»œè¿æ¥é”™è¯¯
      if (error.code === 'ECONNREFUSED' || error.code === 'ENETUNREACH') {
        return {
          canRetry: true,
          retryDelay: 2000,
          userMessage: 'ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œæ­£åœ¨é‡è¯•...'
        }
      }
      
      // æœåŠ¡å™¨é”™è¯¯ (5xx)
      if (error.response?.status >= 500) {
        return {
          canRetry: true,
          retryDelay: 5000,
          userMessage: 'P2LæœåŠ¡æš‚æ—¶ç¹å¿™ï¼Œæ­£åœ¨é‡è¯•...'
        }
      }
      
      // è¯·æ±‚è¿‡äºé¢‘ç¹ (429)
      if (error.response?.status === 429) {
        return {
          canRetry: true,
          retryDelay: 10000,
          userMessage: 'è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•...'
        }
      }
      
      // å®¢æˆ·ç«¯é”™è¯¯ (4xx) - é€šå¸¸ä¸é‡è¯•
      if (error.response?.status >= 400 && error.response?.status < 500) {
        return {
          canRetry: false,
          retryDelay: 0,
          userMessage: error.response.data?.message || 'P2Låˆ†æè¯·æ±‚æœ‰è¯¯'
        }
      }
      
      // å…¶ä»–æœªçŸ¥é”™è¯¯
      return {
        canRetry: false,
        retryDelay: 0,
        userMessage: 'P2Låˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•'
      }
    },

    // å»¶è¿Ÿå·¥å…·å‡½æ•°
    delay(ms) {
      return new Promise(resolve => setTimeout(resolve, ms))
    },

    // è°ƒç”¨LLMç”Ÿæˆå›ç­” - ä½¿ç”¨ç«é€Ÿè¯·æ±‚æœºåˆ¶
    async generateWithLLM(model, prompt, conversationHistory = []) {
      this.loading = true
      
      try {
        console.log(`ğŸ [LLM Store] å¼€å§‹ç«é€ŸLLMè°ƒç”¨: ${model}`)
        
        // æ„å»ºæ¶ˆæ¯å†å²ï¼ŒåŒ…å«å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        const messages = []
        
        // æ·»åŠ å†å²å¯¹è¯è®°å½• - ä¿®å¤ç©ºå†…å®¹é—®é¢˜
        conversationHistory.forEach(item => {
          // åªæ·»åŠ éç©ºçš„ç”¨æˆ·æ¶ˆæ¯
          if (item.prompt && item.prompt.trim()) {
            messages.push({
              role: 'user',
              content: item.prompt.trim()
            })
          }
          // åªæ·»åŠ éç©ºçš„åŠ©æ‰‹å›å¤
          if (item.response && item.response.trim()) {
            messages.push({
              role: 'assistant', 
              content: item.response.trim()
            })
          }
        })
        
        // æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
        messages.push({
          role: 'user',
          content: prompt
        })
        
        // ä½¿ç”¨ç«é€Ÿè¯·æ±‚
        const response = await requestRacer.raceLLMGeneration(model, prompt, messages)
        
        // æ£€æŸ¥åç«¯æ˜¯å¦è¿”å›äº†é”™è¯¯çŠ¶æ€
        if (response.data.provider === 'error') {
          throw new Error(response.data.content || response.data.response || 'LLMæœåŠ¡è°ƒç”¨å¤±è´¥')
        }
        
        const result = {
          id: Date.now(),
          prompt,
          model,
          response: response.data.response || response.data.content || 'æš‚æ— å›å¤å†…å®¹',
          timestamp: new Date(),
          cost: response.data.cost || 0,
          tokens: response.data.tokens || response.data.tokens_used || 0,
          provider: response.data.provider || 'unknown',
          responseTime: response.data.response_time || 0,
          isRaceWinner: true  // æ ‡è®°ä¸ºç«é€Ÿè·èƒœè€…
        }
        
        this.chatHistory.push(result)
        console.log(`ğŸ† [LLM Store] ${model} ç«é€Ÿè°ƒç”¨æˆåŠŸï¼Œå“åº”æ—¶é—´: ${result.responseTime}s`)
        return result
      } catch (error) {
        console.error(`âŒ [LLM Store] ${model} ç«é€Ÿè°ƒç”¨å¤±è´¥:`, error)
        
        // å¦‚æœç«é€Ÿè¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿé‡è¯•æœºåˆ¶
        if (error.allErrors) {
          console.log(`ğŸ”„ [LLM Store] ${model} ç«é€Ÿå¤±è´¥ï¼Œå°è¯•ä¼ ç»Ÿé‡è¯•...`)
          return this._fallbackGenerateWithLLM(model, prompt, conversationHistory)
        }
        
        throw new Error(`${model} æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•`)
      } finally {
        this.loading = false
      }
    },

    // ä¼ ç»ŸLLMè°ƒç”¨ä½œä¸ºç«é€Ÿå¤±è´¥çš„å¤‡ç”¨æ–¹æ¡ˆ
    async _fallbackGenerateWithLLM(model, prompt, conversationHistory = [], retryCount = 0) {
      const maxRetries = 1
      
      try {
        console.log(`ğŸ”„ [LLM Fallback] ${model} ä¼ ç»Ÿé‡è¯• (${retryCount + 1}/${maxRetries + 1})`)
        
        // æ„å»ºæ¶ˆæ¯å†å²
        const messages = []
        conversationHistory.forEach(item => {
          if (item.prompt && item.prompt.trim()) {
            messages.push({ role: 'user', content: item.prompt.trim() })
          }
          if (item.response && item.response.trim()) {
            messages.push({ role: 'assistant', content: item.response.trim() })
          }
        })
        messages.push({ role: 'user', content: prompt })
        
        const response = await p2lApi.post('/llm/generate', {
          model,
          prompt,
          messages,
          max_tokens: 2000
        })
        
        if (response.data.provider === 'error') {
          throw new Error(response.data.content || response.data.response || 'LLMæœåŠ¡è°ƒç”¨å¤±è´¥')
        }
        
        const result = {
          id: Date.now(),
          prompt,
          model,
          response: response.data.response || response.data.content || 'æš‚æ— å›å¤å†…å®¹',
          timestamp: new Date(),
          cost: response.data.cost || 0,
          tokens: response.data.tokens || response.data.tokens_used || 0,
          provider: response.data.provider || 'unknown',
          responseTime: response.data.response_time || 0,
          isRaceWinner: false  // æ ‡è®°ä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        }
        
        this.chatHistory.push(result)
        return result
      } catch (error) {
        if (retryCount < maxRetries) {
          await this.delay(5000)
          return this._fallbackGenerateWithLLM(model, prompt, conversationHistory, retryCount + 1)
        }
        throw error
      }
    },

    // LLMé”™è¯¯åˆ†æå’Œåˆ†ç±»
    analyzeLLMError(error, model) {
      // ç½‘ç»œè¶…æ—¶é”™è¯¯
      if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {
        return {
          canRetry: true,
          retryDelay: 5000,
          userMessage: `${model} å“åº”è¶…æ—¶ï¼Œå¤æ‚é—®é¢˜éœ€è¦æ›´é•¿å¤„ç†æ—¶é—´ï¼Œæ­£åœ¨é‡è¯•...`
        }
      }
      
      // ç½‘ç»œè¿æ¥é”™è¯¯
      if (error.code === 'ECONNREFUSED' || error.code === 'ENETUNREACH') {
        return {
          canRetry: true,
          retryDelay: 3000,
          userMessage: `${model} ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œæ­£åœ¨é‡è¯•...`
        }
      }
      
      // APIé™åˆ¶é”™è¯¯ (429)
      if (error.response?.status === 429) {
        return {
          canRetry: true,
          retryDelay: 15000, // APIé™åˆ¶éœ€è¦æ›´é•¿ç­‰å¾…æ—¶é—´
          userMessage: `${model} è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œ15ç§’åé‡è¯•...`
        }
      }
      
      // æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ (5xx)
      if (error.response?.status >= 500) {
        return {
          canRetry: true,
          retryDelay: 8000,
          userMessage: `${model} æœåŠ¡æš‚æ—¶ç¹å¿™ï¼Œæ­£åœ¨é‡è¯•...`
        }
      }
      
      // APIå¯†é’¥é”™è¯¯ (401, 403)
      if (error.response?.status === 401 || error.response?.status === 403) {
        return {
          canRetry: false,
          retryDelay: 0,
          userMessage: `${model} APIå¯†é’¥æ— æ•ˆæˆ–æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥é…ç½®`
        }
      }
      
      // è¯·æ±‚æ ¼å¼é”™è¯¯ (400)
      if (error.response?.status === 400) {
        return {
          canRetry: false,
          retryDelay: 0,
          userMessage: `${model} è¯·æ±‚æ ¼å¼é”™è¯¯: ${error.response.data?.message || 'å‚æ•°æœ‰è¯¯'}`
        }
      }
      
      // æ¨¡å‹ä¸å¯ç”¨ (404)
      if (error.response?.status === 404) {
        return {
          canRetry: false,
          retryDelay: 0,
          userMessage: `${model} æ¨¡å‹ä¸å¯ç”¨æˆ–å·²ä¸‹çº¿`
        }
      }
      
      // å…¶ä»–æœªçŸ¥é”™è¯¯
      return {
        canRetry: false,
        retryDelay: 0,
        userMessage: `${model} æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: ${error.message || 'æœªçŸ¥é”™è¯¯'}`
      }
    },

    // è®¾ç½®ä¼˜å…ˆæ¨¡å¼
    setPriorityMode(mode) {
      this.priorityMode = mode
    },

    // è®¾ç½®å¯ç”¨çš„æ¨¡å‹
    setEnabledModels(models) {
      this.enabledModels = models
      // ä¿å­˜åˆ°localStorage
      localStorage.setItem('p2l_enabled_models', JSON.stringify(models))
    },

    // åˆå§‹åŒ–å¯ç”¨çš„æ¨¡å‹ï¼ˆä»localStorageåŠ è½½æˆ–é»˜è®¤å…¨éƒ¨å¯ç”¨ï¼‰
    async initializeEnabledModels() {
      try {
        // é¦–å…ˆä»åç«¯åŠ è½½æ¨¡å‹åˆ—è¡¨
        await this.loadModelsFromBackend()
        
        // æ£€æŸ¥é…ç½®ç‰ˆæœ¬ï¼Œå¦‚æœç‰ˆæœ¬ä¸åŒ¹é…åˆ™æ¸…é™¤æ—§é…ç½®
        const savedVersion = localStorage.getItem('p2l_config_version')
        if (savedVersion !== this.configVersion) {
          console.log(`é…ç½®ç‰ˆæœ¬æ›´æ–° (${savedVersion} -> ${this.configVersion})ï¼Œæ¸…é™¤æ—§é…ç½®...`)
          localStorage.removeItem('p2l_enabled_models')
          localStorage.setItem('p2l_config_version', this.configVersion)
        }
        
        const saved = localStorage.getItem('p2l_enabled_models')
        if (saved) {
          const savedModels = JSON.parse(saved)
          // è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„æ¨¡å‹
          const validModels = savedModels.filter(modelName => 
            this.availableModels.some(m => m.name === modelName)
          )
          
          // å¦‚æœæœ‰æ•ˆæ¨¡å‹æ•°é‡ä¸åŒï¼Œè¯´æ˜æœ‰æ—§æ¨¡å‹è¢«ç§»é™¤æˆ–æ–°æ¨¡å‹è¢«æ·»åŠ ï¼Œéœ€è¦æ›´æ–°
          if (validModels.length !== savedModels.length) {
            console.log('æ£€æµ‹åˆ°è¿‡æ—¶çš„æ¨¡å‹é…ç½®ï¼Œæ­£åœ¨æ›´æ–°...')
            // æ·»åŠ æ–°æ¨¡å‹åˆ°å¯ç”¨åˆ—è¡¨
            const currentModelNames = this.availableModels.map(m => m.name)
            const newModels = currentModelNames.filter(name => !validModels.includes(name))
            this.enabledModels = [...validModels, ...newModels]
            // ä¿å­˜æ›´æ–°åçš„é…ç½®
            this.setEnabledModels(this.enabledModels)
          } else {
            this.enabledModels = validModels
          }
        } else {
          // é»˜è®¤å¯ç”¨æ‰€æœ‰æ¨¡å‹
          this.enabledModels = this.availableModels.map(m => m.name)
          this.setEnabledModels(this.enabledModels)
        }
        
        console.log(`âœ… åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨äº† ${this.enabledModels.length} ä¸ªæ¨¡å‹`)
      } catch (error) {
        console.error('åˆå§‹åŒ–æ¨¡å‹é…ç½®å¤±è´¥:', error)
        // å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if (this.availableModels.length > 0) {
          this.enabledModels = this.availableModels.map(m => m.name)
          this.setEnabledModels(this.enabledModels)
        }
      }
    },

    // æ¸…ç©ºèŠå¤©å†å²
    clearChatHistory() {
      this.chatHistory = []
      this.currentAnalysis = null
      this.recommendations = []
    }
  }
})