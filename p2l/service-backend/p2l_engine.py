#!/usr/bin/env python3
"""
P2Læ¨ç†å¼•æ“æ¨¡å—
è´Ÿè´£P2Læ¨¡å‹åŠ è½½å’Œæ¨ç†åˆ†æ
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, Optional
import logging

from config import get_p2l_config

logger = logging.getLogger(__name__)

# å¯¼å…¥P2Læ¨ç†æ¨¡å—
try:
    from p2l.model import load_model as load_p2l_model, generate_text
    from p2l.p2l_inference import P2LInferenceEngine
    P2L_AVAILABLE = True
except ImportError as e:
    logging.warning(f"P2Læ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    P2L_AVAILABLE = False

class P2LEngine:
    """P2Læ¨ç†å¼•æ“ç®¡ç†å™¨"""
    
    def __init__(self, device: torch.device):
        self.device = device
        self.p2l_models = {}
        self.p2l_inference_engine = None
        self.config = get_p2l_config()
        
        # åŠ è½½P2Læ¨¡å‹å’Œæ¨ç†å¼•æ“
        self._load_p2l_models()
        if P2L_AVAILABLE:
            self._load_p2l_inference_engine()
    
    def _load_p2l_models(self):
        """åŠ è½½å¯ç”¨çš„P2Læ¨¡å‹"""
        model_path = self.config["model_path"]
        if not os.path.exists(model_path):
            logger.warning(f"P2Læ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return
        
        for item in os.listdir(models_dir):
            if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                model_path = os.path.join(models_dir, item)
                try:
                    logger.info(f"åŠ è½½P2Læ¨¡å‹: {item}")
                    tokenizer = AutoTokenizer.from_pretrained(model_path)
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float32,
                        device_map=None
                    )
                    model.to(self.device)
                    model.eval()
                    
                    self.p2l_models[item] = {
                        "tokenizer": tokenizer,
                        "model": model,
                        "path": model_path
                    }
                    logger.info(f"âœ… P2Læ¨¡å‹ {item} åŠ è½½æˆåŠŸ")
                    break  # åªåŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
                except Exception as e:
                    logger.error(f"âŒ P2Læ¨¡å‹ {item} åŠ è½½å¤±è´¥: {e}")
    
    def _load_p2l_inference_engine(self):
        """åŠ è½½P2Læ¨ç†å¼•æ“"""
        try:
            logger.info("æ­£åœ¨åŠ è½½P2Læ¨ç†å¼•æ“...")
            
            # å°è¯•ä»modelsç›®å½•åŠ è½½
            models_dir = "./models"
            p2l_model_path = None
            
            if os.path.exists(models_dir):
                for item in os.listdir(models_dir):
                    if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                        p2l_model_path = os.path.join(models_dir, item)
                        break
            
            # ä½¿ç”¨P2Læ¨ç†å¼•æ“
            if p2l_model_path:
                model, tokenizer = load_p2l_model(p2l_model_path, device=str(self.device))
            else:
                # åˆ›å»ºé»˜è®¤æ¨ç†å¼•æ“
                model, tokenizer = load_p2l_model("", device=str(self.device))
            
            if isinstance(model, P2LInferenceEngine):
                self.p2l_inference_engine = model
                logger.info("âœ… P2Læ¨ç†å¼•æ“åŠ è½½æˆåŠŸ")
            else:
                logger.warning("åŠ è½½çš„ä¸æ˜¯P2Læ¨ç†å¼•æ“ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹")
                
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¼•æ“åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºåŸºæœ¬çš„æ¨ç†å¼•æ“ä½œä¸ºåå¤‡
            try:
                self.p2l_inference_engine = P2LInferenceEngine(device=str(self.device))
                logger.info("âœ… åˆ›å»ºäº†åŸºæœ¬P2Læ¨ç†å¼•æ“")
            except Exception as e2:
                logger.error(f"âŒ åŸºæœ¬P2Læ¨ç†å¼•æ“åˆ›å»ºå¤±è´¥: {e2}")
    
    def semantic_analysis(self, prompt: str) -> tuple:
        """ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†æ"""
        if not self.p2l_models:
            return 0.5, 0.5
        
        try:
            model_name = list(self.p2l_models.keys())[0]
            p2l_model = self.p2l_models[model_name]
            tokenizer = p2l_model["tokenizer"]
            model = p2l_model["model"]
            
            logger.info(f"ğŸ§  ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰å¢å¼ºåˆ†æ...")
            
            # ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰ç‰¹å¾æå–
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                
                # è·å–éšè—çŠ¶æ€ä½œä¸ºè¯­ä¹‰ç‰¹å¾
                if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                    # ä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
                    hidden_states = outputs.hidden_states[-1]  # [batch_size, seq_len, hidden_size]
                    # å¹³å‡æ± åŒ–å¾—åˆ°å¥å­çº§åˆ«çš„è¡¨ç¤º
                    sentence_embedding = hidden_states.mean(dim=1)  # [batch_size, hidden_size]
                    semantic_features = sentence_embedding[0]  # [hidden_size]
                else:
                    # å¦‚æœæ²¡æœ‰éšè—çŠ¶æ€ï¼Œä½¿ç”¨logitsçš„ç»Ÿè®¡ç‰¹å¾
                    logits = outputs.logits  # [batch_size, seq_len, vocab_size]
                    # è®¡ç®—logitsçš„ç»Ÿè®¡ç‰¹å¾ä½œä¸ºè¯­ä¹‰è¡¨ç¤º
                    semantic_features = logits.mean(dim=(0, 1))  # [vocab_size] -> å¹³å‡åˆ°æ ‡é‡ç‰¹å¾
            
            # åŸºäºè¯­ä¹‰ç‰¹å¾è®¡ç®—å¤æ‚åº¦å’Œè¯­è¨€åˆ†æ•°
            if semantic_features.dim() == 1:
                # ä½¿ç”¨è¯­ä¹‰ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
                feature_mean = semantic_features.mean().item()
                feature_std = semantic_features.std().item()
                feature_max = semantic_features.max().item()
                
                # å°†ç»Ÿè®¡ç‰¹å¾æ˜ å°„åˆ°0-1èŒƒå›´
                complexity_score = min(max((feature_std / (abs(feature_mean) + 1e-6)), 0), 1)
                language_score = min(max((feature_max / (abs(feature_mean) + 1e-6)), 0), 1)
            else:
                complexity_score = 0.5
                language_score = 0.5
            
            logger.info(f"ğŸ” è¯­ä¹‰ç‰¹å¾åˆ†æ: mean={feature_mean:.3f}, std={feature_std:.3f}, max={feature_max:.3f}")
            logger.info(f"ğŸ” è®¡ç®—å¾—åˆ†: complexity_score={complexity_score:.3f}, language_score={language_score:.3f}")
            
            return complexity_score, language_score
            
        except Exception as e:
            logger.warning(f"P2Læ¨¡å‹åˆ†æå¤±è´¥: {e}")
            return 0.5, 0.5
    
    def inference_engine_analysis(self, prompt: str) -> Optional[Dict]:
        """ä½¿ç”¨P2Læ¨ç†å¼•æ“è¿›è¡Œä»»åŠ¡åˆ†æ"""
        if not self.p2l_inference_engine:
            return None
        
        try:
            logger.info("ä½¿ç”¨P2Læ¨ç†å¼•æ“è¿›è¡Œä»»åŠ¡åˆ†æ...")
            result = self.p2l_inference_engine.analyze_task_complexity(prompt)
            
            # è½¬æ¢P2Læ¨ç†å¼•æ“çš„è¾“å‡ºæ ¼å¼
            task_analysis = {
                "task_type": result.get("task_type", "é€šç”¨"),
                "complexity": result.get("complexity", "ä¸­ç­‰"),
                "language": result.get("language", "ä¸­æ–‡"),
                "length": len(prompt),
                "p2l_scores": {
                    "complexity": result.get("complexity_score", 0.5),
                    "confidence": result.get("confidence", 0.8)
                }
            }
            
            logger.info(f"ğŸ§  P2Læ¨ç†å¼•æ“åˆ†æ: {task_analysis}")
            return task_analysis
            
        except Exception as e:
            logger.warning(f"P2Læ¨ç†å¼•æ“åˆ†æå¤±è´¥: {e}")
            return None
    
    def code_inference(self, code: str, max_length: int = 512, temperature: float = 0.7) -> Dict:
        """P2Lä»£ç æ¨ç† - å°†ä»£ç è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€"""
        if not self.p2l_inference_engine:
            raise Exception("P2Læ¨ç†å¼•æ“æœªåŠ è½½")
        
        logger.info(f"P2Læ¨ç†è¯·æ±‚: {code[:100]}...")
        
        # ä½¿ç”¨P2Læ¨ç†å¼•æ“
        result = self.p2l_inference_engine.infer(
            code,
            max_length=max_length,
            temperature=temperature
        )
        
        return {
            "code": code,
            "natural_language": result["natural_language"],
            "confidence": result.get("confidence", 0.8),
            "processing_time": result.get("processing_time", 0.0),
            "model_info": "P2L-Inference-Engine"
        }
    
    def get_loaded_models(self) -> Dict:
        """è·å–å·²åŠ è½½çš„P2Læ¨¡å‹ä¿¡æ¯"""
        return {
            "p2l_models": list(self.p2l_models.keys()),
            "inference_engine_available": self.p2l_inference_engine is not None,
            "p2l_available": P2L_AVAILABLE
        }