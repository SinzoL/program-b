#!/usr/bin/env python3
"""
P2Læ¨ç†æ¨¡å— - çœŸæ­£çš„P2Lç¥ç»ç½‘ç»œæ¨ç†å®ç°
å®ç°åŸºäºP2Lç ”ç©¶çš„æ™ºèƒ½æ¨¡å‹æ¨è
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from typing import Dict, List, Tuple, Optional
import numpy as np
import logging
import json
import os
import sys

# æ·»åŠ backendè·¯å¾„ä»¥å¯¼å…¥é…ç½® - å…¼å®¹Dockerç¯å¢ƒ
def _add_backend_path():
    """æ™ºèƒ½æ·»åŠ backendè·¯å¾„ï¼Œå…¼å®¹æœ¬åœ°å’ŒDockerç¯å¢ƒ"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å°è¯•å¤šç§å¯èƒ½çš„backendè·¯å¾„
    possible_paths = [
        # Dockerç¯å¢ƒ: /app/p2l/p2l/p2l_inference.py -> /app/backend
        '/app/backend',
        # æœ¬åœ°å¼€å‘ç¯å¢ƒ: p2l/p2l/p2l_inference.py -> ../../../backend
        os.path.join(os.path.dirname(os.path.dirname(current_dir)), '..', 'backend'),
        # ç›¸å¯¹è·¯å¾„å¤‡é€‰
        os.path.join(current_dir, '..', '..', '..', 'backend'),
        # å½“å‰ç›®å½•çš„backend
        os.path.join(os.getcwd(), 'backend'),
        # PYTHONPATHç¯å¢ƒå˜é‡è·¯å¾„
        os.path.join(os.environ.get('PYTHONPATH', ''), 'backend') if os.environ.get('PYTHONPATH') else None
    ]
    
    # è¿‡æ»¤æ‰Noneå€¼
    possible_paths = [p for p in possible_paths if p is not None]
    
    for path in possible_paths:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path) and abs_path not in sys.path:
            sys.path.insert(0, abs_path)
            print(f"âœ… æˆåŠŸæ·»åŠ backendè·¯å¾„: {abs_path}")
            return abs_path
    
    print("âš ï¸  æœªæ‰¾åˆ°backendè·¯å¾„")
    return None

_add_backend_path()

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ä»¥å¯¼å…¥p2l_core
def _add_constants_path():
    """æ™ºèƒ½æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ä»¥å¯¼å…¥p2l_coreï¼Œå…¼å®¹Dockerå’Œæœ¬åœ°ç¯å¢ƒ"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å°è¯•å¤šç§å¯èƒ½çš„é¡¹ç›®æ ¹è·¯å¾„
    possible_paths = [
        # Dockerç¯å¢ƒ: /app/p2l/p2l/p2l_inference.py -> /app
        '/app',
        # æœ¬åœ°å¼€å‘ç¯å¢ƒ: p2l/p2l/p2l_inference.py -> ../../..
        os.path.join(os.path.dirname(os.path.dirname(current_dir)), '..'),
        # ç›¸å¯¹è·¯å¾„å¤‡é€‰
        os.path.join(current_dir, '..', '..', '..'),
        # å½“å‰å·¥ä½œç›®å½•
        os.getcwd(),
        # PYTHONPATHç¯å¢ƒå˜é‡è·¯å¾„
        os.environ.get('PYTHONPATH', '') if os.environ.get('PYTHONPATH') else None
    ]
    
    # è¿‡æ»¤æ‰Noneå€¼
    possible_paths = [p for p in possible_paths if p is not None]
    
    for path in possible_paths:
        abs_path = os.path.abspath(path)
        p2l_core_file = os.path.join(abs_path, 'p2l_core.py')
        if os.path.exists(p2l_core_file) and abs_path not in sys.path:
            sys.path.insert(0, abs_path)
            print(f"âœ… æˆåŠŸæ·»åŠ p2l_coreè·¯å¾„: {abs_path}")
            return abs_path
    
    print("âš ï¸  æœªæ‰¾åˆ°p2l_core.pyæ–‡ä»¶")
    return None

_add_constants_path()

# å¯¼å…¥é¡¹ç›®å¸¸é‡
try:
    from p2l_core import DEFAULT_MODEL, MODEL_MAPPING
    print("âœ… P2Lå¼•æ“æˆåŠŸå¯¼å…¥é¡¹ç›®å¸¸é‡")
except ImportError as e:
    print(f"âš ï¸  P2Lå¼•æ“æ— æ³•å¯¼å…¥å¸¸é‡: {e}")
    # è®¾ç½®é»˜è®¤å€¼
    DEFAULT_MODEL = "p2l-135m-grk-01112025"
    MODEL_MAPPING = {}

logger = logging.getLogger(__name__)

class P2LTaskClassifier(nn.Module):
    """
    P2Lä»»åŠ¡åˆ†ç±»å™¨ - å°†ç”¨æˆ·promptè½¬æ¢ä¸ºä»»åŠ¡ç‰¹å¾å‘é‡
    """
    def __init__(self, base_model_name: str, num_task_types: int = 8, 
                 num_complexity_levels: int = 3, num_languages: int = 2):
        super().__init__()
        
        # åŸºç¡€ç¼–ç å™¨
        self.encoder = AutoModel.from_pretrained(base_model_name)
        hidden_size = self.encoder.config.hidden_size
        
        # ä»»åŠ¡ç‰¹å¾åˆ†ç±»å¤´
        self.task_classifier = nn.Linear(hidden_size, num_task_types)
        self.complexity_classifier = nn.Linear(hidden_size, num_complexity_levels)
        self.language_classifier = nn.Linear(hidden_size, num_languages)
        self.domain_classifier = nn.Linear(hidden_size, 6)  # é¢†åŸŸåˆ†ç±»
        
        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Linear(
            num_task_types + num_complexity_levels + num_languages + 6, 
            128
        )
        
        # æ¨¡å‹åŒ¹é…å±‚ - åŠ¨æ€è·å–æ¨¡å‹æ•°é‡
        try:
            import sys
            import os
            sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
            from backend.config import MODEL_CONFIGS
            self.num_models = len(MODEL_CONFIGS)
            print(f"âœ… åŠ¨æ€è·å–æ¨¡å‹æ•°é‡: {self.num_models}")
        except ImportError:
            self.num_models = 42  # å¤‡ç”¨å€¼ï¼ŒåŸºäºå½“å‰é…ç½®
            print(f"âš ï¸  ä½¿ç”¨å¤‡ç”¨æ¨¡å‹æ•°é‡: {self.num_models}")
        
        self.model_scorer = nn.Linear(128, self.num_models)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        # ç¼–ç è¾“å…¥
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state.mean(dim=1)
        
        pooled_output = self.dropout(pooled_output)
        
        # å¤šä»»åŠ¡åˆ†ç±»
        task_logits = self.task_classifier(pooled_output)
        complexity_logits = self.complexity_classifier(pooled_output)
        language_logits = self.language_classifier(pooled_output)
        domain_logits = self.domain_classifier(pooled_output)
        
        # ç‰¹å¾èåˆ
        task_probs = F.softmax(task_logits, dim=-1)
        complexity_probs = F.softmax(complexity_logits, dim=-1)
        language_probs = F.softmax(language_logits, dim=-1)
        domain_probs = F.softmax(domain_logits, dim=-1)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        fused_features = torch.cat([task_probs, complexity_probs, language_probs, domain_probs], dim=-1)
        fused_features = self.dropout(fused_features)
        
        # æ¨¡å‹è¯„åˆ†
        model_scores = self.model_scorer(self.feature_fusion(fused_features))
        
        return {
            'task_logits': task_logits,
            'complexity_logits': complexity_logits,
            'language_logits': language_logits,
            'domain_logits': domain_logits,
            'model_scores': model_scores,
            'fused_features': fused_features
        }

class P2LInferenceEngine:
    """
    P2Læ¨ç†å¼•æ“ - å®Œæ•´çš„P2Læ¨ç†æµç¨‹
    """
    
    def __init__(self, model_path: Optional[str] = None, device: str = "auto"):
        self.device = self._setup_device(device)
        self.model = None
        self.tokenizer = None
        
        # å¯¼å…¥é…ç½® - å…¼å®¹Dockerç¯å¢ƒ
        try:
            from config import get_p2l_config
            self.config = get_p2l_config()
            print("âœ… æˆåŠŸå¯¼å…¥P2Lé…ç½®")
        except ImportError as e:
            print(f"âš ï¸  æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶: {e}")
            # æ™ºèƒ½æ£€æµ‹ç¯å¢ƒå¹¶è®¾ç½®é»˜è®¤é…ç½®
            if os.path.exists("/app/models"):
                # Dockerç¯å¢ƒ
                default_model_path = "/app/models"
            elif os.path.exists("./models"):
                # æœ¬åœ°ç¯å¢ƒ
                default_model_path = "./models"
            else:
                # å¤‡ç”¨è·¯å¾„
                default_model_path = "models"
            
            self.config = {
                "model_path": default_model_path, 
                "default_model": DEFAULT_MODEL,
                "available_models": []
            }
            print(f"ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œæ¨¡å‹è·¯å¾„: {default_model_path}")
        
        # è®¾ç½®æ¨¡å‹è·¯å¾„ - æ™ºèƒ½è·¯å¾„è§£æ
        self.p2l_model_path = self._resolve_model_path(model_path)
        
        # ä»»åŠ¡ç±»å‹æ˜ å°„
        self.task_types = [
            "ç¼–ç¨‹", "åˆ›æ„å†™ä½œ", "ç¿»è¯‘", "æ•°å­¦", "åˆ†æ", "é—®ç­”", "æ€»ç»“", "é€šç”¨"
        ]
        
        # å¤æ‚åº¦çº§åˆ«
        self.complexity_levels = ["ç®€å•", "ä¸­ç­‰", "å¤æ‚"]
        
        # è¯­è¨€ç±»å‹
        self.languages = ["ä¸­æ–‡", "è‹±æ–‡"]
        
        # é¢†åŸŸç±»å‹
        self.domains = ["æŠ€æœ¯", "æ–‡å­¦", "å•†ä¸š", "å­¦æœ¯", "æ—¥å¸¸", "ä¸“ä¸š"]
        
        # LLMæ¨¡å‹åˆ—è¡¨ - åŠ¨æ€è·å–
        self.llm_models = self._load_llm_models()
        
        # æ¨¡å‹é…ç½®
        self.model_configs = self._load_model_configs()
        
        # åŠ è½½æˆ–åˆå§‹åŒ–æ¨¡å‹
        if self.p2l_model_path and os.path.exists(self.p2l_model_path):
            print("=" * 50)
            print("ğŸ¯ P2Læ¨¡å‹åŠ è½½")
            print("=" * 50)
            print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {self.p2l_model_path}")
            print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
            self.load_model(self.p2l_model_path)
            print("âœ… P2Læ¨¡å‹åŠ è½½å®Œæˆ")
            print("=" * 50)
        else:
            print("=" * 50)
            print("âš ï¸  P2Læ¨¡å‹æœªæ‰¾åˆ°")
            print("=" * 50)
            print(f"ğŸ” æŸ¥æ‰¾è·¯å¾„: {self.p2l_model_path}")
            print("ğŸ’¡ å»ºè®®æ“ä½œ:")
            print("   1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½")
            print("   2. è¿è¡Œ python download_current_model.py ä¸‹è½½æ¨¡å‹")
            print("   3. æˆ–ç­‰å¾…backendæœåŠ¡è‡ªåŠ¨ä¸‹è½½")
            print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–å¤‡ç”¨æ¨¡å¼...")
            print("=" * 50)
            self._initialize_model()
    
    def _resolve_model_path(self, model_path: Optional[str] = None) -> str:
        """æ™ºèƒ½è§£ææ¨¡å‹è·¯å¾„ï¼Œå…¼å®¹æœ¬åœ°å’ŒDockerç¯å¢ƒ"""
        if model_path:
            return model_path
        
        # è·å–é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
        default_model_name = self.config.get("default_model", DEFAULT_MODEL)
        
        # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°åç§°
        # é¦–å…ˆä»MODEL_MAPPINGè·å–
        if default_model_name in MODEL_MAPPING:
            local_name = MODEL_MAPPING[default_model_name]["local_name"]
            print(f"âœ… ä»MODEL_MAPPINGè·å–local_name: {local_name}")
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»é…ç½®æ–‡ä»¶æŸ¥æ‰¾
            local_name = "p2l-135m-grk"  # æœ€ç»ˆå¤‡ç”¨å€¼
            available_models = self.config.get("available_models", [])
            for model in available_models:
                if model.get("name") == default_model_name:
                    local_name = model.get("local_name", "p2l-135m-grk")
                    break
            print(f"âš ï¸  ä»é…ç½®æ–‡ä»¶è·å–local_name: {local_name}")
        
        # æ™ºèƒ½è·¯å¾„è§£æ - Dockerä¼˜å…ˆ
        base_model_path = self.config.get("model_path", "./models")
        
        # å°è¯•å¤šç§å¯èƒ½çš„è·¯å¾„ - Dockerç¯å¢ƒä¼˜å…ˆ
        possible_paths = [
            # Dockerç¯å¢ƒè·¯å¾„ï¼ˆä¼˜å…ˆï¼‰
            f"/app/models/{local_name}",
            # é…ç½®è·¯å¾„
            os.path.join(base_model_path, local_name),
            # æœ¬åœ°ç¯å¢ƒè·¯å¾„
            f"./models/{local_name}",
            f"models/{local_name}",
            # ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
            os.path.join(os.getcwd(), "models", local_name),
            # å¤‡ç”¨è·¯å¾„ï¼ˆæœ¬åœ°å¼€å‘ï¼‰
            f"/Users/sinzol/Desktop/program-b/models/{local_name}"
        ]
        
        # æ£€æµ‹è¿è¡Œç¯å¢ƒ
        is_docker = os.path.exists('/app') and os.getcwd().startswith('/app')
        if is_docker:
            print("ğŸ³ æ£€æµ‹åˆ°Dockerç¯å¢ƒ")
        else:
            print("ğŸ’» æ£€æµ‹åˆ°æœ¬åœ°ç¯å¢ƒ")
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"ğŸ¯ æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {path}")
                return path
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œæ ¹æ®ç¯å¢ƒè¿”å›é»˜è®¤è·¯å¾„
        if is_docker:
            default_path = f"/app/models/{local_name}"
        else:
            default_path = possible_paths[1]  # é…ç½®è·¯å¾„
        
        print(f"ğŸ” ä½¿ç”¨é»˜è®¤è·¯å¾„: {default_path}")
        return default_path
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif torch.backends.mps.is_available():
                return torch.device("mps")
            else:
                return torch.device("cpu")
        return torch.device(device)
    

    
    def _load_llm_models(self) -> List[str]:
        """åŠ¨æ€åŠ è½½LLMæ¨¡å‹åˆ—è¡¨ï¼Œå…¼å®¹Dockerå’Œæœ¬åœ°ç¯å¢ƒ"""
        try:
            # ä¼˜å…ˆå°è¯•ä»å¤–ç½®é…ç½®æ–‡ä»¶åŠ è½½
            import sys
            import os
            
            # æ™ºèƒ½æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ - Dockerä¼˜å…ˆ
            possible_roots = [
                '/app',  # Dockerç¯å¢ƒ
                os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),  # æœ¬åœ°ç¯å¢ƒ
                os.environ.get('PYTHONPATH', '') if os.environ.get('PYTHONPATH') else None
            ]
            
            project_root = None
            for root in possible_roots:
                if root and os.path.exists(os.path.join(root, 'model_configs.py')):
                    project_root = root
                    break
            
            if project_root and project_root not in sys.path:
                sys.path.insert(0, project_root)
                print(f"âœ… æ·»åŠ é…ç½®è·¯å¾„: {project_root}")
            
            from model_configs import get_model_names
            models = get_model_names()
            print(f"âœ… ä»å¤–ç½®é…ç½®åŠ è½½LLMæ¨¡å‹: {len(models)} ä¸ª")
            return models
        except ImportError:
            try:
                # å¤‡ç”¨ï¼šä»backendé…ç½®åŠ è½½
                from config import MODEL_CONFIGS
                models = list(MODEL_CONFIGS.keys())
                print(f"âœ… ä»backendé…ç½®åŠ è½½LLMæ¨¡å‹: {len(models)} ä¸ª")
                return models
            except ImportError as e:
                raise RuntimeError(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹é…ç½®: {e}ã€‚è¯·ç¡®ä¿model_configs.pyæˆ–backend/config.pyå­˜åœ¨ä¸”å¯è®¿é—®ã€‚")
    
    def _load_model_configs(self) -> Dict:
        """åŠ¨æ€åŠ è½½æ¨¡å‹é…ç½®ï¼Œå…¼å®¹Dockerå’Œæœ¬åœ°ç¯å¢ƒ"""
        try:
            # ä¼˜å…ˆå°è¯•ä»å¤–ç½®é…ç½®æ–‡ä»¶åŠ è½½
            import sys
            import os
            
            # æ™ºèƒ½æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ - Dockerä¼˜å…ˆ
            possible_roots = [
                '/app',  # Dockerç¯å¢ƒ
                os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),  # æœ¬åœ°ç¯å¢ƒ
                os.environ.get('PYTHONPATH', '') if os.environ.get('PYTHONPATH') else None
            ]
            
            project_root = None
            for root in possible_roots:
                if root and os.path.exists(os.path.join(root, 'model_configs.py')):
                    project_root = root
                    break
            
            if project_root and project_root not in sys.path:
                sys.path.insert(0, project_root)
                print(f"âœ… æ·»åŠ é…ç½®è·¯å¾„: {project_root}")
            
            from model_configs import get_all_models
            configs = get_all_models()
            print(f"âœ… ä»å¤–ç½®é…ç½®åŠ è½½æ¨¡å‹é…ç½®: {len(configs)} ä¸ª")
            return configs
        except ImportError:
            try:
                # å¤‡ç”¨ï¼šä»backendé…ç½®åŠ è½½
                from config import MODEL_CONFIGS
                print(f"âœ… ä»backendé…ç½®åŠ è½½æ¨¡å‹é…ç½®: {len(MODEL_CONFIGS)} ä¸ª")
                return MODEL_CONFIGS
            except ImportError as e:
                raise RuntimeError(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹é…ç½®: {e}ã€‚è¯·ç¡®ä¿model_configs.pyæˆ–backend/config.pyå­˜åœ¨ä¸”å¯è®¿é—®ã€‚")
    
    def _initialize_model(self):
        """åˆå§‹åŒ–P2Læ¨¡å‹"""
        logger.info("åˆå§‹åŒ–P2Lä»»åŠ¡åˆ†ç±»å™¨...")
        
        # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ä½œä¸ºåŸºç¡€ç¼–ç å™¨
        base_model_name = "sentence-transformers/all-MiniLM-L6-v2"
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            self.model = P2LTaskClassifier(
                base_model_name=base_model_name,
                num_task_types=len(self.task_types),
                num_complexity_levels=len(self.complexity_levels),
                num_languages=len(self.languages)
            )
            
            self.model.to(self.device)
            self.model.eval()
            
            # åˆå§‹åŒ–æƒé‡
            self._initialize_weights()
            
            logger.info(f"âœ… P2Læ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œè®¾å¤‡: {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # é™çº§åˆ°è§„åˆ™æ–¹æ³•
            self.model = None
            self.tokenizer = None
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.model.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„P2Læ¨¡å‹"""
        try:
            logger.info(f"åŠ è½½P2Læ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºSafeTensorsæ ¼å¼
            if os.path.exists(os.path.join(model_path, "model.safetensors")):
                logger.info("ğŸ”’ æ£€æµ‹åˆ°SafeTensorsæ ¼å¼ï¼Œä½¿ç”¨TransformersåŠ è½½")
                from transformers import AutoModel, AutoTokenizer
                
                # åŠ è½½tokenizerå’Œæ¨¡å‹
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                self.model = AutoModel.from_pretrained(model_path)
                self.model.to(self.device)
                self.model.eval()
                
                logger.info("âœ… P2Læ¨¡å‹(SafeTensors)åŠ è½½æˆåŠŸ")
                return
            
            # ä¼ ç»Ÿpytorch_model.binæ ¼å¼
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # åŠ è½½æ¨¡å‹
            checkpoint = torch.load(os.path.join(model_path, "pytorch_model.bin"), map_location=self.device)
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            self.model = P2LTaskClassifier(
                base_model_name=model_path,
                num_task_types=len(self.task_types),
                num_complexity_levels=len(self.complexity_levels),
                num_languages=len(self.languages)
            )
            
            self.model.load_state_dict(checkpoint)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("âœ… P2Læ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self._initialize_model()  # é™çº§åˆ°åˆå§‹åŒ–æ¨¡å‹
    
    def analyze_prompt(self, prompt: str) -> Dict:
        """
        åˆ†æç”¨æˆ·promptï¼Œæå–ä»»åŠ¡ç‰¹å¾
        """
        if not self.model or not self.tokenizer:
            logger.warning("P2Læ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨è§„åˆ™æ–¹æ³•")
            return self._rule_based_analysis(prompt)
        
        try:
            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            model_type = type(self.model).__name__
            
            if model_type == "P2LTaskClassifier":
                # è‡ªå®šä¹‰P2Låˆ†ç±»å™¨
                return self._analyze_with_custom_classifier(prompt)
            else:
                # çœŸæ­£çš„P2Læ¨¡å‹ï¼ˆLlamaModelç­‰ï¼‰
                return self._analyze_with_real_p2l_model(prompt)
            
        except Exception as e:
            logger.error(f"P2Læ¨ç†å¤±è´¥: {e}")
            return self._rule_based_analysis(prompt)
    
    def _analyze_with_custom_classifier(self, prompt: str) -> Dict:
        """ä½¿ç”¨è‡ªå®šä¹‰P2Låˆ†ç±»å™¨è¿›è¡Œåˆ†æ"""
        # é¢„å¤„ç†è¾“å…¥
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # è§£æè¾“å‡º
        task_probs = F.softmax(outputs['task_logits'], dim=-1)[0]
        complexity_probs = F.softmax(outputs['complexity_logits'], dim=-1)[0]
        language_probs = F.softmax(outputs['language_logits'], dim=-1)[0]
        domain_probs = F.softmax(outputs['domain_logits'], dim=-1)[0]
        model_scores = outputs['model_scores'][0]
        
        # è·å–æœ€å¯èƒ½çš„åˆ†ç±»
        task_idx = torch.argmax(task_probs).item()
        complexity_idx = torch.argmax(complexity_probs).item()
        language_idx = torch.argmax(language_probs).item()
        domain_idx = torch.argmax(domain_probs).item()
        
        analysis = {
            "task_type": self.task_types[task_idx],
            "task_confidence": task_probs[task_idx].item(),
            "complexity": self.complexity_levels[complexity_idx],
            "complexity_confidence": complexity_probs[complexity_idx].item(),
            "language": self.languages[language_idx],
            "language_confidence": language_probs[language_idx].item(),
            "domain": self.domains[domain_idx],
            "domain_confidence": domain_probs[domain_idx].item(),
            "length": len(prompt),
            "model_scores": model_scores.cpu().numpy().tolist(),
            "neural_network_used": True
        }
        
        logger.info(f"ğŸ§  P2Lè‡ªå®šä¹‰åˆ†ç±»å™¨åˆ†æ: {analysis['task_type']}/{analysis['complexity']}/{analysis['language']}")
        return analysis
    
    def _analyze_with_real_p2l_model(self, prompt: str) -> Dict:
        """ä½¿ç”¨çœŸæ­£çš„P2Læ¨¡å‹è¿›è¡Œåˆ†æ"""
        # é¢„å¤„ç†è¾“å…¥
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # ä»çœŸæ­£çš„P2Læ¨¡å‹è¾“å‡ºä¸­æå–ç‰¹å¾
        # P2Læ¨¡å‹çš„è¾“å‡ºæ˜¯hidden statesï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå¤„ç†
        if hasattr(outputs, 'last_hidden_state'):
            hidden_states = outputs.last_hidden_state
            # ä½¿ç”¨å¹³å‡æ± åŒ–è·å–å¥å­è¡¨ç¤º
            sentence_embedding = hidden_states.mean(dim=1)[0]  # [hidden_size]
            
            # åŸºäºembeddingè¿›è¡Œç®€å•çš„ç‰¹å¾æå–
            embedding_norm = torch.norm(sentence_embedding).item()
            embedding_mean = torch.mean(sentence_embedding).item()
            embedding_std = torch.std(sentence_embedding).item()
            
            # åŸºäºembeddingç‰¹å¾è¿›è¡Œä»»åŠ¡åˆ†ç±»
            task_type, task_confidence = self._classify_task_from_embedding(prompt, sentence_embedding)
            complexity, complexity_confidence = self._classify_complexity_from_embedding(prompt, sentence_embedding)
            language, language_confidence = self._classify_language_from_embedding(prompt, sentence_embedding)
            
            analysis = {
                "task_type": task_type,
                "task_confidence": task_confidence,
                "complexity": complexity,
                "complexity_confidence": complexity_confidence,
                "language": language,
                "language_confidence": language_confidence,
                "domain": "æŠ€æœ¯",
                "domain_confidence": 0.8,
                "length": len(prompt),
                "embedding_norm": embedding_norm,
                "embedding_mean": embedding_mean,
                "embedding_std": embedding_std,
                "neural_network_used": True
            }
            
            logger.info(f"ğŸ§  P2LçœŸå®æ¨¡å‹åˆ†æ: {analysis['task_type']}/{analysis['complexity']}/{analysis['language']}")
            return analysis
        else:
            # å¦‚æœè¾“å‡ºæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œé™çº§åˆ°è§„åˆ™æ–¹æ³•
            logger.warning("P2Læ¨¡å‹è¾“å‡ºæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œé™çº§åˆ°è§„åˆ™æ–¹æ³•")
            return self._rule_based_analysis(prompt)
    
    def _classify_task_from_embedding(self, prompt: str, embedding: torch.Tensor) -> Tuple[str, float]:
        """åŸºäºembeddingåˆ†ç±»ä»»åŠ¡ç±»å‹"""
        prompt_lower = prompt.lower()
        
        # ç»“åˆè§„åˆ™å’Œembeddingç‰¹å¾
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            return "ç¼–ç¨‹", 0.9
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            return "åˆ›æ„å†™ä½œ", 0.85
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english"]):
            return "ç¿»è¯‘", 0.9
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—"]):
            return "æ•°å­¦", 0.85
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š"]):
            return "åˆ†æ", 0.8
        else:
            return "é€šç”¨", 0.7
    
    def _classify_complexity_from_embedding(self, prompt: str, embedding: torch.Tensor) -> Tuple[str, float]:
        """åŸºäºembeddingåˆ†ç±»å¤æ‚åº¦"""
        # åŸºäºé•¿åº¦å’Œå…³é”®è¯
        if len(prompt) > 200 or any(word in prompt.lower() for word in ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´"]):
            return "å¤æ‚", 0.8
        elif len(prompt) > 100:
            return "ä¸­ç­‰", 0.75
        else:
            return "ç®€å•", 0.7
    
    def _classify_language_from_embedding(self, prompt: str, embedding: torch.Tensor) -> Tuple[str, float]:
        """åŸºäºembeddingåˆ†ç±»è¯­è¨€"""
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(prompt) * 0.3:
            return "ä¸­æ–‡", 0.9
        else:
            return "è‹±æ–‡", 0.8
    
    def recommend_models(self, prompt: str, priority: str = "performance") -> Dict:
        """
        åŸºäºP2Låˆ†ææ¨èæœ€é€‚åˆçš„æ¨¡å‹
        """
        # åˆ†æä»»åŠ¡ç‰¹å¾
        analysis = self.analyze_prompt(prompt)
        
        # è®¡ç®—æ¨¡å‹åˆ†æ•°
        model_rankings = self._calculate_model_rankings(analysis, priority)
        
        # ç”Ÿæˆæ¨èç»“æœ
        best_model = model_rankings[0]
        
        # ç”Ÿæˆæ¨èç†ç”±
        reasoning = self._generate_reasoning(analysis, best_model, priority)
        
        result = {
            "recommended_model": best_model["model"],
            "confidence": best_model["score"],
            "task_analysis": analysis,
            "reasoning": reasoning,
            "model_rankings": model_rankings[:5],
            "priority_mode": priority,
            "p2l_version": "2.0",
            "inference_method": "neural_network" if analysis.get("neural_network_used") else "rule_based"
        }
        
        return result
    
    def _calculate_model_rankings(self, analysis: Dict, priority: str) -> List[Dict]:
        """è®¡ç®—æ¨¡å‹æ’å"""
        rankings = []
        
        for i, model_name in enumerate(self.llm_models):
            config = self.model_configs[model_name]
            
            # åŸºç¡€åˆ†æ•°
            base_score = config["quality_score"]
            
            # P2Lç¥ç»ç½‘ç»œåˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if "model_scores" in analysis:
                neural_score = analysis["model_scores"][i]
                # å°†ç¥ç»ç½‘ç»œè¾“å‡ºè½¬æ¢ä¸º0-1èŒƒå›´
                neural_score = torch.sigmoid(torch.tensor(neural_score)).item()
                base_score = 0.6 * base_score + 0.4 * neural_score
            
            # ä»»åŠ¡åŒ¹é…åŠ åˆ†
            task_bonus = 0
            if analysis["task_type"] in config["strengths"]:
                task_bonus = 0.15 * analysis.get("task_confidence", 1.0)
            
            # è¯­è¨€åŒ¹é…åŠ åˆ†
            language_bonus = 0
            if analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in config["strengths"]:
                language_bonus = 0.20 * analysis.get("language_confidence", 1.0)
            elif analysis["language"] == "è‹±æ–‡" and "ä¸­æ–‡" not in config["strengths"]:
                language_bonus = 0.10 * analysis.get("language_confidence", 1.0)
            
            # å¤æ‚åº¦åŒ¹é…
            complexity_bonus = 0
            if analysis["complexity"] == "å¤æ‚" and config["quality_score"] > 0.90:
                complexity_bonus = 0.10 * analysis.get("complexity_confidence", 1.0)
            elif analysis["complexity"] == "ç®€å•" and config["avg_response_time"] < 2.0:
                complexity_bonus = 0.05 * analysis.get("complexity_confidence", 1.0)
            
            # ä¼˜å…ˆçº§è°ƒæ•´
            priority_bonus = 0
            if priority == "cost" and config["cost_per_1k"] < 0.01:
                priority_bonus = 0.20
            elif priority == "speed" and config["avg_response_time"] < 2.0:
                priority_bonus = 0.15
            elif priority == "performance" and config["quality_score"] > 0.90:
                priority_bonus = 0.10
            
            final_score = base_score + task_bonus + language_bonus + complexity_bonus + priority_bonus
            final_score = min(final_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼
            
            rankings.append({
                "model": model_name,
                "score": round(final_score, 4),
                "provider": config["provider"],
                "cost_per_1k": config["cost_per_1k"],
                "avg_response_time": config["avg_response_time"],
                "quality_score": config["quality_score"],
                "strengths": config["strengths"]
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        rankings.sort(key=lambda x: x["score"], reverse=True)
        return rankings
    
    def _generate_reasoning(self, analysis: Dict, best_model: Dict, priority: str) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        reasons = []
        
        # ä»»åŠ¡åŒ¹é…
        if analysis["task_type"] in best_model["strengths"]:
            confidence = analysis.get("task_confidence", 1.0)
            reasons.append(f"æ“…é•¿{analysis['task_type']}ä»»åŠ¡ (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # è¯­è¨€åŒ¹é…
        if analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in best_model["strengths"]:
            confidence = analysis.get("language_confidence", 1.0)
            reasons.append(f"ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # å¤æ‚åº¦åŒ¹é…
        if analysis["complexity"] == "å¤æ‚" and best_model["quality_score"] > 0.90:
            confidence = analysis.get("complexity_confidence", 1.0)
            reasons.append(f"é€‚åˆå¤æ‚ä»»åŠ¡ (ç½®ä¿¡åº¦: {confidence:.2f})")
        elif analysis["complexity"] == "ç®€å•" and best_model["avg_response_time"] < 2.0:
            confidence = analysis.get("complexity_confidence", 1.0)
            reasons.append(f"å¿«é€Ÿå¤„ç†ç®€å•ä»»åŠ¡ (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # ä¼˜å…ˆçº§åŒ¹é…
        if priority == "cost" and best_model["cost_per_1k"] < 0.01:
            reasons.append("æˆæœ¬æ•ˆç›Šæœ€ä¼˜")
        elif priority == "speed" and best_model["avg_response_time"] < 2.0:
            reasons.append("å“åº”é€Ÿåº¦æœ€å¿«")
        elif priority == "performance" and best_model["quality_score"] > 0.90:
            reasons.append("æ€§èƒ½è¡¨ç°æœ€ä½³")
        
        # P2Lç¥ç»ç½‘ç»œæ¨ç†
        if analysis.get("neural_network_used"):
            reasons.append("åŸºäºP2Lç¥ç»ç½‘ç»œæ™ºèƒ½åˆ†æ")
        
        return "ï¼›".join(reasons) if reasons else "ç»¼åˆè¯„ä¼°æœ€é€‚åˆ"
    
    def _rule_based_analysis(self, prompt: str) -> Dict:
        """å¤‡ç”¨è§„åˆ™åˆ†ææ–¹æ³•"""
        prompt_lower = prompt.lower()
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        task_confidence = 0.8
        
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            task_type = "ç¼–ç¨‹"
            task_confidence = 0.9
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            task_type = "åˆ›æ„å†™ä½œ"
            task_confidence = 0.85
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french"]):
            task_type = "ç¿»è¯‘"
            task_confidence = 0.9
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation"]):
            task_type = "æ•°å­¦"
            task_confidence = 0.85
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe"]):
            task_type = "åˆ†æ"
            task_confidence = 0.8
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = "ç®€å•"
        complexity_confidence = 0.7
        
        if len(prompt) > 200 or any(word in prompt_lower for word in ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´", "æ·±å…¥"]):
            complexity = "å¤æ‚"
            complexity_confidence = 0.8
        elif len(prompt) > 100:
            complexity = "ä¸­ç­‰"
            complexity_confidence = 0.75
        
        # è¯­è¨€æ£€æµ‹
        language = "è‹±æ–‡"
        language_confidence = 0.8
        
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(prompt) * 0.3:
            language = "ä¸­æ–‡"
            language_confidence = 0.9
        
        # é¢†åŸŸæ£€æµ‹
        domain = "é€šç”¨"
        domain_confidence = 0.7
        
        if any(word in prompt_lower for word in ["tech", "technology", "æŠ€æœ¯", "ç§‘æŠ€"]):
            domain = "æŠ€æœ¯"
            domain_confidence = 0.8
        elif any(word in prompt_lower for word in ["business", "å•†ä¸š", "å¸‚åœº", "è¥é”€"]):
            domain = "å•†ä¸š"
            domain_confidence = 0.8
        
        return {
            "task_type": task_type,
            "task_confidence": task_confidence,
            "complexity": complexity,
            "complexity_confidence": complexity_confidence,
            "language": language,
            "language_confidence": language_confidence,
            "domain": domain,
            "domain_confidence": domain_confidence,
            "length": len(prompt),
            "neural_network_used": False
        }
    
    def save_model(self, save_path: str):
        """ä¿å­˜P2Læ¨¡å‹"""
        if not self.model:
            logger.error("æ²¡æœ‰æ¨¡å‹å¯ä¿å­˜")
            return
        
        try:
            os.makedirs(save_path, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹æƒé‡
            torch.save(self.model.state_dict(), os.path.join(save_path, "pytorch_model.bin"))
            
            # ä¿å­˜tokenizer
            if self.tokenizer:
                self.tokenizer.save_pretrained(save_path)
            
            # ä¿å­˜é…ç½®
            config = {
                "task_types": self.task_types,
                "complexity_levels": self.complexity_levels,
                "languages": self.languages,
                "domains": self.domains,
                "llm_models": self.llm_models
            }
            
            with open(os.path.join(save_path, "p2l_config.json"), "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… P2Læ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜P2Læ¨¡å‹å¤±è´¥: {e}")

# å…¨å±€P2Læ¨ç†å¼•æ“å®ä¾‹
_p2l_engine = None

def get_p2l_engine() -> P2LInferenceEngine:
    """è·å–å…¨å±€P2Læ¨ç†å¼•æ“å®ä¾‹"""
    global _p2l_engine
    if _p2l_engine is None:
        _p2l_engine = P2LInferenceEngine()
    return _p2l_engine

def analyze_prompt_with_p2l(prompt: str) -> Dict:
    """ä½¿ç”¨P2Låˆ†æprompt"""
    engine = get_p2l_engine()
    return engine.analyze_prompt(prompt)

def recommend_models_with_p2l(prompt: str, priority: str = "performance") -> Dict:
    """ä½¿ç”¨P2Læ¨èæ¨¡å‹"""
    engine = get_p2l_engine()
    return engine.recommend_models(prompt, priority)

if __name__ == "__main__":
    # æµ‹è¯•P2Læ¨ç†å¼•æ“
    engine = P2LInferenceEngine()
    
    test_prompts = [
        "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºå‡½æ•°",
        "å¸®æˆ‘ç¿»è¯‘è¿™æ®µè‹±æ–‡åˆ°ä¸­æ–‡",
        "åˆ†æä¸€ä¸‹å½“å‰çš„ç»æµå½¢åŠ¿",
        "åˆ›ä½œä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—æ­Œ",
        "è§£å†³è¿™ä¸ªæ•°å­¦æ–¹ç¨‹ï¼šx^2 + 5x + 6 = 0"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ æµ‹è¯•prompt: {prompt}")
        result = engine.recommend_models(prompt)
        print(f"ğŸ¯ æ¨èæ¨¡å‹: {result['recommended_model']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"ğŸ§  æ¨ç†æ–¹æ³•: {result['inference_method']}")
        print(f"ğŸ’¡ æ¨èç†ç”±: {result['reasoning']}")