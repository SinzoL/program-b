#!/usr/bin/env python3
"""
P2Læ¨ç†æ¨¡å— - çœŸæ­£çš„P2Lç¥ç»ç½‘ç»œæ¨ç†å®ç°
å®ç°åŸºäºP2Lç ”ç©¶çš„æ™ºèƒ½æ¨¡å‹æ¨è
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from typing import Dict, List, Tuple, Optional
import numpy as np
import logging
import json
import os

logger = logging.getLogger(__name__)

class P2LTaskClassifier(nn.Module):
    """
    P2Lä»»åŠ¡åˆ†ç±»å™¨ - å°†ç”¨æˆ·promptè½¬æ¢ä¸ºä»»åŠ¡ç‰¹å¾å‘é‡
    """
    def __init__(self, base_model_name: str, num_task_types: int = 8, 
                 num_complexity_levels: int = 3, num_languages: int = 2):
        super().__init__()
        
        # åŸºç¡€ç¼–ç å™¨
        self.encoder = AutoModel.from_pretrained(base_model_name)
        hidden_size = self.encoder.config.hidden_size
        
        # ä»»åŠ¡ç‰¹å¾åˆ†ç±»å¤´
        self.task_classifier = nn.Linear(hidden_size, num_task_types)
        self.complexity_classifier = nn.Linear(hidden_size, num_complexity_levels)
        self.language_classifier = nn.Linear(hidden_size, num_languages)
        self.domain_classifier = nn.Linear(hidden_size, 6)  # é¢†åŸŸåˆ†ç±»
        
        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Linear(
            num_task_types + num_complexity_levels + num_languages + 6, 
            128
        )
        
        # æ¨¡å‹åŒ¹é…å±‚
        self.model_scorer = nn.Linear(128, 9)  # 9ä¸ªLLMæ¨¡å‹
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        # ç¼–ç è¾“å…¥
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state.mean(dim=1)
        
        pooled_output = self.dropout(pooled_output)
        
        # å¤šä»»åŠ¡åˆ†ç±»
        task_logits = self.task_classifier(pooled_output)
        complexity_logits = self.complexity_classifier(pooled_output)
        language_logits = self.language_classifier(pooled_output)
        domain_logits = self.domain_classifier(pooled_output)
        
        # ç‰¹å¾èåˆ
        task_probs = F.softmax(task_logits, dim=-1)
        complexity_probs = F.softmax(complexity_logits, dim=-1)
        language_probs = F.softmax(language_logits, dim=-1)
        domain_probs = F.softmax(domain_logits, dim=-1)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        fused_features = torch.cat([task_probs, complexity_probs, language_probs, domain_probs], dim=-1)
        fused_features = self.dropout(fused_features)
        
        # æ¨¡å‹è¯„åˆ†
        model_scores = self.model_scorer(self.feature_fusion(fused_features))
        
        return {
            'task_logits': task_logits,
            'complexity_logits': complexity_logits,
            'language_logits': language_logits,
            'domain_logits': domain_logits,
            'model_scores': model_scores,
            'fused_features': fused_features
        }

class P2LInferenceEngine:
    """
    P2Læ¨ç†å¼•æ“ - å®Œæ•´çš„P2Læ¨ç†æµç¨‹
    """
    
    def __init__(self, model_path: Optional[str] = None, device: str = "auto"):
        self.device = self._setup_device(device)
        self.model = None
        self.tokenizer = None
        
        # ä»»åŠ¡ç±»å‹æ˜ å°„
        self.task_types = [
            "ç¼–ç¨‹", "åˆ›æ„å†™ä½œ", "ç¿»è¯‘", "æ•°å­¦", "åˆ†æ", "é—®ç­”", "æ€»ç»“", "é€šç”¨"
        ]
        
        # å¤æ‚åº¦çº§åˆ«
        self.complexity_levels = ["ç®€å•", "ä¸­ç­‰", "å¤æ‚"]
        
        # è¯­è¨€ç±»å‹
        self.languages = ["ä¸­æ–‡", "è‹±æ–‡"]
        
        # é¢†åŸŸç±»å‹
        self.domains = ["æŠ€æœ¯", "æ–‡å­¦", "å•†ä¸š", "å­¦æœ¯", "æ—¥å¸¸", "ä¸“ä¸š"]
        
        # LLMæ¨¡å‹åˆ—è¡¨
        self.llm_models = [
            "gpt-4o", "gpt-4o-mini", "claude-3-5-sonnet-20241022", 
            "claude-3-7-sonnet-20250219", "claude-3-5-haiku-20241022", 
            "gemini-1.5-pro-002", "gemini-1.5-flash-002", "qwen2.5-72b-instruct", 
            "llama-3.1-70b-instruct", "deepseek-v3"
        ]
        
        # æ¨¡å‹é…ç½®
        self.model_configs = self._load_model_configs()
        
        # åŠ è½½æˆ–åˆå§‹åŒ–æ¨¡å‹
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            self._initialize_model()
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif torch.backends.mps.is_available():
                return torch.device("mps")
            else:
                return torch.device("cpu")
        return torch.device(device)
    
    def _load_model_configs(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        return {
            "gpt-4o": {
                "provider": "openai", "cost_per_1k": 0.03, "avg_response_time": 2.5,
                "strengths": ["ç¼–ç¨‹", "æ•°å­¦", "åˆ†æ"], "quality_score": 0.95,
                "context_length": 128000, "multimodal": True
            },
            "gpt-4o-mini": {
                "provider": "openai", "cost_per_1k": 0.0015, "avg_response_time": 1.2,
                "strengths": ["é—®ç­”", "æ€»ç»“"], "quality_score": 0.82,
                "context_length": 128000, "multimodal": False
            },
            "claude-3-5-sonnet-20241022": {
                "provider": "anthropic", "cost_per_1k": 0.025, "avg_response_time": 2.8,
                "strengths": ["åˆ›æ„å†™ä½œ", "åˆ†æ"], "quality_score": 0.93,
                "context_length": 200000, "multimodal": True
            },
            "claude-3-7-sonnet-20250219": {
                "provider": "anthropic", "cost_per_1k": 0.025, "avg_response_time": 2.5,
                "strengths": ["åˆ›æ„å†™ä½œ", "åˆ†æ", "ç¼–ç¨‹"], "quality_score": 0.95,
                "context_length": 200000, "multimodal": True
            },
            "claude-3-5-haiku-20241022": {
                "provider": "anthropic", "cost_per_1k": 0.008, "avg_response_time": 1.5,
                "strengths": ["é—®ç­”", "æ€»ç»“"], "quality_score": 0.85,
                "context_length": 200000, "multimodal": False
            },
            "gemini-1.5-pro-002": {
                "provider": "google", "cost_per_1k": 0.02, "avg_response_time": 2.2,
                "strengths": ["åˆ†æ", "æ•°å­¦"], "quality_score": 0.90,
                "context_length": 1000000, "multimodal": True
            },
            "gemini-1.5-flash-002": {
                "provider": "google", "cost_per_1k": 0.005, "avg_response_time": 1.0,
                "strengths": ["é—®ç­”", "æ€»ç»“"], "quality_score": 0.80,
                "context_length": 1000000, "multimodal": False
            },
            "qwen2.5-72b-instruct": {
                "provider": "qwen", "cost_per_1k": 0.002, "avg_response_time": 2.0,
                "strengths": ["ä¸­æ–‡ç†è§£", "æ¨ç†", "ç¼–ç¨‹", "æ•°å­¦"], "quality_score": 0.90,
                "context_length": 32768, "multimodal": False
            },
            "llama-3.1-70b-instruct": {
                "provider": "meta", "cost_per_1k": 0.01, "avg_response_time": 2.3,
                "strengths": ["ç¼–ç¨‹", "é€šç”¨"], "quality_score": 0.86,
                "context_length": 128000, "multimodal": False
            },
            "deepseek-v3": {
                "provider": "deepseek", "cost_per_1k": 0.012, "avg_response_time": 1.8,
                "strengths": ["æ•°å­¦", "ç¼–ç¨‹"], "quality_score": 0.87,
                "context_length": 64000, "multimodal": False
            }
        }
    
    def _initialize_model(self):
        """åˆå§‹åŒ–P2Læ¨¡å‹"""
        logger.info("åˆå§‹åŒ–P2Lä»»åŠ¡åˆ†ç±»å™¨...")
        
        # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ä½œä¸ºåŸºç¡€ç¼–ç å™¨
        base_model_name = "sentence-transformers/all-MiniLM-L6-v2"
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            self.model = P2LTaskClassifier(
                base_model_name=base_model_name,
                num_task_types=len(self.task_types),
                num_complexity_levels=len(self.complexity_levels),
                num_languages=len(self.languages)
            )
            
            self.model.to(self.device)
            self.model.eval()
            
            # åˆå§‹åŒ–æƒé‡
            self._initialize_weights()
            
            logger.info(f"âœ… P2Læ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œè®¾å¤‡: {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # é™çº§åˆ°è§„åˆ™æ–¹æ³•
            self.model = None
            self.tokenizer = None
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.model.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„P2Læ¨¡å‹"""
        try:
            logger.info(f"åŠ è½½P2Læ¨¡å‹: {model_path}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # åŠ è½½æ¨¡å‹
            checkpoint = torch.load(os.path.join(model_path, "pytorch_model.bin"), map_location=self.device)
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            self.model = P2LTaskClassifier(
                base_model_name=model_path,
                num_task_types=len(self.task_types),
                num_complexity_levels=len(self.complexity_levels),
                num_languages=len(self.languages)
            )
            
            self.model.load_state_dict(checkpoint)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("âœ… P2Læ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self._initialize_model()  # é™çº§åˆ°åˆå§‹åŒ–æ¨¡å‹
    
    def analyze_prompt(self, prompt: str) -> Dict:
        """
        åˆ†æç”¨æˆ·promptï¼Œæå–ä»»åŠ¡ç‰¹å¾
        """
        if not self.model or not self.tokenizer:
            logger.warning("P2Læ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨è§„åˆ™æ–¹æ³•")
            return self._rule_based_analysis(prompt)
        
        try:
            # é¢„å¤„ç†è¾“å…¥
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                padding=True, 
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # è§£æè¾“å‡º
            task_probs = F.softmax(outputs['task_logits'], dim=-1)[0]
            complexity_probs = F.softmax(outputs['complexity_logits'], dim=-1)[0]
            language_probs = F.softmax(outputs['language_logits'], dim=-1)[0]
            domain_probs = F.softmax(outputs['domain_logits'], dim=-1)[0]
            model_scores = outputs['model_scores'][0]
            
            # è·å–æœ€å¯èƒ½çš„åˆ†ç±»
            task_idx = torch.argmax(task_probs).item()
            complexity_idx = torch.argmax(complexity_probs).item()
            language_idx = torch.argmax(language_probs).item()
            domain_idx = torch.argmax(domain_probs).item()
            
            analysis = {
                "task_type": self.task_types[task_idx],
                "task_confidence": task_probs[task_idx].item(),
                "complexity": self.complexity_levels[complexity_idx],
                "complexity_confidence": complexity_probs[complexity_idx].item(),
                "language": self.languages[language_idx],
                "language_confidence": language_probs[language_idx].item(),
                "domain": self.domains[domain_idx],
                "domain_confidence": domain_probs[domain_idx].item(),
                "length": len(prompt),
                "model_scores": model_scores.cpu().numpy().tolist(),
                "neural_network_used": True
            }
            
            logger.info(f"ğŸ§  P2Lç¥ç»ç½‘ç»œåˆ†æ: {analysis['task_type']}/{analysis['complexity']}/{analysis['language']}")
            return analysis
            
        except Exception as e:
            logger.error(f"P2Læ¨ç†å¤±è´¥: {e}")
            return self._rule_based_analysis(prompt)
    
    def recommend_models(self, prompt: str, priority: str = "performance") -> Dict:
        """
        åŸºäºP2Låˆ†ææ¨èæœ€é€‚åˆçš„æ¨¡å‹
        """
        # åˆ†æä»»åŠ¡ç‰¹å¾
        analysis = self.analyze_prompt(prompt)
        
        # è®¡ç®—æ¨¡å‹åˆ†æ•°
        model_rankings = self._calculate_model_rankings(analysis, priority)
        
        # ç”Ÿæˆæ¨èç»“æœ
        best_model = model_rankings[0]
        
        # ç”Ÿæˆæ¨èç†ç”±
        reasoning = self._generate_reasoning(analysis, best_model, priority)
        
        result = {
            "recommended_model": best_model["model"],
            "confidence": best_model["score"],
            "task_analysis": analysis,
            "reasoning": reasoning,
            "model_rankings": model_rankings[:5],
            "priority_mode": priority,
            "p2l_version": "2.0",
            "inference_method": "neural_network" if analysis.get("neural_network_used") else "rule_based"
        }
        
        return result
    
    def _calculate_model_rankings(self, analysis: Dict, priority: str) -> List[Dict]:
        """è®¡ç®—æ¨¡å‹æ’å"""
        rankings = []
        
        for i, model_name in enumerate(self.llm_models):
            config = self.model_configs[model_name]
            
            # åŸºç¡€åˆ†æ•°
            base_score = config["quality_score"]
            
            # P2Lç¥ç»ç½‘ç»œåˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if "model_scores" in analysis:
                neural_score = analysis["model_scores"][i]
                # å°†ç¥ç»ç½‘ç»œè¾“å‡ºè½¬æ¢ä¸º0-1èŒƒå›´
                neural_score = torch.sigmoid(torch.tensor(neural_score)).item()
                base_score = 0.6 * base_score + 0.4 * neural_score
            
            # ä»»åŠ¡åŒ¹é…åŠ åˆ†
            task_bonus = 0
            if analysis["task_type"] in config["strengths"]:
                task_bonus = 0.15 * analysis.get("task_confidence", 1.0)
            
            # è¯­è¨€åŒ¹é…åŠ åˆ†
            language_bonus = 0
            if analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in config["strengths"]:
                language_bonus = 0.20 * analysis.get("language_confidence", 1.0)
            elif analysis["language"] == "è‹±æ–‡" and "ä¸­æ–‡" not in config["strengths"]:
                language_bonus = 0.10 * analysis.get("language_confidence", 1.0)
            
            # å¤æ‚åº¦åŒ¹é…
            complexity_bonus = 0
            if analysis["complexity"] == "å¤æ‚" and config["quality_score"] > 0.90:
                complexity_bonus = 0.10 * analysis.get("complexity_confidence", 1.0)
            elif analysis["complexity"] == "ç®€å•" and config["avg_response_time"] < 2.0:
                complexity_bonus = 0.05 * analysis.get("complexity_confidence", 1.0)
            
            # ä¼˜å…ˆçº§è°ƒæ•´
            priority_bonus = 0
            if priority == "cost" and config["cost_per_1k"] < 0.01:
                priority_bonus = 0.20
            elif priority == "speed" and config["avg_response_time"] < 2.0:
                priority_bonus = 0.15
            elif priority == "performance" and config["quality_score"] > 0.90:
                priority_bonus = 0.10
            
            final_score = base_score + task_bonus + language_bonus + complexity_bonus + priority_bonus
            final_score = min(final_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼
            
            rankings.append({
                "model": model_name,
                "score": round(final_score, 4),
                "provider": config["provider"],
                "cost_per_1k": config["cost_per_1k"],
                "avg_response_time": config["avg_response_time"],
                "quality_score": config["quality_score"],
                "strengths": config["strengths"]
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        rankings.sort(key=lambda x: x["score"], reverse=True)
        return rankings
    
    def _generate_reasoning(self, analysis: Dict, best_model: Dict, priority: str) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        reasons = []
        
        # ä»»åŠ¡åŒ¹é…
        if analysis["task_type"] in best_model["strengths"]:
            confidence = analysis.get("task_confidence", 1.0)
            reasons.append(f"æ“…é•¿{analysis['task_type']}ä»»åŠ¡ (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # è¯­è¨€åŒ¹é…
        if analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in best_model["strengths"]:
            confidence = analysis.get("language_confidence", 1.0)
            reasons.append(f"ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # å¤æ‚åº¦åŒ¹é…
        if analysis["complexity"] == "å¤æ‚" and best_model["quality_score"] > 0.90:
            confidence = analysis.get("complexity_confidence", 1.0)
            reasons.append(f"é€‚åˆå¤æ‚ä»»åŠ¡ (ç½®ä¿¡åº¦: {confidence:.2f})")
        elif analysis["complexity"] == "ç®€å•" and best_model["avg_response_time"] < 2.0:
            confidence = analysis.get("complexity_confidence", 1.0)
            reasons.append(f"å¿«é€Ÿå¤„ç†ç®€å•ä»»åŠ¡ (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # ä¼˜å…ˆçº§åŒ¹é…
        if priority == "cost" and best_model["cost_per_1k"] < 0.01:
            reasons.append("æˆæœ¬æ•ˆç›Šæœ€ä¼˜")
        elif priority == "speed" and best_model["avg_response_time"] < 2.0:
            reasons.append("å“åº”é€Ÿåº¦æœ€å¿«")
        elif priority == "performance" and best_model["quality_score"] > 0.90:
            reasons.append("æ€§èƒ½è¡¨ç°æœ€ä½³")
        
        # P2Lç¥ç»ç½‘ç»œæ¨ç†
        if analysis.get("neural_network_used"):
            reasons.append("åŸºäºP2Lç¥ç»ç½‘ç»œæ™ºèƒ½åˆ†æ")
        
        return "ï¼›".join(reasons) if reasons else "ç»¼åˆè¯„ä¼°æœ€é€‚åˆ"
    
    def _rule_based_analysis(self, prompt: str) -> Dict:
        """å¤‡ç”¨è§„åˆ™åˆ†ææ–¹æ³•"""
        prompt_lower = prompt.lower()
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        task_confidence = 0.8
        
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            task_type = "ç¼–ç¨‹"
            task_confidence = 0.9
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            task_type = "åˆ›æ„å†™ä½œ"
            task_confidence = 0.85
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french"]):
            task_type = "ç¿»è¯‘"
            task_confidence = 0.9
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation"]):
            task_type = "æ•°å­¦"
            task_confidence = 0.85
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe"]):
            task_type = "åˆ†æ"
            task_confidence = 0.8
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = "ç®€å•"
        complexity_confidence = 0.7
        
        if len(prompt) > 200 or any(word in prompt_lower for word in ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´", "æ·±å…¥"]):
            complexity = "å¤æ‚"
            complexity_confidence = 0.8
        elif len(prompt) > 100:
            complexity = "ä¸­ç­‰"
            complexity_confidence = 0.75
        
        # è¯­è¨€æ£€æµ‹
        language = "è‹±æ–‡"
        language_confidence = 0.8
        
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(prompt) * 0.3:
            language = "ä¸­æ–‡"
            language_confidence = 0.9
        
        # é¢†åŸŸæ£€æµ‹
        domain = "é€šç”¨"
        domain_confidence = 0.7
        
        if any(word in prompt_lower for word in ["tech", "technology", "æŠ€æœ¯", "ç§‘æŠ€"]):
            domain = "æŠ€æœ¯"
            domain_confidence = 0.8
        elif any(word in prompt_lower for word in ["business", "å•†ä¸š", "å¸‚åœº", "è¥é”€"]):
            domain = "å•†ä¸š"
            domain_confidence = 0.8
        
        return {
            "task_type": task_type,
            "task_confidence": task_confidence,
            "complexity": complexity,
            "complexity_confidence": complexity_confidence,
            "language": language,
            "language_confidence": language_confidence,
            "domain": domain,
            "domain_confidence": domain_confidence,
            "length": len(prompt),
            "neural_network_used": False
        }
    
    def save_model(self, save_path: str):
        """ä¿å­˜P2Læ¨¡å‹"""
        if not self.model:
            logger.error("æ²¡æœ‰æ¨¡å‹å¯ä¿å­˜")
            return
        
        try:
            os.makedirs(save_path, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹æƒé‡
            torch.save(self.model.state_dict(), os.path.join(save_path, "pytorch_model.bin"))
            
            # ä¿å­˜tokenizer
            if self.tokenizer:
                self.tokenizer.save_pretrained(save_path)
            
            # ä¿å­˜é…ç½®
            config = {
                "task_types": self.task_types,
                "complexity_levels": self.complexity_levels,
                "languages": self.languages,
                "domains": self.domains,
                "llm_models": self.llm_models
            }
            
            with open(os.path.join(save_path, "p2l_config.json"), "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… P2Læ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜P2Læ¨¡å‹å¤±è´¥: {e}")

# å…¨å±€P2Læ¨ç†å¼•æ“å®ä¾‹
_p2l_engine = None

def get_p2l_engine() -> P2LInferenceEngine:
    """è·å–å…¨å±€P2Læ¨ç†å¼•æ“å®ä¾‹"""
    global _p2l_engine
    if _p2l_engine is None:
        _p2l_engine = P2LInferenceEngine()
    return _p2l_engine

def analyze_prompt_with_p2l(prompt: str) -> Dict:
    """ä½¿ç”¨P2Låˆ†æprompt"""
    engine = get_p2l_engine()
    return engine.analyze_prompt(prompt)

def recommend_models_with_p2l(prompt: str, priority: str = "performance") -> Dict:
    """ä½¿ç”¨P2Læ¨èæ¨¡å‹"""
    engine = get_p2l_engine()
    return engine.recommend_models(prompt, priority)

if __name__ == "__main__":
    # æµ‹è¯•P2Læ¨ç†å¼•æ“
    engine = P2LInferenceEngine()
    
    test_prompts = [
        "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºå‡½æ•°",
        "å¸®æˆ‘ç¿»è¯‘è¿™æ®µè‹±æ–‡åˆ°ä¸­æ–‡",
        "åˆ†æä¸€ä¸‹å½“å‰çš„ç»æµå½¢åŠ¿",
        "åˆ›ä½œä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—æ­Œ",
        "è§£å†³è¿™ä¸ªæ•°å­¦æ–¹ç¨‹ï¼šx^2 + 5x + 6 = 0"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ æµ‹è¯•prompt: {prompt}")
        result = engine.recommend_models(prompt)
        print(f"ğŸ¯ æ¨èæ¨¡å‹: {result['recommended_model']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"ğŸ§  æ¨ç†æ–¹æ³•: {result['inference_method']}")
        print(f"ğŸ’¡ æ¨èç†ç”±: {result['reasoning']}")