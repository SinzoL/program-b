#!/usr/bin/env python3
"""
P2Låç«¯æœåŠ¡ - æ¨¡å—åŒ–ç‰ˆæœ¬
ä½¿ç”¨æ¨¡å—åŒ–æ¶æ„é‡æ„çš„åç«¯æœåŠ¡
"""

import os
import sys
import asyncio
import logging
import time
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
try:
    from modules.config_manager import ConfigManager
    from modules.p2l_engine import P2LEngine
    from modules.task_analyzer import TaskAnalyzer
    from modules.model_scorer import ModelScorer
    from modules.llm_handler import LLMHandler
    logger.info("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger.error(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# è¯·æ±‚æ¨¡å‹
class P2LAnalysisRequest(BaseModel):
    prompt: str
    priority: str = "balanced"
    enabled_models: Optional[List[str]] = None

class LLMRequest(BaseModel):
    model: str
    prompt: str

class P2LInferenceRequest(BaseModel):
    code: str
    max_length: int = 512
    temperature: float = 0.7

# ä¸»æœåŠ¡ç±»
class P2LBackendService:
    """P2Låç«¯æœåŠ¡ - æ¨¡å—åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        # è®¾å¤‡æ£€æµ‹
        self.device = self._detect_device()
        logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.config_manager = ConfigManager()
        self.p2l_engine = P2LEngine(self.device)
        self.task_analyzer = TaskAnalyzer()
        self.model_scorer = ModelScorer(self.config_manager.get_all_models())
        self.llm_handler = LLMHandler()
        
        logger.info("ğŸš€ P2Låç«¯æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _detect_device(self) -> torch.device:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("ğŸš€ æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("ğŸš€ æ£€æµ‹åˆ°MPSï¼Œä½¿ç”¨Apple SiliconåŠ é€Ÿ")
        else:
            device = torch.device("cpu")
            logger.info("ğŸ’» ä½¿ç”¨CPUè¿è¡Œ")
        return device
    
    async def analyze_prompt(self, request: P2LAnalysisRequest) -> Dict:
        """P2Læ™ºèƒ½åˆ†æä¸»æ¥å£"""
        logger.info(f"ğŸ“ æ”¶åˆ°P2Låˆ†æè¯·æ±‚: {request.prompt[:50]}...")
        start_time = time.time()
        
        try:
            # 1. P2Lè¯­ä¹‰åˆ†æ
            complexity_score, language_score = self.p2l_engine.semantic_analysis(request.prompt)
            
            # 2. ä»»åŠ¡åˆ†æ
            task_analysis = self.task_analyzer.analyze_task(
                request.prompt, 
                complexity_score, 
                language_score
            )
            
            # 3. æ¨¡å‹è¯„åˆ†å’Œæ’åº
            model_scores = self.model_scorer.calculate_model_scores(
                task_analysis, 
                request.priority, 
                request.enabled_models
            )
            
            # 4. ç”Ÿæˆæ¨èç†ç”±
            if model_scores:
                best_model = model_scores[0]
                reasoning = self.model_scorer.generate_recommendation_reasoning(
                    best_model, task_analysis, request.priority
                )
            else:
                reasoning = "æ— å¯ç”¨æ¨¡å‹"
            
            processing_time = round(time.time() - start_time, 3)
            
            # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            recommendations = []
            for score_data in model_scores:
                recommendations.append({
                    "model": score_data["model"],
                    "score": score_data["score"],
                    "provider": score_data["config"]["provider"],
                    "cost_per_1k": score_data["config"]["cost_per_1k"],
                    "avg_response_time": score_data["config"]["avg_response_time"],
                    "strengths": score_data["config"]["strengths"],
                    "quality_score": score_data["config"]["quality_score"]
                })
            
            result = {
                "task_analysis": task_analysis,
                "model_ranking": model_scores,
                "recommendations": recommendations,  # å‰ç«¯æœŸæœ›çš„å­—æ®µ
                "recommended_model": model_scores[0]["model"] if model_scores else None,
                "confidence": model_scores[0]["score"] if model_scores else 0,
                "reasoning": reasoning,
                "processing_time": processing_time,
                "device": str(self.device),
                # å…¼å®¹æ—§ç‰ˆæœ¬å‰ç«¯
                "recommendation": {
                    "model": model_scores[0]["model"] if model_scores else None,
                    "score": model_scores[0]["score"] if model_scores else 0,
                    "reasoning": reasoning
                }
            }
            
            logger.info(f"âœ… P2Låˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ P2Låˆ†æå¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"P2Låˆ†æå¤±è´¥: {str(e)}")
    
    async def generate_llm_response(self, request: LLMRequest) -> Dict:
        """LLMå“åº”ç”Ÿæˆæ¥å£"""
        logger.info(f"ğŸ¤– LLMè¯·æ±‚: {request.model}")
        
        try:
            result = await self.llm_handler.generate_response(
                request.model, 
                request.prompt
            )
            return result
            
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
    
    async def p2l_inference(self, request: P2LInferenceRequest) -> Dict:
        """P2Læ¨ç†æ¥å£"""
        logger.info(f"ğŸ§  P2Læ¨ç†è¯·æ±‚")
        
        try:
            result = self.p2l_engine.code_inference(
                request.code,
                request.max_length,
                request.temperature
            )
            return result
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"P2Læ¨ç†å¤±è´¥: {str(e)}")
    
    def get_health_status(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        p2l_models = self.p2l_engine.get_loaded_models()
        llm_info = self.llm_handler.get_client_info()
        
        return {
            "status": "healthy",
            "p2l_models_loaded": len(p2l_models["p2l_models"]),
            "llm_models_available": len(self.config_manager.get_all_models()),
            "device": str(self.device),
            "p2l_available": p2l_models["p2l_available"],
            "llm_client_available": llm_info["llm_client_available"],
            "real_api_enabled": llm_info["real_api_enabled"]
        }

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="P2L Backend Service - Modular", version="2.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æœåŠ¡
service = P2LBackendService()

# APIè·¯ç”±
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return service.get_health_status()

@app.post("/api/p2l/analyze")
async def analyze_prompt(request: P2LAnalysisRequest):
    """P2Læ™ºèƒ½åˆ†ææ¥å£"""
    return await service.analyze_prompt(request)

@app.post("/api/llm/generate")
async def generate_response(request: LLMRequest):
    """LLMå“åº”ç”Ÿæˆæ¥å£"""
    return await service.generate_llm_response(request)

@app.post("/api/p2l/inference")
async def p2l_inference(request: P2LInferenceRequest):
    """P2Læ¨ç†æ¥å£"""
    return await service.p2l_inference(request)

@app.get("/api/models")
async def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return {
        "models": list(service.config_manager.get_all_models().keys()),
        "total": len(service.config_manager.get_all_models())
    }

# å…¼å®¹æ€§è·¯ç”± (ä¿æŒå‘åå…¼å®¹)
@app.post("/analyze")
async def analyze_prompt_compat(request: P2LAnalysisRequest):
    """P2Læ™ºèƒ½åˆ†ææ¥å£ (å…¼å®¹æ€§)"""
    return await service.analyze_prompt(request)

@app.post("/generate")
async def generate_response_compat(request: LLMRequest):
    """LLMå“åº”ç”Ÿæˆæ¥å£ (å…¼å®¹æ€§)"""
    return await service.generate_llm_response(request)

@app.get("/models")
async def get_models_compat():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (å…¼å®¹æ€§)"""
    return {
        "models": list(service.config_manager.get_all_models().keys()),
        "total": len(service.config_manager.get_all_models())
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("ğŸš€ å¯åŠ¨P2Låç«¯æœåŠ¡ (æ¨¡å—åŒ–ç‰ˆæœ¬)")
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")