#!/usr/bin/env python3
"""
P2Låç«¯æœåŠ¡
æä¾›P2Læ™ºèƒ½è·¯ç”±å’ŒLLMè°ƒç”¨çš„å®Œæ•´åç«¯API
"""

import asyncio
import json
import time
import os
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡",
    description="æä¾›P2Låˆ†æå’ŒLLMè°ƒç”¨çš„å®Œæ•´åç«¯API",
    version="2.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000", 
        "http://192.168.117.66:3000",
        "http://192.168.255.10:3000",
        "*"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# è¯·æ±‚æ¨¡å‹
class P2LAnalysisRequest(BaseModel):
    prompt: str
    priority: str = "performance"  # performance, cost, speed, balanced

class LLMGenerateRequest(BaseModel):
    model: str
    prompt: str
    analysis: Optional[Dict] = None

class P2LBackendService:
    def __init__(self):
        self.p2l_models = {}
        self.model_configs = self._load_model_configs()
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å°è¯•åŠ è½½P2Læ¨¡å‹
        self._load_p2l_models()
    
    def _load_model_configs(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            "gpt-4o": {
                "provider": "openai",
                "cost_per_1k": 0.03,
                "avg_response_time": 2.5,
                "strengths": ["ç¼–ç¨‹", "å¤æ‚æ¨ç†", "æ•°å­¦"],
                "quality_score": 0.95
            },
            "gpt-4o-mini": {
                "provider": "openai", 
                "cost_per_1k": 0.0015,
                "avg_response_time": 1.2,
                "strengths": ["å¿«é€Ÿå“åº”", "æˆæœ¬æ•ˆç›Š"],
                "quality_score": 0.82
            },
            "claude-3-5-sonnet-20241022": {
                "provider": "anthropic",
                "cost_per_1k": 0.025,
                "avg_response_time": 2.8,
                "strengths": ["åˆ›æ„å†™ä½œ", "æ–‡å­¦åˆ†æ"],
                "quality_score": 0.93
            },
            "claude-3-5-haiku-20241022": {
                "provider": "anthropic",
                "cost_per_1k": 0.008,
                "avg_response_time": 1.5,
                "strengths": ["å¿«é€Ÿå“åº”", "ç®€æ´å›ç­”"],
                "quality_score": 0.85
            },
            "gemini-1.5-pro-002": {
                "provider": "google",
                "cost_per_1k": 0.02,
                "avg_response_time": 2.2,
                "strengths": ["å¤šæ¨¡æ€", "é•¿æ–‡æœ¬"],
                "quality_score": 0.90
            },
            "gemini-1.5-flash-002": {
                "provider": "google",
                "cost_per_1k": 0.005,
                "avg_response_time": 1.0,
                "strengths": ["å¿«é€Ÿå¤„ç†"],
                "quality_score": 0.80
            },
            "qwen2.5-72b-instruct": {
                "provider": "alibaba",
                "cost_per_1k": 0.015,
                "avg_response_time": 2.0,
                "strengths": ["ä¸­æ–‡ç†è§£", "ä¸­æ–‡åˆ›ä½œ"],
                "quality_score": 0.88
            },
            "llama-3.1-70b-instruct": {
                "provider": "meta",
                "cost_per_1k": 0.01,
                "avg_response_time": 2.3,
                "strengths": ["å¼€æº", "å¯æ§"],
                "quality_score": 0.86
            },
            "deepseek-v3": {
                "provider": "deepseek",
                "cost_per_1k": 0.012,
                "avg_response_time": 1.8,
                "strengths": ["æ•°å­¦", "é€»è¾‘æ¨ç†"],
                "quality_score": 0.87
            }
        }
    
    def _load_p2l_models(self):
        """åŠ è½½å¯ç”¨çš„P2Læ¨¡å‹"""
        models_dir = "./models"
        if not os.path.exists(models_dir):
            logger.warning("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return
        
        for item in os.listdir(models_dir):
            if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                model_path = os.path.join(models_dir, item)
                try:
                    logger.info(f"åŠ è½½P2Læ¨¡å‹: {item}")
                    tokenizer = AutoTokenizer.from_pretrained(model_path)
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float32,
                        device_map=None
                    )
                    model.to(self.device)
                    model.eval()
                    
                    self.p2l_models[item] = {
                        "tokenizer": tokenizer,
                        "model": model,
                        "path": model_path
                    }
                    logger.info(f"âœ… P2Læ¨¡å‹ {item} åŠ è½½æˆåŠŸ")
                    break  # åªåŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
                except Exception as e:
                    logger.error(f"âŒ P2Læ¨¡å‹ {item} åŠ è½½å¤±è´¥: {e}")
    
    def analyze_task(self, prompt: str) -> Dict:
        """ä½¿ç”¨P2Lç¥ç»ç½‘ç»œæ¨¡å‹åˆ†æä»»åŠ¡ç‰¹å¾"""
        # å¦‚æœæœ‰P2Læ¨¡å‹ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¨ç†
        if self.p2l_models:
            try:
                model_name = list(self.p2l_models.keys())[0]
                p2l_model = self.p2l_models[model_name]
                tokenizer = p2l_model["tokenizer"]
                model = p2l_model["model"]
                
                # ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œæ¨ç†
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits = outputs.logits
                    logger.info(f"ğŸ” P2Læ¨¡å‹è¾“å‡ºè°ƒè¯•: logits.shape={logits.shape}, logits={logits}")
                    
                # è§£é‡ŠP2Læ¨¡å‹è¾“å‡º (2ä¸ªè¾“å‡ºï¼šå¤æ‚åº¦å’Œè¯­è¨€ç±»å‹)
                probs = torch.softmax(logits, dim=-1)
                logger.info(f"ğŸ” æ¦‚ç‡åˆ†å¸ƒè°ƒè¯•: probs.shape={probs.shape}, probs={probs}")
                
                # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ - shape: [2]
                sample_probs = probs[0]
                logger.info(f"ğŸ” æ ·æœ¬æ¦‚ç‡è°ƒè¯•: sample_probs.shape={sample_probs.shape}, sample_probs={sample_probs}")
                
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡ºä½œä¸ºå¤æ‚åº¦åˆ†æ•°ï¼Œç¬¬äºŒä¸ªä½œä¸ºè¯­è¨€åˆ†æ•°
                complexity_score = sample_probs[0].item()
                language_score = sample_probs[1].item() if sample_probs.size(0) > 1 else 0.5
                
                # åŸºäºç¥ç»ç½‘ç»œè¾“å‡ºç¡®å®šä»»åŠ¡ç‰¹å¾
                task_analysis = self._interpret_p2l_output(prompt, complexity_score, language_score)
                logger.info(f"ğŸ§  P2Lç¥ç»ç½‘ç»œæ¨ç†: å¤æ‚åº¦={complexity_score:.3f}, è¯­è¨€={language_score:.3f}")
                
                return task_analysis
                
            except Exception as e:
                logger.warning(f"P2Læ¨¡å‹æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™æ–¹æ³•: {e}")
        
        # å¤‡ç”¨è§„åˆ™æ–¹æ³•
        return self._rule_based_analysis(prompt)
    
    def _interpret_p2l_output(self, prompt: str, complexity_score: float, language_score: float) -> Dict:
        """è§£é‡ŠP2Læ¨¡å‹è¾“å‡º"""
        prompt_lower = prompt.lower()
        
        # åŸºäºç¥ç»ç½‘ç»œè¾“å‡ºå’Œæç¤ºè¯å†…å®¹ç¡®å®šä»»åŠ¡ç±»å‹
        task_type = "é€šç”¨"
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            task_type = "ç¼–ç¨‹"
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            task_type = "åˆ›æ„å†™ä½œ"
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french"]):
            task_type = "ç¿»è¯‘"
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation"]):
            task_type = "æ•°å­¦"
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe"]):
            task_type = "åˆ†æ"
        
        # åŸºäºç¥ç»ç½‘ç»œè¾“å‡ºç¡®å®šå¤æ‚åº¦
        if complexity_score > 0.7:
            complexity = "å¤æ‚"
        elif complexity_score < 0.3:
            complexity = "ç®€å•"
        else:
            complexity = "ä¸­ç­‰"
        
        # åŸºäºç¥ç»ç½‘ç»œè¾“å‡ºç¡®å®šè¯­è¨€
        language = "ä¸­æ–‡" if language_score > 0.5 else "è‹±æ–‡"
        
        return {
            "task_type": task_type,
            "complexity": complexity,
            "language": language,
            "length": len(prompt),
            "p2l_scores": {
                "complexity": complexity_score,
                "language": language_score
            }
        }
    
    def _rule_based_analysis(self, prompt: str) -> Dict:
        """å¤‡ç”¨çš„è§„åˆ™åˆ†ææ–¹æ³•"""
        prompt_lower = prompt.lower()
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            task_type = "ç¼–ç¨‹"
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            task_type = "åˆ›æ„å†™ä½œ"
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french"]):
            task_type = "ç¿»è¯‘"
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation"]):
            task_type = "æ•°å­¦"
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe"]):
            task_type = "åˆ†æ"
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = "ç®€å•"
        if len(prompt) > 100 or any(word in prompt_lower for word in ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´"]):
            complexity = "å¤æ‚"
        elif len(prompt) > 50:
            complexity = "ä¸­ç­‰"
        
        # è¯­è¨€æ£€æµ‹
        language = "è‹±æ–‡"
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(prompt) * 0.3:
            language = "ä¸­æ–‡"
        
        return {
            "task_type": task_type,
            "complexity": complexity,
            "language": language,
            "length": len(prompt)
        }
    
    def calculate_model_scores(self, task_analysis: Dict, priority: str) -> List[Dict]:
        """è®¡ç®—æ¨¡å‹åˆ†æ•°å¹¶æ’åº"""
        scores = []
        
        for model_name, config in self.model_configs.items():
            base_score = config["quality_score"]
            
            # ä»»åŠ¡ç±»å‹åŒ¹é…
            task_bonus = 0
            if task_analysis["task_type"] in config["strengths"]:
                task_bonus = 0.15
            
            # è¯­è¨€åŒ¹é…
            language_bonus = 0
            if task_analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in config["strengths"]:
                language_bonus = 0.20
            
            # å¤æ‚åº¦åŒ¹é…
            complexity_bonus = 0
            if task_analysis["complexity"] == "å¤æ‚" and config["quality_score"] > 0.90:
                complexity_bonus = 0.10
            elif task_analysis["complexity"] == "ç®€å•" and config["avg_response_time"] < 2.0:
                complexity_bonus = 0.05
            
            # ä¼˜å…ˆçº§è°ƒæ•´
            priority_bonus = 0
            if priority == "cost" and config["cost_per_1k"] < 0.01:
                priority_bonus = 0.20
            elif priority == "speed" and config["avg_response_time"] < 2.0:
                priority_bonus = 0.15
            elif priority == "performance" and config["quality_score"] > 0.90:
                priority_bonus = 0.10
            
            final_score = base_score + task_bonus + language_bonus + complexity_bonus + priority_bonus
            
            scores.append({
                "model": model_name,
                "score": round(final_score, 4),
                "config": config
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x["score"], reverse=True)
        return scores
    
    async def p2l_analyze(self, request: P2LAnalysisRequest) -> Dict:
        """P2Læ™ºèƒ½åˆ†æ"""
        logger.info(f"P2Låˆ†æè¯·æ±‚: {request.prompt[:50]}...")
        
        # åˆ†æä»»åŠ¡
        task_analysis = self.analyze_task(request.prompt)
        
        # è®¡ç®—æ¨¡å‹åˆ†æ•°
        model_scores = self.calculate_model_scores(task_analysis, request.priority)
        
        # ç”Ÿæˆæ¨èç†ç”±
        best_model = model_scores[0]
        reasoning_parts = []
        
        if task_analysis["task_type"] in best_model["config"]["strengths"]:
            reasoning_parts.append(f"æ“…é•¿{task_analysis['task_type']}ä»»åŠ¡")
        
        if task_analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in best_model["config"]["strengths"]:
            reasoning_parts.append("ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º")
        
        if request.priority == "cost" and best_model["config"]["cost_per_1k"] < 0.01:
            reasoning_parts.append("æˆæœ¬æ•ˆç›Šé«˜")
        elif request.priority == "speed" and best_model["config"]["avg_response_time"] < 2.0:
            reasoning_parts.append("å“åº”é€Ÿåº¦å¿«")
        elif request.priority == "performance" and best_model["config"]["quality_score"] > 0.90:
            reasoning_parts.append("æ€§èƒ½è¡¨ç°ä¼˜ç§€")
        
        reasoning = "ï¼›".join(reasoning_parts) if reasoning_parts else "ç»¼åˆæ€§èƒ½æœ€ä½³"
        
        result = {
            "recommended_model": best_model["model"],
            "confidence": best_model["score"],
            "estimated_cost": f"${best_model['config']['cost_per_1k']}/1K tokens",
            "estimated_time": f"{best_model['config']['avg_response_time']}s",
            "task_analysis": task_analysis,
            "reasoning": reasoning,
            "recommendations": [
                {
                    "model": item["model"], 
                    "score": item["score"],
                    "provider": item["config"]["provider"],
                    "cost_per_1k": item["config"]["cost_per_1k"],
                    "avg_response_time": item["config"]["avg_response_time"],
                    "strengths": item["config"]["strengths"],
                    "quality_score": item["config"]["quality_score"]
                } 
                for item in model_scores[:5]
            ],
            "model_rankings": [
                {"model": item["model"], "score": item["score"]} 
                for item in model_scores[:5]
            ],
            "priority_mode": request.priority
        }
        
        logger.info(f"P2Læ¨è: {result['recommended_model']}")
        return result
    
    async def generate_llm_response(self, request: LLMGenerateRequest) -> Dict:
        """æ¨¡æ‹ŸLLMå“åº”ç”Ÿæˆ"""
        logger.info(f"è°ƒç”¨LLM: {request.model}")
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„å“åº”æ—¶é—´
        model_config = self.model_configs.get(request.model, {})
        response_time = model_config.get("avg_response_time", 2.0)
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(min(response_time * 0.3, 2.0))
        
        # ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        response_content = self._generate_mock_response(request.model, request.prompt)
        
        actual_time = time.time() - start_time
        
        return {
            "model": request.model,
            "content": response_content,
            "response_time": round(actual_time, 2),
            "tokens_used": len(response_content.split()) * 1.3,  # ä¼°ç®—tokenæ•°
            "cost": round(model_config.get("cost_per_1k", 0.02) * len(response_content.split()) * 1.3 / 1000, 4)
        }
    
    def _generate_mock_response(self, model: str, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„LLMå“åº”"""
        if "javascript" in prompt.lower() or "js" in prompt.lower():
            if "gpt-4o" in model:
                return """// é«˜è´¨é‡çš„JavaScriptä¸‹åˆ’çº¿è½¬é©¼å³°å®ç°

/**
 * å°†ä¸‹åˆ’çº¿å‘½åè½¬æ¢ä¸ºé©¼å³°å‘½å
 * @param {string} str - åŒ…å«ä¸‹åˆ’çº¿çš„å­—ç¬¦ä¸²
 * @returns {string} è½¬æ¢åçš„é©¼å³°å‘½åå­—ç¬¦ä¸²
 */
function toCamelCase(str) {
    if (!str || typeof str !== 'string') return '';
    
    return str
        .toLowerCase()
        .split('_')
        .map((word, index) => 
            index === 0 ? word : word.charAt(0).toUpperCase() + word.slice(1)
        )
        .join('');
}

// ä½¿ç”¨ç¤ºä¾‹
console.log(toCamelCase('hello_world_example')); // helloWorldExample
console.log(toCamelCase('user_name')); // userName
console.log(toCamelCase('api_response_data')); // apiResponseData

// å¤„ç†è¾¹ç•Œæƒ…å†µçš„å¢å¼ºç‰ˆæœ¬
function toCamelCaseAdvanced(str) {
    if (!str || typeof str !== 'string') return '';
    
    return str
        .trim()
        .toLowerCase()
        .replace(/[_-]+(.)?/g, (_, char) => char ? char.toUpperCase() : '');
}

// æµ‹è¯•ç”¨ä¾‹
const testCases = [
    'hello_world',
    'user_name_field', 
    'api_response_data',
    '_leading_underscore',
    'trailing_underscore_',
    'multiple___underscores'
];

testCases.forEach(test => {
    console.log(`${test} -> ${toCamelCaseAdvanced(test)}`);
});"""
            elif "claude" in model:
                return """// ä¼˜é›…çš„ä¸‹åˆ’çº¿è½¬é©¼å³°è§£å†³æ–¹æ¡ˆ

// ç®€æ´çš„å®ç°æ–¹å¼
const toCamelCase = str => 
    str.replace(/_([a-z])/g, (_, letter) => letter.toUpperCase());

// æ›´å®Œæ•´çš„å®ç°ï¼Œå¤„ç†å„ç§è¾¹ç•Œæƒ…å†µ
function convertToCamelCase(input) {
    return input
        .toLowerCase()
        .replace(/[^a-zA-Z0-9]+(.)/g, (_, chr) => chr.toUpperCase());
}

// ä½¿ç”¨ç¤ºä¾‹
console.log(toCamelCase('my_variable_name')); // myVariableName
console.log(toCamelCase('hello_world')); // helloWorld
console.log(convertToCamelCase('user-name_field')); // userNameField

// å‡½æ•°å¼ç¼–ç¨‹é£æ ¼
const camelCase = str => str
    .split(/[_-]+/)
    .map((word, i) => i === 0 ? word.toLowerCase() : 
         word.charAt(0).toUpperCase() + word.slice(1).toLowerCase())
    .join('');"""
            else:
                return f"""// {model} ç”Ÿæˆçš„ä¸‹åˆ’çº¿è½¬é©¼å³°å®ç°

function underscoreToCamelCase(inputString) {{
    return inputString
        .split('_')
        .map((word, index) => {{
            if (index === 0) return word.toLowerCase();
            return word.charAt(0).toUpperCase() + word.slice(1).toLowerCase();
        }})
        .join('');
}}

// ä½¿ç”¨ç¤ºä¾‹
const examples = ['hello_world', 'user_name', 'convert_this_string'];
examples.forEach(example => {{
    console.log(`${{example}} -> ${{underscoreToCamelCase(example)}}`);
}});

// è¾“å‡º:
// hello_world -> helloWorld
// user_name -> userName  
// convert_this_string -> convertThisString"""
        
        # å…¶ä»–ç±»å‹çš„å“åº”
        return f"è¿™æ˜¯ {model} å¯¹æ‚¨é—®é¢˜çš„å›ç­”ï¼š\n\n{prompt}\n\n[æ¨¡æ‹Ÿå“åº”å†…å®¹]"

# å…¨å±€æœåŠ¡å®ä¾‹
p2l_service = P2LBackendService()

# APIè·¯ç”±
@app.post("/api/p2l/analyze")
async def analyze_with_p2l(request: P2LAnalysisRequest):
    """P2Læ™ºèƒ½åˆ†æAPI"""
    try:
        result = await p2l_service.p2l_analyze(request)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"P2Låˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/llm/generate")
async def generate_with_llm(request: LLMGenerateRequest):
    """LLMç”ŸæˆAPI"""
    try:
        result = await p2l_service.generate_llm_response(request)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return JSONResponse(content={
        "p2l_models": list(p2l_service.p2l_models.keys()),
        "llm_models": list(p2l_service.model_configs.keys()),
        "total_models": len(p2l_service.model_configs)
    })

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse(content={
        "status": "healthy",
        "p2l_models_loaded": len(p2l_service.p2l_models),
        "llm_models_available": len(p2l_service.model_configs),
        "device": str(p2l_service.device)
    })

# é™æ€æ–‡ä»¶æœåŠ¡
# é™æ€æ–‡ä»¶æœåŠ¡å·²ç§»é™¤ - ä½¿ç”¨ç‹¬ç«‹çš„Vueå‰ç«¯

@app.get("/", response_class=HTMLResponse)
async def serve_root():
    """æ ¹è·¯å¾„ä¿¡æ¯é¡µé¢"""
    return HTMLResponse(
        content="""
        <html>
        <head><title>P2Læ™ºèƒ½è·¯ç”±åç«¯</title></head>
        <body style="font-family: Arial, sans-serif; text-align: center; padding: 50px;">
            <h1>ğŸ§  P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡</h1>
            <p>åç«¯æœåŠ¡è¿è¡Œæ­£å¸¸</p>
            <div style="margin: 20px;">
                <a href="/docs" style="background: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px;">ğŸ“š APIæ–‡æ¡£</a>
                <a href="/api/health" style="background: #28a745; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-left: 10px;">ğŸ” å¥åº·æ£€æŸ¥</a>
            </div>
            <p style="color: #666; margin-top: 30px;">
                å‰ç«¯ç•Œé¢è¯·è®¿é—®: <a href="http://localhost:3000">http://localhost:3000</a>
            </p>
        </body>
        </html>
        """,
        status_code=200
    )

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡...")
    print(f"ğŸ“Š æ”¯æŒ {len(p2l_service.model_configs)} ä¸ªLLMæ¨¡å‹")
    print(f"ğŸ§  åŠ è½½ {len(p2l_service.p2l_models)} ä¸ªP2Læ¨¡å‹")
    print("ğŸŒ å‰ç«¯è®¿é—®: http://localhost:8080")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8080/docs")
    
    uvicorn.run(
        "backend_service:app",
        host="0.0.0.0",
        port=8080,
        reload=False,
        log_level="info"
    )