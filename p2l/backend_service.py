#!/usr/bin/env python3
"""
P2Låç«¯æœåŠ¡
æä¾›P2Læ™ºèƒ½è·¯ç”±å’ŒLLMè°ƒç”¨çš„å®Œæ•´åç«¯API
"""

import asyncio
import json
import time
import os
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn
import logging

# å¯¼å…¥P2Læ¨ç†æ¨¡å—
try:
    from p2l.model import load_model as load_p2l_model, generate_text
    from p2l.p2l_inference import P2LInferenceEngine
    P2L_AVAILABLE = True
except ImportError as e:
    logging.warning(f"P2Læ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    P2L_AVAILABLE = False

# å¯¼å…¥LLMå®¢æˆ·ç«¯
try:
    from llm_client import LLMClient
    LLM_CLIENT_AVAILABLE = True
except ImportError as e:
    logging.warning(f"LLMå®¢æˆ·ç«¯å¯¼å…¥å¤±è´¥: {e}")
    LLM_CLIENT_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡",
    description="æä¾›P2Låˆ†æå’ŒLLMè°ƒç”¨çš„å®Œæ•´åç«¯API",
    version="2.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000", 
        "http://192.168.117.66:3000",
        "http://192.168.255.10:3000",
        "*"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# è¯·æ±‚æ¨¡å‹
class P2LAnalysisRequest(BaseModel):
    prompt: str
    mode: str = "balanced"  # performance, cost, speed, balanced
    models: Optional[List[str]] = None  # å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    priority: str = "performance"  # å…¼å®¹æ—§å­—æ®µ

class LLMGenerateRequest(BaseModel):
    model: str
    prompt: str
    analysis: Optional[Dict] = None

class P2LInferenceRequest(BaseModel):
    code: str
    max_length: Optional[int] = 512
    temperature: Optional[float] = 0.7

class P2LBackendService:
    def __init__(self):
        self.p2l_models = {}
        self.p2l_inference_engine = None
        self.model_configs = self._load_model_configs()
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.llm_client = None
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å°è¯•åŠ è½½P2Læ¨¡å‹
        self._load_p2l_models()
        
        # å°è¯•åŠ è½½P2Læ¨ç†å¼•æ“
        if P2L_AVAILABLE:
            self._load_p2l_inference_engine()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if LLM_CLIENT_AVAILABLE:
            self.llm_client = LLMClient()
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("âŒ LLMå®¢æˆ·ç«¯ä¸å¯ç”¨")
    
    def _load_model_configs(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®ä¿¡æ¯ - åªåŒ…å«æœ‰APIå¯†é’¥çš„ä¸»æµæ¨¡å‹"""
        return {
            # OpenAI ä¸»æµæ¨¡å‹
            "gpt-4o": {
                "provider": "openai",
                "cost_per_1k": 0.03,
                "avg_response_time": 2.5,
                "strengths": ["ç¼–ç¨‹", "å¤æ‚æ¨ç†", "æ•°å­¦"],
                "quality_score": 0.95
            },
            "gpt-4o-mini": {
                "provider": "openai", 
                "cost_per_1k": 0.0015,
                "avg_response_time": 1.2,
                "strengths": ["å¿«é€Ÿå“åº”", "æˆæœ¬æ•ˆç›Š"],
                "quality_score": 0.82
            },
            # Claude ä¸»æµæ¨¡å‹
            "claude-3-5-sonnet-20241022": {
                "provider": "anthropic",
                "cost_per_1k": 0.025,
                "avg_response_time": 2.8,
                "strengths": ["åˆ›æ„å†™ä½œ", "æ–‡å­¦åˆ†æ"],
                "quality_score": 0.93
            },
            "claude-3-7-sonnet-20250219": {
                "provider": "anthropic",
                "cost_per_1k": 0.025,
                "avg_response_time": 2.5,
                "strengths": ["åˆ›æ„å†™ä½œ", "åˆ†æ", "ç¼–ç¨‹", "æ¨ç†"],
                "quality_score": 0.95
            },
            # Gemini ä¸»æµæ¨¡å‹
            "gemini-1.5-pro": {
                "provider": "google",
                "cost_per_1k": 0.015,
                "avg_response_time": 2.0,
                "strengths": ["å¤šæ¨¡æ€", "é•¿æ–‡æœ¬", "æ¨ç†"],
                "quality_score": 0.89
            },
            # DeepSeek ä¸»æµæ¨¡å‹
            "deepseek-chat": {
                "provider": "deepseek",
                "cost_per_1k": 0.002,
                "avg_response_time": 1.8,
                "strengths": ["å¯¹è¯", "ä¸­æ–‡ç†è§£", "å¿«é€Ÿå“åº”"],
                "quality_score": 0.86
            },
            "deepseek-coder": {
                "provider": "deepseek",
                "cost_per_1k": 0.002,
                "avg_response_time": 1.6,
                "strengths": ["ç¼–ç¨‹", "ä»£ç ç”Ÿæˆ", "æŠ€æœ¯é—®ç­”"],
                "quality_score": 0.88
            },
            # åƒé—®ä¸»æµæ¨¡å‹
            "qwen2.5-72b-instruct": {
                "provider": "qwen",
                "cost_per_1k": 0.002,  # çº¦$0.002 (Â¥0.015è½¬æ¢)
                "avg_response_time": 2.0,
                "strengths": ["ä¸­æ–‡ç†è§£", "æ¨ç†", "ç¼–ç¨‹", "æ•°å­¦"],
                "quality_score": 0.90
            },
            "qwen-plus": {
                "provider": "qwen", 
                "cost_per_1k": 0.004,
                "avg_response_time": 2.5,
                "strengths": ["å¤æ‚æ¨ç†", "é•¿æ–‡æœ¬", "å¤šè½®å¯¹è¯"],
                "quality_score": 0.92
            },
            "qwen-turbo": {
                "provider": "qwen",
                "cost_per_1k": 0.001,
                "avg_response_time": 1.0,
                "strengths": ["å¿«é€Ÿå“åº”", "æˆæœ¬æ•ˆç›Š", "æ—¥å¸¸å¯¹è¯"],
                "quality_score": 0.85
            }
        }
    
    def _load_p2l_models(self):
        """åŠ è½½å¯ç”¨çš„P2Læ¨¡å‹"""
        models_dir = "./models"
        if not os.path.exists(models_dir):
            logger.warning("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return
        
        for item in os.listdir(models_dir):
            if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                model_path = os.path.join(models_dir, item)
                try:
                    logger.info(f"åŠ è½½P2Læ¨¡å‹: {item}")
                    tokenizer = AutoTokenizer.from_pretrained(model_path)
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float32,
                        device_map=None
                    )
                    model.to(self.device)
                    model.eval()
                    
                    self.p2l_models[item] = {
                        "tokenizer": tokenizer,
                        "model": model,
                        "path": model_path
                    }
                    logger.info(f"âœ… P2Læ¨¡å‹ {item} åŠ è½½æˆåŠŸ")
                    break  # åªåŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
                except Exception as e:
                    logger.error(f"âŒ P2Læ¨¡å‹ {item} åŠ è½½å¤±è´¥: {e}")
    
    def _load_p2l_inference_engine(self):
        """åŠ è½½P2Læ¨ç†å¼•æ“"""
        try:
            logger.info("æ­£åœ¨åŠ è½½P2Læ¨ç†å¼•æ“...")
            
            # å°è¯•ä»modelsç›®å½•åŠ è½½
            models_dir = "./models"
            p2l_model_path = None
            
            if os.path.exists(models_dir):
                for item in os.listdir(models_dir):
                    if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                        p2l_model_path = os.path.join(models_dir, item)
                        break
            
            # ä½¿ç”¨P2Læ¨ç†å¼•æ“
            if p2l_model_path:
                model, tokenizer = load_p2l_model(p2l_model_path, device=str(self.device))
            else:
                # åˆ›å»ºé»˜è®¤æ¨ç†å¼•æ“
                model, tokenizer = load_p2l_model("", device=str(self.device))
            
            if isinstance(model, P2LInferenceEngine):
                self.p2l_inference_engine = model
                logger.info("âœ… P2Læ¨ç†å¼•æ“åŠ è½½æˆåŠŸ")
            else:
                logger.warning("åŠ è½½çš„ä¸æ˜¯P2Læ¨ç†å¼•æ“ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹")
                
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¼•æ“åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºåŸºæœ¬çš„æ¨ç†å¼•æ“ä½œä¸ºåå¤‡
            try:
                self.p2l_inference_engine = P2LInferenceEngine(device=str(self.device))
                logger.info("âœ… åˆ›å»ºäº†åŸºæœ¬P2Læ¨ç†å¼•æ“")
            except Exception as e2:
                logger.error(f"âŒ åŸºæœ¬P2Læ¨ç†å¼•æ“åˆ›å»ºå¤±è´¥: {e2}")
    
    def analyze_task(self, prompt: str) -> Dict:
        """ä½¿ç”¨P2Lç¥ç»ç½‘ç»œæ¨¡å‹åˆ†æä»»åŠ¡ç‰¹å¾"""
        # ä¼˜å…ˆä½¿ç”¨P2Læ¨ç†å¼•æ“
        if self.p2l_inference_engine:
            try:
                logger.info("ä½¿ç”¨P2Læ¨ç†å¼•æ“è¿›è¡Œä»»åŠ¡åˆ†æ...")
                result = self.p2l_inference_engine.analyze_task_complexity(prompt)
                
                # è½¬æ¢P2Læ¨ç†å¼•æ“çš„è¾“å‡ºæ ¼å¼
                task_analysis = {
                    "task_type": result.get("task_type", "é€šç”¨"),
                    "complexity": result.get("complexity", "ä¸­ç­‰"),
                    "language": result.get("language", "ä¸­æ–‡"),
                    "length": len(prompt),
                    "p2l_scores": {
                        "complexity": result.get("complexity_score", 0.5),
                        "confidence": result.get("confidence", 0.8)
                    }
                }
                
                logger.info(f"ğŸ§  P2Læ¨ç†å¼•æ“åˆ†æ: {task_analysis}")
                return task_analysis
                
            except Exception as e:
                logger.warning(f"P2Læ¨ç†å¼•æ“åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
        
        # å¦‚æœæœ‰ä¼ ç»ŸP2Læ¨¡å‹ï¼Œä½¿ç”¨å¢å¼ºçš„è§„åˆ™+è¯­ä¹‰åˆ†æ
        elif self.p2l_models:
            try:
                model_name = list(self.p2l_models.keys())[0]
                p2l_model = self.p2l_models[model_name]
                tokenizer = p2l_model["tokenizer"]
                model = p2l_model["model"]
                
                logger.info(f"ğŸ§  ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰å¢å¼ºåˆ†æ...")
                
                # ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰ç‰¹å¾æå–
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model(**inputs)
                    
                    # è·å–éšè—çŠ¶æ€ä½œä¸ºè¯­ä¹‰ç‰¹å¾
                    if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                        # ä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
                        hidden_states = outputs.hidden_states[-1]  # [batch_size, seq_len, hidden_size]
                        # å¹³å‡æ± åŒ–å¾—åˆ°å¥å­çº§åˆ«çš„è¡¨ç¤º
                        sentence_embedding = hidden_states.mean(dim=1)  # [batch_size, hidden_size]
                        semantic_features = sentence_embedding[0]  # [hidden_size]
                    else:
                        # å¦‚æœæ²¡æœ‰éšè—çŠ¶æ€ï¼Œä½¿ç”¨logitsçš„ç»Ÿè®¡ç‰¹å¾
                        logits = outputs.logits  # [batch_size, seq_len, vocab_size]
                        # è®¡ç®—logitsçš„ç»Ÿè®¡ç‰¹å¾ä½œä¸ºè¯­ä¹‰è¡¨ç¤º
                        semantic_features = logits.mean(dim=(0, 1))  # [vocab_size] -> å¹³å‡åˆ°æ ‡é‡ç‰¹å¾
                
                # åŸºäºè¯­ä¹‰ç‰¹å¾è®¡ç®—å¤æ‚åº¦å’Œè¯­è¨€åˆ†æ•°
                if semantic_features.dim() == 1:
                    # ä½¿ç”¨è¯­ä¹‰ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
                    feature_mean = semantic_features.mean().item()
                    feature_std = semantic_features.std().item()
                    feature_max = semantic_features.max().item()
                    
                    # å°†ç»Ÿè®¡ç‰¹å¾æ˜ å°„åˆ°0-1èŒƒå›´
                    complexity_score = min(max((feature_std / (abs(feature_mean) + 1e-6)), 0), 1)
                    language_score = min(max((feature_max / (abs(feature_mean) + 1e-6)), 0), 1)
                else:
                    complexity_score = 0.5
                    language_score = 0.5
                
                logger.info(f"ğŸ” è¯­ä¹‰ç‰¹å¾åˆ†æ: mean={feature_mean:.3f}, std={feature_std:.3f}, max={feature_max:.3f}")
                logger.info(f"ğŸ” è®¡ç®—å¾—åˆ†: complexity_score={complexity_score:.3f}, language_score={language_score:.3f}")
                
                # ç»“åˆè§„åˆ™æ–¹æ³•å’Œè¯­ä¹‰åˆ†æ
                task_analysis = self._enhanced_task_analysis(prompt, complexity_score, language_score)
                logger.info(f"ğŸ§  P2Lå¢å¼ºåˆ†æå®Œæˆ: {task_analysis}")
                
                return task_analysis
                
            except Exception as e:
                logger.warning(f"P2Læ¨¡å‹åˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™æ–¹æ³•: {e}")
        
        # å¤‡ç”¨è§„åˆ™æ–¹æ³•
        return self._rule_based_analysis(prompt)
    
    def _enhanced_task_analysis(self, prompt: str, complexity_score: float, language_score: float) -> Dict:
        """å¢å¼ºçš„ä»»åŠ¡åˆ†ææ–¹æ³•ï¼Œç»“åˆè§„åˆ™å’Œè¯­ä¹‰ç‰¹å¾"""
        prompt_lower = prompt.lower()
        
        # æ›´ç²¾ç¡®çš„ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        confidence = 0.5
        
        # ç¼–ç¨‹ç›¸å…³å…³é”®è¯æ£€æµ‹ï¼ˆæƒé‡æ›´é«˜ï¼‰
        programming_keywords = [
            "code", "python", "javascript", "js", "function", "method", "class",
            "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "æ–¹æ³•", "ç±»", "ç®—æ³•", "å®ç°",
            "ä¸‹åˆ’çº¿", "é©¼å³°", "camelcase", "underscore", "è½¬æ¢", "è½¬åŒ–",
            "å˜é‡", "å‘½å", "æ ¼å¼", "string", "å­—ç¬¦ä¸²"
        ]
        programming_score = sum(1 for word in programming_keywords if word in prompt_lower)
        
        # åˆ›æ„å†™ä½œå…³é”®è¯
        creative_keywords = ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ", "å°è¯´", "æ•£æ–‡"]
        creative_score = sum(1 for word in creative_keywords if word in prompt_lower)
        
        # ç¿»è¯‘å…³é”®è¯
        translation_keywords = ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french", "è¯­è¨€", "è½¬è¯‘"]
        translation_score = sum(1 for word in translation_keywords if word in prompt_lower)
        
        # æ•°å­¦å…³é”®è¯
        math_keywords = ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation", "å…¬å¼", "æ±‚è§£"]
        math_score = sum(1 for word in math_keywords if word in prompt_lower)
        
        # åˆ†æå…³é”®è¯
        analysis_keywords = ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe", "æè¿°", "è¯„ä»·"]
        analysis_score = sum(1 for word in analysis_keywords if word in prompt_lower)
        
        # ç¡®å®šä»»åŠ¡ç±»å‹å’Œç½®ä¿¡åº¦
        scores = {
            "ç¼–ç¨‹": programming_score,
            "åˆ›æ„å†™ä½œ": creative_score,
            "ç¿»è¯‘": translation_score,
            "æ•°å­¦": math_score,
            "åˆ†æ": analysis_score
        }
        
        max_score = max(scores.values())
        if max_score > 0:
            task_type = max(scores, key=scores.get)
            confidence = min(0.9, 0.5 + max_score * 0.1)
        
        # ç‰¹æ®Šæ¨¡å¼æ£€æµ‹ï¼šä¸‹åˆ’çº¿è½¬é©¼å³°
        if any(word in prompt_lower for word in ["ä¸‹åˆ’çº¿", "é©¼å³°", "camelcase", "underscore"]):
            task_type = "ç¼–ç¨‹"
            confidence = 0.95
            logger.info("ğŸ¯ æ£€æµ‹åˆ°å­—ç¬¦ä¸²æ ¼å¼è½¬æ¢ä»»åŠ¡ï¼Œé«˜ç½®ä¿¡åº¦è¯†åˆ«ä¸ºç¼–ç¨‹ç±»å‹")
        
        # åŸºäºè¯­ä¹‰ç‰¹å¾å’Œå…³é”®è¯è°ƒæ•´å¤æ‚åº¦
        base_complexity = complexity_score
        if task_type == "ç¼–ç¨‹" and max_score >= 2:
            base_complexity = max(base_complexity, 0.6)  # ç¼–ç¨‹ä»»åŠ¡é€šå¸¸è¾ƒå¤æ‚
        
        if base_complexity > 0.7:
            complexity = "å¤æ‚"
        elif base_complexity < 0.3:
            complexity = "ç®€å•"
        else:
            complexity = "ä¸­ç­‰"
        
        # è¯­è¨€æ£€æµ‹ï¼ˆä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼‰
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        total_chars = len(prompt)
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
        
        language = "ä¸­æ–‡" if chinese_ratio > 0.3 else "è‹±æ–‡"
        
        result = {
            "task_type": task_type,
            "complexity": complexity,
            "language": language,
            "length": len(prompt),
            "confidence": confidence,
            "p2l_scores": {
                "complexity": base_complexity,
                "language": language_score,
                "keyword_scores": scores,
                "chinese_ratio": chinese_ratio
            }
        }
        
        logger.info(f"ğŸ“Š ä»»åŠ¡åˆ†æè¯¦æƒ…: {result}")
        return result

    def _interpret_p2l_output(self, prompt: str, complexity_score: float, language_score: float) -> Dict:
        """è§£é‡ŠP2Læ¨¡å‹è¾“å‡ºï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return self._enhanced_task_analysis(prompt, complexity_score, language_score)
    
    def _rule_based_analysis(self, prompt: str) -> Dict:
        """å¤‡ç”¨çš„è§„åˆ™åˆ†ææ–¹æ³•"""
        prompt_lower = prompt.lower()
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            task_type = "ç¼–ç¨‹"
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            task_type = "åˆ›æ„å†™ä½œ"
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french"]):
            task_type = "ç¿»è¯‘"
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation"]):
            task_type = "æ•°å­¦"
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe"]):
            task_type = "åˆ†æ"
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = "ç®€å•"
        if len(prompt) > 100 or any(word in prompt_lower for word in ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´"]):
            complexity = "å¤æ‚"
        elif len(prompt) > 50:
            complexity = "ä¸­ç­‰"
        
        # è¯­è¨€æ£€æµ‹
        language = "è‹±æ–‡"
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(prompt) * 0.3:
            language = "ä¸­æ–‡"
        
        return {
            "task_type": task_type,
            "complexity": complexity,
            "language": language,
            "length": len(prompt)
        }
    
    def calculate_model_scores(self, task_analysis: Dict, priority: str, enabled_models: Optional[List[str]] = None) -> List[Dict]:
        """è®¡ç®—æ¨¡å‹åˆ†æ•°å¹¶æ’åº - ä½¿ç”¨ç™¾åˆ†åˆ¶è¯„åˆ†"""
        scores = []
        
        # å¦‚æœæŒ‡å®šäº†å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œåªè®¡ç®—è¿™äº›æ¨¡å‹çš„åˆ†æ•°
        models_to_score = self.model_configs.items()
        if enabled_models:
            models_to_score = [(name, config) for name, config in self.model_configs.items() if name in enabled_models]
            logger.info(f"åªè®¡ç®—å¯ç”¨æ¨¡å‹çš„åˆ†æ•°: {[name for name, _ in models_to_score]}")
        
        for model_name, config in models_to_score:
            # åŸºç¡€åˆ†æ•° (40åˆ†)
            base_score = config["quality_score"] * 40
            
            # ä»»åŠ¡åŒ¹é…åº¦ (25åˆ†)
            task_score = 0
            if task_analysis["task_type"] in config["strengths"]:
                task_score = 25
            elif any(strength in task_analysis["task_type"] for strength in config["strengths"]):
                task_score = 15
            else:
                task_score = 5
            
            # è¯­è¨€åŒ¹é…åº¦ (15åˆ†)
            language_score = 0
            if task_analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in config["strengths"]:
                language_score = 15
            elif task_analysis["language"] == "ä¸­æ–‡":
                language_score = 8
            else:
                language_score = 10
            
            # ä¼˜å…ˆçº§åŒ¹é…åº¦ (20åˆ†)
            priority_score = 0
            if priority == "cost":
                if config["cost_per_1k"] < 0.005:
                    priority_score = 20
                elif config["cost_per_1k"] < 0.015:
                    priority_score = 15
                else:
                    priority_score = 8
            elif priority == "speed":
                if config["avg_response_time"] < 1.5:
                    priority_score = 20
                elif config["avg_response_time"] < 2.5:
                    priority_score = 15
                else:
                    priority_score = 8
            elif priority == "performance":
                if config["quality_score"] > 0.90:
                    priority_score = 20
                elif config["quality_score"] > 0.85:
                    priority_score = 15
                else:
                    priority_score = 10
            else:  # balanced
                priority_score = 15
            
            # æ€»åˆ† = åŸºç¡€åˆ† + ä»»åŠ¡åˆ† + è¯­è¨€åˆ† + ä¼˜å…ˆçº§åˆ† (æ»¡åˆ†100)
            final_score = base_score + task_score + language_score + priority_score
            
            # ç¡®ä¿åˆ†æ•°åœ¨0-100ä¹‹é—´
            final_score = max(0, min(100, final_score))
            
            scores.append({
                "model": model_name,
                "score": round(final_score, 1),
                "config": config
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x["score"], reverse=True)
        return scores
    
    async def p2l_analyze(self, request: P2LAnalysisRequest) -> Dict:
        """P2Læ™ºèƒ½åˆ†æ"""
        logger.info(f"P2Låˆ†æè¯·æ±‚: {request.prompt[:50]}...")
        logger.info(f"å¯ç”¨çš„æ¨¡å‹: {request.models}")
        
        # åˆ†æä»»åŠ¡
        task_analysis = self.analyze_task(request.prompt)
        
        # ä½¿ç”¨modeå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨priorityå­—æ®µ
        priority_mode = request.mode or request.priority
        
        # è®¡ç®—æ‰€æœ‰æ¨¡å‹çš„åˆ†æ•°ï¼ˆä¸å†é™åˆ¶ä¸ºå¯ç”¨çš„æ¨¡å‹ï¼‰
        model_scores = self.calculate_model_scores(task_analysis, priority_mode, enabled_models=None)
        
        # ç”Ÿæˆæ¨èç†ç”±
        best_model = model_scores[0]
        reasoning_parts = []
        
        if task_analysis["task_type"] in best_model["config"]["strengths"]:
            reasoning_parts.append(f"æ“…é•¿{task_analysis['task_type']}ä»»åŠ¡")
        
        if task_analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in best_model["config"]["strengths"]:
            reasoning_parts.append("ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º")
        
        if request.priority == "cost" and best_model["config"]["cost_per_1k"] < 0.01:
            reasoning_parts.append("æˆæœ¬æ•ˆç›Šé«˜")
        elif request.priority == "speed" and best_model["config"]["avg_response_time"] < 2.0:
            reasoning_parts.append("å“åº”é€Ÿåº¦å¿«")
        elif request.priority == "performance" and best_model["config"]["quality_score"] > 0.90:
            reasoning_parts.append("æ€§èƒ½è¡¨ç°ä¼˜ç§€")
        
        reasoning = "ï¼›".join(reasoning_parts) if reasoning_parts else "ç»¼åˆæ€§èƒ½æœ€ä½³"
        
        result = {
            "recommended_model": best_model["model"],
            "confidence": best_model["score"],
            "estimated_cost": f"${best_model['config']['cost_per_1k']}/1K tokens",
            "estimated_time": f"{best_model['config']['avg_response_time']}s",
            "task_analysis": task_analysis,
            "reasoning": reasoning,
            "recommendations": [
                {
                    "model": item["model"], 
                    "score": item["score"],
                    "provider": item["config"]["provider"],
                    "cost_per_1k": item["config"]["cost_per_1k"],
                    "avg_response_time": item["config"]["avg_response_time"],
                    "strengths": item["config"]["strengths"],
                    "quality_score": item["config"]["quality_score"]
                } 
                for item in model_scores
            ],
            "model_rankings": [
                {"model": item["model"], "score": item["score"]} 
                for item in model_scores
            ],
            "priority_mode": request.priority
        }
        
        logger.info(f"P2Læ¨è: {result['recommended_model']}")
        return result
    
    async def generate_llm_response(self, request: LLMGenerateRequest) -> Dict:
        """çœŸå®LLMå“åº”ç”Ÿæˆ"""
        logger.info(f"è°ƒç”¨LLM: {request.model}")
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯å¤„ç†æ‰€æœ‰æ¨¡å‹
            if self.llm_client:
                async with self.llm_client as client:
                    llm_response = await client.generate_response(
                        model=request.model,
                        prompt=request.prompt,
                        max_tokens=2000,
                        temperature=0.7
                    )
                    
                    logger.info(f"âœ… çœŸå®APIè°ƒç”¨æˆåŠŸ: {request.model}")
                    
                    return {
                        "model": llm_response.model,
                        "response": llm_response.content,
                        "content": llm_response.content,
                        "response_time": round(llm_response.response_time, 2),
                        "tokens": llm_response.tokens_used,
                        "tokens_used": llm_response.tokens_used,
                        "cost": round(llm_response.cost, 4),
                        "provider": llm_response.provider,
                        "is_real_api": True
                    }
            else:
                raise Exception("LLMå®¢æˆ·ç«¯ä¸å¯ç”¨")
                
        except Exception as e:
            logger.error(f"âŒ LLM APIè°ƒç”¨å¤±è´¥: {request.model} - {e}")
            return {
                "model": request.model,
                "response": f"æŠ±æ­‰ï¼Œ{request.model} æ¨¡å‹APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "content": f"æŠ±æ­‰ï¼Œ{request.model} æ¨¡å‹APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "response_time": 0.1,
                "tokens": 0,
                "tokens_used": 0,
                "cost": 0.0,
                "provider": "error",
                "is_real_api": False
            }
    


# å…¨å±€æœåŠ¡å®ä¾‹
p2l_service = P2LBackendService()

# APIè·¯ç”±
@app.post("/api/p2l/analyze")
async def analyze_with_p2l(request: P2LAnalysisRequest):
    """P2Læ™ºèƒ½åˆ†æAPI"""
    try:
        result = await p2l_service.p2l_analyze(request)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"P2Låˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/p2l/inference")
async def p2l_inference(request: P2LInferenceRequest):
    """P2Lä»£ç æ¨ç†API - å°†ä»£ç è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€"""
    try:
        if not p2l_service.p2l_inference_engine:
            raise HTTPException(status_code=503, detail="P2Læ¨ç†å¼•æ“æœªåŠ è½½")
        
        logger.info(f"P2Læ¨ç†è¯·æ±‚: {request.code[:100]}...")
        
        # ä½¿ç”¨P2Læ¨ç†å¼•æ“
        result = p2l_service.p2l_inference_engine.infer(
            request.code,
            max_length=request.max_length,
            temperature=request.temperature
        )
        
        return JSONResponse(content={
            "code": request.code,
            "natural_language": result["natural_language"],
            "confidence": result.get("confidence", 0.8),
            "processing_time": result.get("processing_time", 0.0),
            "model_info": "P2L-Inference-Engine"
        })
        
    except Exception as e:
        logger.error(f"P2Læ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/llm/generate")
async def generate_with_llm(request: LLMGenerateRequest):
    """LLMç”ŸæˆAPI"""
    try:
        result = await p2l_service.generate_llm_response(request)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return JSONResponse(content={
        "p2l_models": list(p2l_service.p2l_models.keys()),
        "llm_models": list(p2l_service.model_configs.keys()),
        "total_models": len(p2l_service.model_configs)
    })

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse(content={
        "status": "healthy",
        "p2l_models_loaded": len(p2l_service.p2l_models),
        "p2l_inference_engine": p2l_service.p2l_inference_engine is not None,
        "llm_models_available": len(p2l_service.model_configs),
        "device": str(p2l_service.device),
        "p2l_available": P2L_AVAILABLE,
        "llm_client_available": LLM_CLIENT_AVAILABLE,
        "real_api_enabled": p2l_service.llm_client is not None
    })

# é™æ€æ–‡ä»¶æœåŠ¡
# é™æ€æ–‡ä»¶æœåŠ¡å·²ç§»é™¤ - ä½¿ç”¨ç‹¬ç«‹çš„Vueå‰ç«¯

@app.get("/", response_class=HTMLResponse)
async def serve_root():
    """æ ¹è·¯å¾„ä¿¡æ¯é¡µé¢"""
    return HTMLResponse(
        content="""
        <html>
        <head><title>P2Læ™ºèƒ½è·¯ç”±åç«¯</title></head>
        <body style="font-family: Arial, sans-serif; text-align: center; padding: 50px;">
            <h1>ğŸ§  P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡</h1>
            <p>åç«¯æœåŠ¡è¿è¡Œæ­£å¸¸</p>
            <div style="margin: 20px;">
                <a href="/docs" style="background: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px;">ğŸ“š APIæ–‡æ¡£</a>
                <a href="/api/health" style="background: #28a745; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-left: 10px;">ğŸ” å¥åº·æ£€æŸ¥</a>
            </div>
            <p style="color: #666; margin-top: 30px;">
                å‰ç«¯ç•Œé¢è¯·è®¿é—®: <a href="http://localhost:3000">http://localhost:3000</a>
            </p>
        </body>
        </html>
        """,
        status_code=200
    )

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡...")
    print(f"ğŸ“Š æ”¯æŒ {len(p2l_service.model_configs)} ä¸ªLLMæ¨¡å‹")
    print(f"ğŸ§  åŠ è½½ {len(p2l_service.p2l_models)} ä¸ªP2Læ¨¡å‹")
    print("ğŸŒ å‰ç«¯è®¿é—®: http://localhost:8080")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8080/docs")
    
    uvicorn.run(
        "backend_service:app",
        host="0.0.0.0",
        port=8080,
        reload=False,
        log_level="info"
    )