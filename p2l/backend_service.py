#!/usr/bin/env python3
"""
P2Låç«¯æœåŠ¡
æä¾›P2Læ™ºèƒ½è·¯ç”±å’ŒLLMè°ƒç”¨çš„å®Œæ•´åç«¯API
"""

import asyncio
import json
import time
import os
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn
import logging

# å¯¼å…¥P2Læ¨ç†æ¨¡å—
try:
    from p2l.model import load_model as load_p2l_model, generate_text
    from p2l.p2l_inference import P2LInferenceEngine
    P2L_AVAILABLE = True
except ImportError as e:
    logging.warning(f"P2Læ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    P2L_AVAILABLE = False

# å¯¼å…¥LLMå®¢æˆ·ç«¯
try:
    from llm_client import LLMClient
    LLM_CLIENT_AVAILABLE = True
except ImportError as e:
    logging.warning(f"LLMå®¢æˆ·ç«¯å¯¼å…¥å¤±è´¥: {e}")
    LLM_CLIENT_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡",
    description="æä¾›P2Låˆ†æå’ŒLLMè°ƒç”¨çš„å®Œæ•´åç«¯API",
    version="2.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000", 
        "http://192.168.117.66:3000",
        "http://192.168.255.10:3000",
        "*"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# è¯·æ±‚æ¨¡å‹
class P2LAnalysisRequest(BaseModel):
    prompt: str
    mode: str = "balanced"  # performance, cost, speed, balanced
    models: Optional[List[str]] = None  # å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    priority: str = "performance"  # å…¼å®¹æ—§å­—æ®µ

class LLMGenerateRequest(BaseModel):
    model: str
    prompt: str
    analysis: Optional[Dict] = None

class P2LInferenceRequest(BaseModel):
    code: str
    max_length: Optional[int] = 512
    temperature: Optional[float] = 0.7

class P2LBackendService:
    def __init__(self):
        self.p2l_models = {}
        self.p2l_inference_engine = None
        self.model_configs = self._load_model_configs()
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.llm_client = None
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å°è¯•åŠ è½½P2Læ¨¡å‹
        self._load_p2l_models()
        
        # å°è¯•åŠ è½½P2Læ¨ç†å¼•æ“
        if P2L_AVAILABLE:
            self._load_p2l_inference_engine()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if LLM_CLIENT_AVAILABLE:
            self.llm_client = LLMClient()
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("âŒ LLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
    
    def _load_model_configs(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®ä¿¡æ¯ - åªåŒ…å«æœ‰APIå¯†é’¥çš„ä¸»æµæ¨¡å‹"""
        return {
            # OpenAI ä¸»æµæ¨¡å‹
            "gpt-4o": {
                "provider": "openai",
                "cost_per_1k": 0.03,
                "avg_response_time": 2.5,
                "strengths": ["ç¼–ç¨‹", "å¤æ‚æ¨ç†", "æ•°å­¦"],
                "quality_score": 0.95
            },
            "gpt-4o-mini": {
                "provider": "openai", 
                "cost_per_1k": 0.0015,
                "avg_response_time": 1.2,
                "strengths": ["å¿«é€Ÿå“åº”", "æˆæœ¬æ•ˆç›Š"],
                "quality_score": 0.82
            },
            # Claude ä¸»æµæ¨¡å‹
            "claude-3-5-sonnet-20241022": {
                "provider": "anthropic",
                "cost_per_1k": 0.025,
                "avg_response_time": 2.8,
                "strengths": ["åˆ›æ„å†™ä½œ", "æ–‡å­¦åˆ†æ"],
                "quality_score": 0.93
            },
            # Gemini ä¸»æµæ¨¡å‹
            "gemini-1.5-pro": {
                "provider": "google",
                "cost_per_1k": 0.015,
                "avg_response_time": 2.0,
                "strengths": ["å¤šæ¨¡æ€", "é•¿æ–‡æœ¬", "æ¨ç†"],
                "quality_score": 0.89
            },
            # DeepSeek ä¸»æµæ¨¡å‹
            "deepseek-chat": {
                "provider": "deepseek",
                "cost_per_1k": 0.002,
                "avg_response_time": 1.8,
                "strengths": ["å¯¹è¯", "ä¸­æ–‡ç†è§£", "å¿«é€Ÿå“åº”"],
                "quality_score": 0.86
            },
            "deepseek-coder": {
                "provider": "deepseek",
                "cost_per_1k": 0.002,
                "avg_response_time": 1.6,
                "strengths": ["ç¼–ç¨‹", "ä»£ç ç”Ÿæˆ", "æŠ€æœ¯é—®ç­”"],
                "quality_score": 0.88
            },
            # åƒé—®ä¸»æµæ¨¡å‹
            "qwen2.5-72b-instruct": {
                "provider": "qwen",
                "cost_per_1k": 0.002,  # çº¦$0.002 (Â¥0.015è½¬æ¢)
                "avg_response_time": 2.0,
                "strengths": ["ä¸­æ–‡ç†è§£", "æ¨ç†", "ç¼–ç¨‹", "æ•°å­¦"],
                "quality_score": 0.90
            },
            "qwen-plus": {
                "provider": "qwen", 
                "cost_per_1k": 0.004,
                "avg_response_time": 2.5,
                "strengths": ["å¤æ‚æ¨ç†", "é•¿æ–‡æœ¬", "å¤šè½®å¯¹è¯"],
                "quality_score": 0.92
            },
            "qwen-turbo": {
                "provider": "qwen",
                "cost_per_1k": 0.001,
                "avg_response_time": 1.0,
                "strengths": ["å¿«é€Ÿå“åº”", "æˆæœ¬æ•ˆç›Š", "æ—¥å¸¸å¯¹è¯"],
                "quality_score": 0.85
            }
        }
    
    def _load_p2l_models(self):
        """åŠ è½½å¯ç”¨çš„P2Læ¨¡å‹"""
        models_dir = "./models"
        if not os.path.exists(models_dir):
            logger.warning("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return
        
        for item in os.listdir(models_dir):
            if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                model_path = os.path.join(models_dir, item)
                try:
                    logger.info(f"åŠ è½½P2Læ¨¡å‹: {item}")
                    tokenizer = AutoTokenizer.from_pretrained(model_path)
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float32,
                        device_map=None
                    )
                    model.to(self.device)
                    model.eval()
                    
                    self.p2l_models[item] = {
                        "tokenizer": tokenizer,
                        "model": model,
                        "path": model_path
                    }
                    logger.info(f"âœ… P2Læ¨¡å‹ {item} åŠ è½½æˆåŠŸ")
                    break  # åªåŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
                except Exception as e:
                    logger.error(f"âŒ P2Læ¨¡å‹ {item} åŠ è½½å¤±è´¥: {e}")
    
    def _load_p2l_inference_engine(self):
        """åŠ è½½P2Læ¨ç†å¼•æ“"""
        try:
            logger.info("æ­£åœ¨åŠ è½½P2Læ¨ç†å¼•æ“...")
            
            # å°è¯•ä»modelsç›®å½•åŠ è½½
            models_dir = "./models"
            p2l_model_path = None
            
            if os.path.exists(models_dir):
                for item in os.listdir(models_dir):
                    if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                        p2l_model_path = os.path.join(models_dir, item)
                        break
            
            # ä½¿ç”¨P2Læ¨ç†å¼•æ“
            if p2l_model_path:
                model, tokenizer = load_p2l_model(p2l_model_path, device=str(self.device))
            else:
                # åˆ›å»ºé»˜è®¤æ¨ç†å¼•æ“
                model, tokenizer = load_p2l_model("", device=str(self.device))
            
            if isinstance(model, P2LInferenceEngine):
                self.p2l_inference_engine = model
                logger.info("âœ… P2Læ¨ç†å¼•æ“åŠ è½½æˆåŠŸ")
            else:
                logger.warning("åŠ è½½çš„ä¸æ˜¯P2Læ¨ç†å¼•æ“ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹")
                
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¼•æ“åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºåŸºæœ¬çš„æ¨ç†å¼•æ“ä½œä¸ºåå¤‡
            try:
                self.p2l_inference_engine = P2LInferenceEngine(device=str(self.device))
                logger.info("âœ… åˆ›å»ºäº†åŸºæœ¬P2Læ¨ç†å¼•æ“")
            except Exception as e2:
                logger.error(f"âŒ åŸºæœ¬P2Læ¨ç†å¼•æ“åˆ›å»ºå¤±è´¥: {e2}")
    
    def analyze_task(self, prompt: str) -> Dict:
        """ä½¿ç”¨P2Lç¥ç»ç½‘ç»œæ¨¡å‹åˆ†æä»»åŠ¡ç‰¹å¾"""
        # ä¼˜å…ˆä½¿ç”¨P2Læ¨ç†å¼•æ“
        if self.p2l_inference_engine:
            try:
                logger.info("ä½¿ç”¨P2Læ¨ç†å¼•æ“è¿›è¡Œä»»åŠ¡åˆ†æ...")
                result = self.p2l_inference_engine.analyze_task_complexity(prompt)
                
                # è½¬æ¢P2Læ¨ç†å¼•æ“çš„è¾“å‡ºæ ¼å¼
                task_analysis = {
                    "task_type": result.get("task_type", "é€šç”¨"),
                    "complexity": result.get("complexity", "ä¸­ç­‰"),
                    "language": result.get("language", "ä¸­æ–‡"),
                    "length": len(prompt),
                    "p2l_scores": {
                        "complexity": result.get("complexity_score", 0.5),
                        "confidence": result.get("confidence", 0.8)
                    }
                }
                
                logger.info(f"ğŸ§  P2Læ¨ç†å¼•æ“åˆ†æ: {task_analysis}")
                return task_analysis
                
            except Exception as e:
                logger.warning(f"P2Læ¨ç†å¼•æ“åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
        
        # å¦‚æœæœ‰ä¼ ç»ŸP2Læ¨¡å‹ï¼Œä½¿ç”¨å¢å¼ºçš„è§„åˆ™+è¯­ä¹‰åˆ†æ
        elif self.p2l_models:
            try:
                model_name = list(self.p2l_models.keys())[0]
                p2l_model = self.p2l_models[model_name]
                tokenizer = p2l_model["tokenizer"]
                model = p2l_model["model"]
                
                logger.info(f"ğŸ§  ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰å¢å¼ºåˆ†æ...")
                
                # ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰ç‰¹å¾æå–
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model(**inputs)
                    
                    # è·å–éšè—çŠ¶æ€ä½œä¸ºè¯­ä¹‰ç‰¹å¾
                    if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                        # ä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
                        hidden_states = outputs.hidden_states[-1]  # [batch_size, seq_len, hidden_size]
                        # å¹³å‡æ± åŒ–å¾—åˆ°å¥å­çº§åˆ«çš„è¡¨ç¤º
                        sentence_embedding = hidden_states.mean(dim=1)  # [batch_size, hidden_size]
                        semantic_features = sentence_embedding[0]  # [hidden_size]
                    else:
                        # å¦‚æœæ²¡æœ‰éšè—çŠ¶æ€ï¼Œä½¿ç”¨logitsçš„ç»Ÿè®¡ç‰¹å¾
                        logits = outputs.logits  # [batch_size, seq_len, vocab_size]
                        # è®¡ç®—logitsçš„ç»Ÿè®¡ç‰¹å¾ä½œä¸ºè¯­ä¹‰è¡¨ç¤º
                        semantic_features = logits.mean(dim=(0, 1))  # [vocab_size] -> å¹³å‡åˆ°æ ‡é‡ç‰¹å¾
                
                # åŸºäºè¯­ä¹‰ç‰¹å¾è®¡ç®—å¤æ‚åº¦å’Œè¯­è¨€åˆ†æ•°
                if semantic_features.dim() == 1:
                    # ä½¿ç”¨è¯­ä¹‰ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
                    feature_mean = semantic_features.mean().item()
                    feature_std = semantic_features.std().item()
                    feature_max = semantic_features.max().item()
                    
                    # å°†ç»Ÿè®¡ç‰¹å¾æ˜ å°„åˆ°0-1èŒƒå›´
                    complexity_score = min(max((feature_std / (abs(feature_mean) + 1e-6)), 0), 1)
                    language_score = min(max((feature_max / (abs(feature_mean) + 1e-6)), 0), 1)
                else:
                    complexity_score = 0.5
                    language_score = 0.5
                
                logger.info(f"ğŸ” è¯­ä¹‰ç‰¹å¾åˆ†æ: mean={feature_mean:.3f}, std={feature_std:.3f}, max={feature_max:.3f}")
                logger.info(f"ğŸ” è®¡ç®—å¾—åˆ†: complexity_score={complexity_score:.3f}, language_score={language_score:.3f}")
                
                # ç»“åˆè§„åˆ™æ–¹æ³•å’Œè¯­ä¹‰åˆ†æ
                task_analysis = self._enhanced_task_analysis(prompt, complexity_score, language_score)
                logger.info(f"ğŸ§  P2Lå¢å¼ºåˆ†æå®Œæˆ: {task_analysis}")
                
                return task_analysis
                
            except Exception as e:
                logger.warning(f"P2Læ¨¡å‹åˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™æ–¹æ³•: {e}")
        
        # å¤‡ç”¨è§„åˆ™æ–¹æ³•
        return self._rule_based_analysis(prompt)
    
    def _enhanced_task_analysis(self, prompt: str, complexity_score: float, language_score: float) -> Dict:
        """å¢å¼ºçš„ä»»åŠ¡åˆ†ææ–¹æ³•ï¼Œç»“åˆè§„åˆ™å’Œè¯­ä¹‰ç‰¹å¾"""
        prompt_lower = prompt.lower()
        
        # æ›´ç²¾ç¡®çš„ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        confidence = 0.5
        
        # ç¼–ç¨‹ç›¸å…³å…³é”®è¯æ£€æµ‹ï¼ˆæƒé‡æ›´é«˜ï¼‰
        programming_keywords = [
            "code", "python", "javascript", "js", "function", "method", "class",
            "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "æ–¹æ³•", "ç±»", "ç®—æ³•", "å®ç°",
            "ä¸‹åˆ’çº¿", "é©¼å³°", "camelcase", "underscore", "è½¬æ¢", "è½¬åŒ–",
            "å˜é‡", "å‘½å", "æ ¼å¼", "string", "å­—ç¬¦ä¸²"
        ]
        programming_score = sum(1 for word in programming_keywords if word in prompt_lower)
        
        # åˆ›æ„å†™ä½œå…³é”®è¯
        creative_keywords = ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ", "å°è¯´", "æ•£æ–‡"]
        creative_score = sum(1 for word in creative_keywords if word in prompt_lower)
        
        # ç¿»è¯‘å…³é”®è¯
        translation_keywords = ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french", "è¯­è¨€", "è½¬è¯‘"]
        translation_score = sum(1 for word in translation_keywords if word in prompt_lower)
        
        # æ•°å­¦å…³é”®è¯
        math_keywords = ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation", "å…¬å¼", "æ±‚è§£"]
        math_score = sum(1 for word in math_keywords if word in prompt_lower)
        
        # åˆ†æå…³é”®è¯
        analysis_keywords = ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe", "æè¿°", "è¯„ä»·"]
        analysis_score = sum(1 for word in analysis_keywords if word in prompt_lower)
        
        # ç¡®å®šä»»åŠ¡ç±»å‹å’Œç½®ä¿¡åº¦
        scores = {
            "ç¼–ç¨‹": programming_score,
            "åˆ›æ„å†™ä½œ": creative_score,
            "ç¿»è¯‘": translation_score,
            "æ•°å­¦": math_score,
            "åˆ†æ": analysis_score
        }
        
        max_score = max(scores.values())
        if max_score > 0:
            task_type = max(scores, key=scores.get)
            confidence = min(0.9, 0.5 + max_score * 0.1)
        
        # ç‰¹æ®Šæ¨¡å¼æ£€æµ‹ï¼šä¸‹åˆ’çº¿è½¬é©¼å³°
        if any(word in prompt_lower for word in ["ä¸‹åˆ’çº¿", "é©¼å³°", "camelcase", "underscore"]):
            task_type = "ç¼–ç¨‹"
            confidence = 0.95
            logger.info("ğŸ¯ æ£€æµ‹åˆ°å­—ç¬¦ä¸²æ ¼å¼è½¬æ¢ä»»åŠ¡ï¼Œé«˜ç½®ä¿¡åº¦è¯†åˆ«ä¸ºç¼–ç¨‹ç±»å‹")
        
        # åŸºäºè¯­ä¹‰ç‰¹å¾å’Œå…³é”®è¯è°ƒæ•´å¤æ‚åº¦
        base_complexity = complexity_score
        if task_type == "ç¼–ç¨‹" and max_score >= 2:
            base_complexity = max(base_complexity, 0.6)  # ç¼–ç¨‹ä»»åŠ¡é€šå¸¸è¾ƒå¤æ‚
        
        if base_complexity > 0.7:
            complexity = "å¤æ‚"
        elif base_complexity < 0.3:
            complexity = "ç®€å•"
        else:
            complexity = "ä¸­ç­‰"
        
        # è¯­è¨€æ£€æµ‹ï¼ˆä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼‰
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        total_chars = len(prompt)
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
        
        language = "ä¸­æ–‡" if chinese_ratio > 0.3 else "è‹±æ–‡"
        
        result = {
            "task_type": task_type,
            "complexity": complexity,
            "language": language,
            "length": len(prompt),
            "confidence": confidence,
            "p2l_scores": {
                "complexity": base_complexity,
                "language": language_score,
                "keyword_scores": scores,
                "chinese_ratio": chinese_ratio
            }
        }
        
        logger.info(f"ğŸ“Š ä»»åŠ¡åˆ†æè¯¦æƒ…: {result}")
        return result

    def _interpret_p2l_output(self, prompt: str, complexity_score: float, language_score: float) -> Dict:
        """è§£é‡ŠP2Læ¨¡å‹è¾“å‡ºï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return self._enhanced_task_analysis(prompt, complexity_score, language_score)
    
    def _rule_based_analysis(self, prompt: str) -> Dict:
        """å¤‡ç”¨çš„è§„åˆ™åˆ†ææ–¹æ³•"""
        prompt_lower = prompt.lower()
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type = "é€šç”¨"
        if any(word in prompt_lower for word in ["code", "python", "javascript", "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "function"]):
            task_type = "ç¼–ç¨‹"
        elif any(word in prompt_lower for word in ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ"]):
            task_type = "åˆ›æ„å†™ä½œ"
        elif any(word in prompt_lower for word in ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french"]):
            task_type = "ç¿»è¯‘"
        elif any(word in prompt_lower for word in ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation"]):
            task_type = "æ•°å­¦"
        elif any(word in prompt_lower for word in ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe"]):
            task_type = "åˆ†æ"
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = "ç®€å•"
        if len(prompt) > 100 or any(word in prompt_lower for word in ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´"]):
            complexity = "å¤æ‚"
        elif len(prompt) > 50:
            complexity = "ä¸­ç­‰"
        
        # è¯­è¨€æ£€æµ‹
        language = "è‹±æ–‡"
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(prompt) * 0.3:
            language = "ä¸­æ–‡"
        
        return {
            "task_type": task_type,
            "complexity": complexity,
            "language": language,
            "length": len(prompt)
        }
    
    def calculate_model_scores(self, task_analysis: Dict, priority: str, enabled_models: Optional[List[str]] = None) -> List[Dict]:
        """è®¡ç®—æ¨¡å‹åˆ†æ•°å¹¶æ’åº - ä½¿ç”¨ç™¾åˆ†åˆ¶è¯„åˆ†"""
        scores = []
        
        # å¦‚æœæŒ‡å®šäº†å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œåªè®¡ç®—è¿™äº›æ¨¡å‹çš„åˆ†æ•°
        models_to_score = self.model_configs.items()
        if enabled_models:
            models_to_score = [(name, config) for name, config in self.model_configs.items() if name in enabled_models]
            logger.info(f"åªè®¡ç®—å¯ç”¨æ¨¡å‹çš„åˆ†æ•°: {[name for name, _ in models_to_score]}")
        
        for model_name, config in models_to_score:
            # åŸºç¡€åˆ†æ•° (40åˆ†)
            base_score = config["quality_score"] * 40
            
            # ä»»åŠ¡åŒ¹é…åº¦ (25åˆ†)
            task_score = 0
            if task_analysis["task_type"] in config["strengths"]:
                task_score = 25
            elif any(strength in task_analysis["task_type"] for strength in config["strengths"]):
                task_score = 15
            else:
                task_score = 5
            
            # è¯­è¨€åŒ¹é…åº¦ (15åˆ†)
            language_score = 0
            if task_analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in config["strengths"]:
                language_score = 15
            elif task_analysis["language"] == "ä¸­æ–‡":
                language_score = 8
            else:
                language_score = 10
            
            # ä¼˜å…ˆçº§åŒ¹é…åº¦ (20åˆ†)
            priority_score = 0
            if priority == "cost":
                if config["cost_per_1k"] < 0.005:
                    priority_score = 20
                elif config["cost_per_1k"] < 0.015:
                    priority_score = 15
                else:
                    priority_score = 8
            elif priority == "speed":
                if config["avg_response_time"] < 1.5:
                    priority_score = 20
                elif config["avg_response_time"] < 2.5:
                    priority_score = 15
                else:
                    priority_score = 8
            elif priority == "performance":
                if config["quality_score"] > 0.90:
                    priority_score = 20
                elif config["quality_score"] > 0.85:
                    priority_score = 15
                else:
                    priority_score = 10
            else:  # balanced
                priority_score = 15
            
            # æ€»åˆ† = åŸºç¡€åˆ† + ä»»åŠ¡åˆ† + è¯­è¨€åˆ† + ä¼˜å…ˆçº§åˆ† (æ»¡åˆ†100)
            final_score = base_score + task_score + language_score + priority_score
            
            # ç¡®ä¿åˆ†æ•°åœ¨0-100ä¹‹é—´
            final_score = max(0, min(100, final_score))
            
            scores.append({
                "model": model_name,
                "score": round(final_score, 1),
                "config": config
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x["score"], reverse=True)
        return scores
    
    async def p2l_analyze(self, request: P2LAnalysisRequest) -> Dict:
        """P2Læ™ºèƒ½åˆ†æ"""
        logger.info(f"P2Låˆ†æè¯·æ±‚: {request.prompt[:50]}...")
        logger.info(f"å¯ç”¨çš„æ¨¡å‹: {request.models}")
        
        # åˆ†æä»»åŠ¡
        task_analysis = self.analyze_task(request.prompt)
        
        # ä½¿ç”¨modeå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨priorityå­—æ®µ
        priority_mode = request.mode or request.priority
        
        # è®¡ç®—æ‰€æœ‰æ¨¡å‹çš„åˆ†æ•°ï¼ˆä¸å†é™åˆ¶ä¸ºå¯ç”¨çš„æ¨¡å‹ï¼‰
        model_scores = self.calculate_model_scores(task_analysis, priority_mode, enabled_models=None)
        
        # ç”Ÿæˆæ¨èç†ç”±
        best_model = model_scores[0]
        reasoning_parts = []
        
        if task_analysis["task_type"] in best_model["config"]["strengths"]:
            reasoning_parts.append(f"æ“…é•¿{task_analysis['task_type']}ä»»åŠ¡")
        
        if task_analysis["language"] == "ä¸­æ–‡" and "ä¸­æ–‡" in best_model["config"]["strengths"]:
            reasoning_parts.append("ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º")
        
        if request.priority == "cost" and best_model["config"]["cost_per_1k"] < 0.01:
            reasoning_parts.append("æˆæœ¬æ•ˆç›Šé«˜")
        elif request.priority == "speed" and best_model["config"]["avg_response_time"] < 2.0:
            reasoning_parts.append("å“åº”é€Ÿåº¦å¿«")
        elif request.priority == "performance" and best_model["config"]["quality_score"] > 0.90:
            reasoning_parts.append("æ€§èƒ½è¡¨ç°ä¼˜ç§€")
        
        reasoning = "ï¼›".join(reasoning_parts) if reasoning_parts else "ç»¼åˆæ€§èƒ½æœ€ä½³"
        
        result = {
            "recommended_model": best_model["model"],
            "confidence": best_model["score"],
            "estimated_cost": f"${best_model['config']['cost_per_1k']}/1K tokens",
            "estimated_time": f"{best_model['config']['avg_response_time']}s",
            "task_analysis": task_analysis,
            "reasoning": reasoning,
            "recommendations": [
                {
                    "model": item["model"], 
                    "score": item["score"],
                    "provider": item["config"]["provider"],
                    "cost_per_1k": item["config"]["cost_per_1k"],
                    "avg_response_time": item["config"]["avg_response_time"],
                    "strengths": item["config"]["strengths"],
                    "quality_score": item["config"]["quality_score"]
                } 
                for item in model_scores
            ],
            "model_rankings": [
                {"model": item["model"], "score": item["score"]} 
                for item in model_scores
            ],
            "priority_mode": request.priority
        }
        
        logger.info(f"P2Læ¨è: {result['recommended_model']}")
        return result
    
    async def generate_llm_response(self, request: LLMGenerateRequest) -> Dict:
        """çœŸå®LLMå“åº”ç”Ÿæˆ"""
        logger.info(f"è°ƒç”¨LLM: {request.model}")
        
        start_time = time.time()
        
        try:
            # å¯¹äºDeepSeekæ¨¡å‹ï¼Œä½¿ç”¨DeepSeekå®¢æˆ·ç«¯
            if request.model.startswith('deepseek'):

                
                if deepseek_client.api_key:
                    logger.info(f"ä½¿ç”¨DeepSeek APIè°ƒç”¨: {request.model}")
                    llm_response = deepseek_client.generate_response(
                        model=request.model,
                        prompt=request.prompt,
                        max_tokens=2000,
                        temperature=0.7
                    )
                    
                    logger.info(f"âœ… DeepSeek APIè°ƒç”¨æˆåŠŸ: {request.model}")
                    return llm_response
                else:
                    raise Exception("DeepSeek APIå¯†é’¥æœªé…ç½®")
            

            
            # å°è¯•ä½¿ç”¨å…¶ä»–LLM API
            elif self.llm_client:
                async with self.llm_client as client:
                    llm_response = await client.generate_response(
                        model=request.model,
                        prompt=request.prompt,
                        max_tokens=2000,
                        temperature=0.7
                    )
                    
                    logger.info(f"âœ… çœŸå®APIè°ƒç”¨æˆåŠŸ: {request.model}")
                    
                    return {
                        "model": llm_response.model,
                        "response": llm_response.content,
                        "content": llm_response.content,
                        "response_time": round(llm_response.response_time, 2),
                        "tokens": llm_response.tokens_used,
                        "tokens_used": llm_response.tokens_used,
                        "cost": round(llm_response.cost, 4),
                        "provider": llm_response.provider,
                        "is_real_api": True
                    }
            else:
                raise Exception("LLMå®¢æˆ·ç«¯ä¸å¯ç”¨")
                
        except Exception as e:
            error_msg = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢ä¸è¶³é”™è¯¯
            if "insufficient_quota" in error_msg or "exceeded your current quota" in error_msg:
                logger.error(f"âŒ OpenAI APIé…é¢ä¸è¶³: {e}")
                # è¿”å›é…é¢ä¸è¶³çš„æ˜ç¡®æç¤º
                return {
                    "model": request.model,
                    "response": "âš ï¸ **OpenAI APIé…é¢ä¸è¶³**\n\næ‚¨çš„OpenAI APIå¯†é’¥é…é¢å·²ç”¨å®Œï¼Œè¯·ï¼š\n1. æ£€æŸ¥æ‚¨çš„OpenAIè´¦æˆ·ä½™é¢\n2. å‡çº§æ‚¨çš„ä»˜è´¹è®¡åˆ’\n3. æˆ–ç­‰å¾…é…é¢é‡ç½®\n\nå½“å‰ä½¿ç”¨æ¨¡æ‹Ÿå“åº”ä»£æ›¿çœŸå®APIè°ƒç”¨ã€‚",
                    "content": "âš ï¸ OpenAI APIé…é¢ä¸è¶³ï¼Œè¯·æ£€æŸ¥è´¦æˆ·ä½™é¢",
                    "response_time": 0.1,
                    "tokens": 50,
                    "tokens_used": 50,
                    "cost": 0.0,
                    "provider": "openai",
                    "is_real_api": False,
                    "error_type": "quota_exceeded"
                }
            else:
                logger.warning(f"çœŸå®APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”: {e}")
            
            # å›é€€åˆ°æ¨¡æ‹Ÿå“åº”
            model_config = self.model_configs.get(request.model, {})
            response_time = model_config.get("avg_response_time", 2.0)
            
            # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            await asyncio.sleep(min(response_time * 0.3, 1.0))
            
            # ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
            response_content = self._generate_mock_response(request.model, request.prompt)
            
            actual_time = time.time() - start_time
            
            return {
                "model": request.model,
                "response": response_content,
                "content": response_content,
                "response_time": round(actual_time, 2),
                "tokens": int(len(response_content.split()) * 1.3),
                "tokens_used": int(len(response_content.split()) * 1.3),
                "cost": round(model_config.get("cost_per_1k", 0.02) * len(response_content.split()) * 1.3 / 1000, 4),
                "provider": "simulation",
                "is_real_api": False
            }
    
    def _generate_mock_response(self, model: str, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„LLMå“åº”"""
        if "javascript" in prompt.lower() or "js" in prompt.lower():
            if "gpt-4o" in model:
                return """// é«˜è´¨é‡çš„JavaScriptä¸‹åˆ’çº¿è½¬é©¼å³°å®ç°

/**
 * å°†ä¸‹åˆ’çº¿å‘½åè½¬æ¢ä¸ºé©¼å³°å‘½å
 * @param {string} str - åŒ…å«ä¸‹åˆ’çº¿çš„å­—ç¬¦ä¸²
 * @returns {string} è½¬æ¢åçš„é©¼å³°å‘½åå­—ç¬¦ä¸²
 */
function toCamelCase(str) {
    if (!str || typeof str !== 'string') return '';
    
    return str
        .toLowerCase()
        .split('_')
        .map((word, index) => 
            index === 0 ? word : word.charAt(0).toUpperCase() + word.slice(1)
        )
        .join('');
}

// ä½¿ç”¨ç¤ºä¾‹
console.log(toCamelCase('hello_world_example')); // helloWorldExample
console.log(toCamelCase('user_name')); // userName
console.log(toCamelCase('api_response_data')); // apiResponseData

// å¤„ç†è¾¹ç•Œæƒ…å†µçš„å¢å¼ºç‰ˆæœ¬
function toCamelCaseAdvanced(str) {
    if (!str || typeof str !== 'string') return '';
    
    return str
        .trim()
        .toLowerCase()
        .replace(/[_-]+(.)?/g, (_, char) => char ? char.toUpperCase() : '');
}

// æµ‹è¯•ç”¨ä¾‹
const testCases = [
    'hello_world',
    'user_name_field', 
    'api_response_data',
    '_leading_underscore',
    'trailing_underscore_',
    'multiple___underscores'
];

testCases.forEach(test => {
    console.log(`${test} -> ${toCamelCaseAdvanced(test)}`);
});"""
            elif "claude" in model:
                return """// ä¼˜é›…çš„ä¸‹åˆ’çº¿è½¬é©¼å³°è§£å†³æ–¹æ¡ˆ

// ç®€æ´çš„å®ç°æ–¹å¼
const toCamelCase = str => 
    str.replace(/_([a-z])/g, (_, letter) => letter.toUpperCase());

// æ›´å®Œæ•´çš„å®ç°ï¼Œå¤„ç†å„ç§è¾¹ç•Œæƒ…å†µ
function convertToCamelCase(input) {
    return input
        .toLowerCase()
        .replace(/[^a-zA-Z0-9]+(.)/g, (_, chr) => chr.toUpperCase());
}

// ä½¿ç”¨ç¤ºä¾‹
console.log(toCamelCase('my_variable_name')); // myVariableName
console.log(toCamelCase('hello_world')); // helloWorld
console.log(convertToCamelCase('user-name_field')); // userNameField

// å‡½æ•°å¼ç¼–ç¨‹é£æ ¼
const camelCase = str => str
    .split(/[_-]+/)
    .map((word, i) => i === 0 ? word.toLowerCase() : 
         word.charAt(0).toUpperCase() + word.slice(1).toLowerCase())
    .join('');"""
            else:
                return f"""// {model} ç”Ÿæˆçš„ä¸‹åˆ’çº¿è½¬é©¼å³°å®ç°

function underscoreToCamelCase(inputString) {{
    return inputString
        .split('_')
        .map((word, index) => {{
            if (index === 0) return word.toLowerCase();
            return word.charAt(0).toUpperCase() + word.slice(1).toLowerCase();
        }})
        .join('');
}}

// ä½¿ç”¨ç¤ºä¾‹
const examples = ['hello_world', 'user_name', 'convert_this_string'];
examples.forEach(example => {{
    console.log(`${{example}} -> ${{underscoreToCamelCase(example)}}`);
}});

// è¾“å‡º:
// hello_world -> helloWorld
// user_name -> userName  
// convert_this_string -> convertThisString"""
        
        # å¤„ç†è¯—æ­Œåˆ›ä½œç±»é—®é¢˜
        elif any(keyword in prompt.lower() for keyword in ["è¯—", "poem", "poetry", "æ™´å¤©", "é˜³å…‰", "å¤©æ°”"]):
            if "gpt-4o" in model:
                return """ã€Šæ™´ç©ºä¸‡é‡Œã€‹

è”šè“å¤©ç©ºæ— äº‘é®ï¼Œ
é‡‘è¾‰æ´’å‘å¤§åœ°èŠ±ã€‚
å¾®é£è½»æŠšç»¿å¶èˆï¼Œ
é¸Ÿå„¿æ¬¢æ­Œæ»¡ææ¡ ã€‚

é˜³å…‰é€è¿‡çª—æ£‚ç…§ï¼Œ
æ¸©æš–å¿ƒæˆ¿é©±é˜´éœ¾ã€‚
æ­¤åˆ»æ—¶å…‰å¤šç¾å¥½ï¼Œ
æ™´å¤©å¦‚è¯—ç”»ä¸­æ¥ã€‚

æ¸…æ™¨éœ²ç æ˜ æœé˜³ï¼Œ
åˆåå…‰å½±é•¿åˆé•¿ã€‚
é»„æ˜å½©éœæŸ“å¤©é™…ï¼Œ
ä¸€æ—¥æ™´å¥½èƒœæ˜¥å…‰ã€‚"""
            elif "claude" in model:
                return """ã€Šé˜³å…‰è¯—ç¯‡ã€‹

å¤©ç©ºæ¾„æ¾ˆå¦‚æ°´æ™¶ï¼Œ
ç™½äº‘æ‚ æ‚ è‡ªåœ¨è¡Œã€‚
æš–é˜³è½»å»å¤§åœ°é¢ï¼Œ
ä¸‡ç‰©ç”Ÿè¾‰æ˜¾ç²¾ç¥ã€‚

æ¸…é£å¾æ¥èŠ±é¦™æµ“ï¼Œ
è´è¶ç¿©ç¿©èˆå…¶ä¸­ã€‚
æ­¤æ—¶æ­¤åˆ»å¿ƒæƒ…å¥½ï¼Œ
æ™´å¤©å¸¦æ¥æ— é™æƒ…ã€‚

å…‰å½±æ–‘é©³æ ‘è«ä¸‹ï¼Œ
å­©ç«¥å¬‰æˆç¬‘å£°å“—ã€‚
ç¾å¥½æ—¶å…‰å¦‚è¯—å¥ï¼Œ
æ™´æœ—å¤©æ°”èƒœç”»å®¶ã€‚"""
            else:
                return f"""ã€Š{model}åˆ›ä½œ - æ™´å¤©é¢‚ã€‹

ç¢§ç©ºå¦‚æ´—ä¸‡é‡Œæ™´ï¼Œ
æœé˜³åˆå‡ç…§å¤§åœ°ã€‚
å¾®é£ä¹ ä¹ èŠ±å„¿ç¬‘ï¼Œ
é¸Ÿè¯­èŠ±é¦™æ»¡å›­æ˜¥ã€‚

åˆåé˜³å…‰æš–å¦‚é…’ï¼Œ
æ ‘å½±å©†å¨‘èˆç¿©ç¿©ã€‚
æ­¤æƒ…æ­¤æ™¯å¤šç¾å¦™ï¼Œ
æ™´å¤©å¦‚æ¢¦ä¼¼ç¥ä»™ã€‚

é»„æ˜æ—¶åˆ†éœæ»¡å¤©ï¼Œ
é‡‘è¾‰æ´’å‘äººé—´è·¯ã€‚
ä¸€æ—¥æ™´å¥½å¿ƒæ¬¢ç•…ï¼Œ
è¯—æ„ç›ç„¶åœ¨å¿ƒå¤´ã€‚"""
        
        # å¤„ç†ç¼–ç¨‹é—®é¢˜
        elif any(keyword in prompt.lower() for keyword in ["python", "ç®—æ³•", "ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "æ’åº"]):
            if "gpt-4o" in model:
                return """# Pythonå¿«é€Ÿæ’åºç®—æ³•å®ç°

def quicksort(arr):
    \"\"\"
    å¿«é€Ÿæ’åºç®—æ³• - é«˜æ•ˆçš„åˆ†æ²»æ’åºæ–¹æ³•
    æ—¶é—´å¤æ‚åº¦: å¹³å‡ O(n log n), æœ€å O(nÂ²)
    ç©ºé—´å¤æ‚åº¦: O(log n)
    \"\"\"
    if len(arr) <= 1:
        return arr
    
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    
    return quicksort(left) + middle + quicksort(right)

# ä½¿ç”¨ç¤ºä¾‹
test_array = [64, 34, 25, 12, 22, 11, 90]
print(f"åŸæ•°ç»„: {test_array}")
sorted_array = quicksort(test_array)
print(f"æ’åºå: {sorted_array}")

# æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
def quicksort_inplace(arr, low=0, high=None):
    if high is None:
        high = len(arr) - 1
    
    if low < high:
        pi = partition(arr, low, high)
        quicksort_inplace(arr, low, pi - 1)
        quicksort_inplace(arr, pi + 1, high)

def partition(arr, low, high):
    pivot = arr[high]
    i = low - 1
    
    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]
    
    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1"""
            else:
                return f"""# {model} - Pythonå¿«é€Ÿæ’åºå®ç°

def quick_sort(numbers):
    if len(numbers) < 2:
        return numbers
    
    pivot_index = len(numbers) // 2
    pivot = numbers[pivot_index]
    
    less = [num for num in numbers if num < pivot]
    equal = [num for num in numbers if num == pivot]
    greater = [num for num in numbers if num > pivot]
    
    return quick_sort(less) + equal + quick_sort(greater)

# æµ‹è¯•
data = [3, 6, 8, 10, 1, 2, 1]
result = quick_sort(data)
print(f"æ’åºç»“æœ: {result}")"""
        
        # å¤„ç†æŠ€æœ¯è§£é‡Šç±»é—®é¢˜
        elif any(keyword in prompt.lower() for keyword in ["ä»€ä¹ˆæ˜¯", "è§£é‡Š", "åŸç†", "æŠ€æœ¯", "åŒºå—é“¾", "ai", "äººå·¥æ™ºèƒ½"]):
            return f"""{model} ä¸“ä¸šè§£ç­”ï¼š

{prompt}

è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ€æœ¯é—®é¢˜ã€‚è®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£é‡Šï¼š

## æ ¸å¿ƒæ¦‚å¿µ
è¿™ä¸ªæ¦‚å¿µæ¶‰åŠå¤šä¸ªæŠ€æœ¯å±‚é¢ï¼ŒåŒ…æ‹¬åº•å±‚åŸç†ã€å®ç°æœºåˆ¶å’Œåº”ç”¨åœºæ™¯ã€‚

## æŠ€æœ¯åŸç†
ä»æŠ€æœ¯æ¶æ„è§’åº¦æ¥çœ‹ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„ä»¶ï¼š
1. åŸºç¡€è®¾æ–½å±‚
2. æ ¸å¿ƒç®—æ³•å±‚  
3. åº”ç”¨æ¥å£å±‚
4. ç”¨æˆ·äº¤äº’å±‚

## å®é™…åº”ç”¨
åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é¡¹æŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äºï¼š
- ä¼ä¸šçº§è§£å†³æ–¹æ¡ˆ
- æ¶ˆè´¹è€…äº§å“
- ç§‘ç ”é¢†åŸŸ
- æ•™è‚²åŸ¹è®­

## å‘å±•è¶‹åŠ¿
æœªæ¥å‘å±•æ–¹å‘ä¸»è¦é›†ä¸­åœ¨æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨æ€§æå‡å’Œç”¨æˆ·ä½“éªŒæ”¹è¿›ç­‰æ–¹é¢ã€‚

å¸Œæœ›è¿™ä¸ªè§£é‡Šå¯¹æ‚¨æœ‰å¸®åŠ©ï¼å¦‚æœ‰æ›´å¤šé—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­äº¤æµã€‚"""
        
        # å¤„ç†æ•°æ®åˆ†æç±»é—®é¢˜
        elif any(keyword in prompt.lower() for keyword in ["åˆ†æ", "æ•°æ®", "æŒ‡æ ‡", "ç»Ÿè®¡", "æŠ¥å‘Š"]):
            return f"""{model} æ•°æ®åˆ†ææŠ¥å‘Šï¼š

## å…³äº "{prompt}" çš„åˆ†æ

### 1. æ•°æ®æ¦‚è§ˆ
- æ•°æ®æ¥æºï¼šå¤šæ¸ é“æ•´åˆ
- æ—¶é—´èŒƒå›´ï¼šè¿‘æœŸè¶‹åŠ¿åˆ†æ
- æ ·æœ¬è§„æ¨¡ï¼šå…·æœ‰ç»Ÿè®¡æ„ä¹‰

### 2. å…³é”®æŒ‡æ ‡
| æŒ‡æ ‡åç§° | æ•°å€¼ | è¶‹åŠ¿ | é‡è¦æ€§ |
|---------|------|------|--------|
| æ ¸å¿ƒæŒ‡æ ‡1 | 85.2% | â†—ï¸ | é«˜ |
| æ ¸å¿ƒæŒ‡æ ‡2 | 1,234 | â†˜ï¸ | ä¸­ |
| æ ¸å¿ƒæŒ‡æ ‡3 | 67.8% | â†’ | é«˜ |

### 3. æ·±åº¦æ´å¯Ÿ
é€šè¿‡æ•°æ®åˆ†æå‘ç°ä»¥ä¸‹å…³é”®è¶‹åŠ¿ï¼š
- è¶‹åŠ¿1ï¼šæ•´ä½“è¡¨ç°ç¨³å®šå‘å¥½
- è¶‹åŠ¿2ï¼šæŸäº›ç»†åˆ†é¢†åŸŸéœ€è¦å…³æ³¨
- è¶‹åŠ¿3ï¼šç”¨æˆ·è¡Œä¸ºæ¨¡å¼å‘ç”Ÿå˜åŒ–

### 4. å»ºè®®æªæ–½
åŸºäºåˆ†æç»“æœï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹è¡ŒåŠ¨ï¼š
1. ä¼˜åŒ–æ ¸å¿ƒæµç¨‹
2. åŠ å¼ºç›‘æ§æœºåˆ¶
3. æå‡ç”¨æˆ·ä½“éªŒ
4. æŒç»­æ•°æ®è·Ÿè¸ª

### 5. ç»“è®º
ç»¼åˆåˆ†æè¡¨æ˜ï¼Œå½“å‰çŠ¶å†µæ€»ä½“è‰¯å¥½ï¼Œä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚"""
        
        # é»˜è®¤é€šç”¨å“åº”
        else:
            return f"""{model} æ™ºèƒ½å›ç­”ï¼š

æ„Ÿè°¢æ‚¨çš„æé—®ï¼š"{prompt}"

è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„ç†è§£ï¼Œæˆ‘å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªè§’åº¦æ¥å›ç­”ï¼š

## ä¸»è¦è§‚ç‚¹
é’ˆå¯¹æ‚¨çš„é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºå…³é”®åœ¨äºç†è§£å…¶æ ¸å¿ƒæœ¬è´¨å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚

## è¯¦ç»†åˆ†æ
1. **èƒŒæ™¯åˆ†æ**ï¼šè¿™ä¸ªé—®é¢˜æ¶‰åŠå¤šä¸ªå±‚é¢çš„è€ƒé‡
2. **æ ¸å¿ƒè¦ç‚¹**ï¼šä¸»è¦åŒ…å«å‡ ä¸ªå…³é”®è¦ç´ 
3. **å®è·µå»ºè®®**ï¼šåœ¨å®é™…åº”ç”¨ä¸­éœ€è¦æ³¨æ„çš„äº‹é¡¹
4. **å»¶ä¼¸æ€è€ƒ**ï¼šç›¸å…³çš„æ·±å…¥æ€è€ƒæ–¹å‘

## æ€»ç»“å»ºè®®
ç»¼åˆè€ƒè™‘å„ç§å› ç´ ï¼Œæˆ‘å»ºè®®é‡‡å–å¹³è¡¡çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæ—¢è¦è€ƒè™‘ç†è®ºåŸºç¡€ï¼Œä¹Ÿè¦ç»“åˆå®é™…æƒ…å†µã€‚

å¸Œæœ›è¿™ä¸ªå›ç­”å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼å¦‚æœæ‚¨éœ€è¦æ›´å…·ä½“çš„ä¿¡æ¯ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚"""

# å…¨å±€æœåŠ¡å®ä¾‹
p2l_service = P2LBackendService()

# APIè·¯ç”±
@app.post("/api/p2l/analyze")
async def analyze_with_p2l(request: P2LAnalysisRequest):
    """P2Læ™ºèƒ½åˆ†æAPI"""
    try:
        result = await p2l_service.p2l_analyze(request)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"P2Låˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/p2l/inference")
async def p2l_inference(request: P2LInferenceRequest):
    """P2Lä»£ç æ¨ç†API - å°†ä»£ç è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€"""
    try:
        if not p2l_service.p2l_inference_engine:
            raise HTTPException(status_code=503, detail="P2Læ¨ç†å¼•æ“æœªåŠ è½½")
        
        logger.info(f"P2Læ¨ç†è¯·æ±‚: {request.code[:100]}...")
        
        # ä½¿ç”¨P2Læ¨ç†å¼•æ“
        result = p2l_service.p2l_inference_engine.infer(
            request.code,
            max_length=request.max_length,
            temperature=request.temperature
        )
        
        return JSONResponse(content={
            "code": request.code,
            "natural_language": result["natural_language"],
            "confidence": result.get("confidence", 0.8),
            "processing_time": result.get("processing_time", 0.0),
            "model_info": "P2L-Inference-Engine"
        })
        
    except Exception as e:
        logger.error(f"P2Læ¨ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/llm/generate")
async def generate_with_llm(request: LLMGenerateRequest):
    """LLMç”ŸæˆAPI"""
    try:
        result = await p2l_service.generate_llm_response(request)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return JSONResponse(content={
        "p2l_models": list(p2l_service.p2l_models.keys()),
        "llm_models": list(p2l_service.model_configs.keys()),
        "total_models": len(p2l_service.model_configs)
    })

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse(content={
        "status": "healthy",
        "p2l_models_loaded": len(p2l_service.p2l_models),
        "p2l_inference_engine": p2l_service.p2l_inference_engine is not None,
        "llm_models_available": len(p2l_service.model_configs),
        "device": str(p2l_service.device),
        "p2l_available": P2L_AVAILABLE,
        "llm_client_available": LLM_CLIENT_AVAILABLE,
        "real_api_enabled": p2l_service.llm_client is not None
    })

# é™æ€æ–‡ä»¶æœåŠ¡
# é™æ€æ–‡ä»¶æœåŠ¡å·²ç§»é™¤ - ä½¿ç”¨ç‹¬ç«‹çš„Vueå‰ç«¯

@app.get("/", response_class=HTMLResponse)
async def serve_root():
    """æ ¹è·¯å¾„ä¿¡æ¯é¡µé¢"""
    return HTMLResponse(
        content="""
        <html>
        <head><title>P2Læ™ºèƒ½è·¯ç”±åç«¯</title></head>
        <body style="font-family: Arial, sans-serif; text-align: center; padding: 50px;">
            <h1>ğŸ§  P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡</h1>
            <p>åç«¯æœåŠ¡è¿è¡Œæ­£å¸¸</p>
            <div style="margin: 20px;">
                <a href="/docs" style="background: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px;">ğŸ“š APIæ–‡æ¡£</a>
                <a href="/api/health" style="background: #28a745; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-left: 10px;">ğŸ” å¥åº·æ£€æŸ¥</a>
            </div>
            <p style="color: #666; margin-top: 30px;">
                å‰ç«¯ç•Œé¢è¯·è®¿é—®: <a href="http://localhost:3000">http://localhost:3000</a>
            </p>
        </body>
        </html>
        """,
        status_code=200
    )

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨P2Læ™ºèƒ½è·¯ç”±åç«¯æœåŠ¡...")
    print(f"ğŸ“Š æ”¯æŒ {len(p2l_service.model_configs)} ä¸ªLLMæ¨¡å‹")
    print(f"ğŸ§  åŠ è½½ {len(p2l_service.p2l_models)} ä¸ªP2Læ¨¡å‹")
    print("ğŸŒ å‰ç«¯è®¿é—®: http://localhost:8080")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8080/docs")
    
    uvicorn.run(
        "backend_service:app",
        host="0.0.0.0",
        port=8080,
        reload=False,
        log_level="info"
    )