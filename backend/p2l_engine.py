#!/usr/bin/env python3
"""
P2Læ¨ç†å¼•æ“æ¨¡å—
è´Ÿè´£P2Læ¨¡å‹åŠ è½½å’Œæ¨ç†åˆ†æ
"""

import os
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, Optional
import logging

try:
    from .config import get_p2l_config
except ImportError:
    from config import get_p2l_config

logger = logging.getLogger(__name__)

# å¯¼å…¥P2Læ¨ç†æ¨¡å—
try:
    import sys
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))  # backendç›®å½•
    project_root = os.path.dirname(current_dir)  # program-bç›®å½•
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    
    from p2l.p2l.model import load_model as load_p2l_model, generate_text
    from p2l.p2l.p2l_inference import P2LInferenceEngine
    P2L_AVAILABLE = True
except ImportError as e:
    logging.warning(f"P2Læ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    P2L_AVAILABLE = False

class P2LEngine:
    """P2Læ¨ç†å¼•æ“ç®¡ç†å™¨"""
    
    def __init__(self, device: torch.device):
        self.device = device
        self.p2l_models = {}
        self.p2l_inference_engine = None
        self.config = get_p2l_config()
        
        # åŠ è½½P2Læ¨¡å‹å’Œæ¨ç†å¼•æ“
        self._load_p2l_models()
        if P2L_AVAILABLE:
            self._load_p2l_inference_engine()
    
    def _load_p2l_models(self):
        """åŠ è½½å¯ç”¨çš„P2Læ¨¡å‹ - æŒ‰é…ç½®ä¼˜å…ˆçº§"""
        # å¯¼å…¥é…ç½®å¸¸é‡
        try:
            import sys
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(current_dir)
            if project_root not in sys.path:
                sys.path.insert(0, project_root)
            from constants import DEFAULT_MODEL, MODEL_MAPPING
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥é…ç½®å¸¸é‡: {e}")
            return
        
        model_path = self.config["model_path"]
        if not os.path.exists(model_path):
            logger.warning(f"P2Læ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return
        
        # 1. ä¼˜å…ˆåŠ è½½é…ç½®æŒ‡å®šçš„é»˜è®¤æ¨¡å‹
        if DEFAULT_MODEL in MODEL_MAPPING:
            target_model = MODEL_MAPPING[DEFAULT_MODEL]["local_name"]
            target_path = os.path.join(model_path, target_model)
            
            if os.path.exists(target_path):
                logger.info(f"ğŸ¯ æŒ‰é…ç½®åŠ è½½æŒ‡å®šæ¨¡å‹: {target_model} (æ¥è‡ª {DEFAULT_MODEL})")
                if self._load_single_model(target_model, target_path):
                    logger.info(f"âœ… é…ç½®æ¨¡å‹ {target_model} åŠ è½½æˆåŠŸï¼Œè·³è¿‡å…¶ä»–æ¨¡å‹")
                    return
                else:
                    logger.warning(f"âš ï¸ é…ç½®æ¨¡å‹ {target_model} åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
            else:
                logger.warning(f"âš ï¸ é…ç½®çš„æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {target_path}")
        else:
            logger.warning(f"âš ï¸ é…ç½®çš„æ¨¡å‹ {DEFAULT_MODEL} ä¸åœ¨æ˜ å°„è¡¨ä¸­")
        
        # 2. å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰«ææ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼Œä½†ä¼šè­¦å‘Šï¼‰
        logger.warning("ğŸ” é…ç½®çš„é»˜è®¤æ¨¡å‹ä¸å¯ç”¨ï¼Œæ‰«ææ‰€æœ‰å¯ç”¨æ¨¡å‹...")
        available_models = []
        for item in os.listdir(model_path):
            if item.startswith('p2l-') and os.path.isdir(os.path.join(model_path, item)):
                available_models.append(item)
        
        # æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä½†ä¼˜å…ˆé€‰æ‹©è¾ƒå°çš„æ¨¡å‹
        available_models.sort(key=lambda x: (x.split('-')[1] if len(x.split('-')) > 1 else 'zzz'))
        
        for item in available_models:
            full_model_path = os.path.join(model_path, item)
            logger.warning(f"âš ï¸ å°è¯•å¤‡ç”¨æ¨¡å‹: {item}")
            if self._load_single_model(item, full_model_path):
                logger.info(f"âœ… å¤‡ç”¨æ¨¡å‹ {item} åŠ è½½æˆåŠŸ")
                break
        
    def _load_single_model(self, item: str, full_model_path: str) -> bool:
        """åŠ è½½å•ä¸ªP2Læ¨¡å‹"""
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½P2Lä¸“ç”¨æ¨¡å‹: {item}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯P2Lä¸“ç”¨æ¨¡å‹æ ¼å¼
                    config_path = os.path.join(full_model_path, "config.json")
                    training_config_path = os.path.join(full_model_path, "training_config.json")
                    
                    if os.path.exists(training_config_path):
                        logger.info("ğŸ¯ æ£€æµ‹åˆ°P2Lè®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨P2Lä¸“ç”¨åŠ è½½å™¨")
                        # ä½¿ç”¨P2Lä¸“ç”¨æ¨¡å‹åŠ è½½å™¨
                        from p2l.p2l.model import get_p2l_model, get_tokenizer
                        import json
                        
                        # è¯»å–è®­ç»ƒé…ç½®
                        with open(training_config_path, 'r') as f:
                            training_config = json.load(f)
                        
                        # è¯»å–æ¨¡å‹é…ç½®
                        with open(config_path, 'r') as f:
                            model_config = json.load(f)
                        
                        # åˆ›å»ºP2Læ¨¡å‹
                        model_type = training_config.get("model_type", "qwen2")
                        loss_type = training_config.get("loss_type", "grk")
                        head_type = training_config.get("head_type", "rk")
                        
                        logger.info(f"ğŸ“‹ P2Læ¨¡å‹é…ç½®: {model_type}/{loss_type}/{head_type}")
                        
                        # è·å–P2Læ¨¡å‹ç±»
                        P2LModel = get_p2l_model(model_type, loss_type, head_type)
                        
                        # åŠ è½½tokenizer
                        tokenizer = get_tokenizer(
                            full_model_path,
                            chat_template=None,
                            pad_token_if_none="<|pad|>",
                            cls_token_if_none="<|cls|>"
                        )
                        
                        # åˆ›å»ºæ¨¡å‹é…ç½®å¯¹è±¡
                        from transformers import AutoConfig
                        config = AutoConfig.from_pretrained(full_model_path)
                        
                        # åˆå§‹åŒ–P2Læ¨¡å‹ - ä»æƒé‡æ–‡ä»¶æ¨æ–­æ­£ç¡®çš„ç±»åˆ«æ•°
                        model_weights_path = os.path.join(full_model_path, "model.safetensors")
                        num_classes = 10  # é»˜è®¤å€¼
                        
                        if os.path.exists(model_weights_path):
                            import safetensors.torch
                            state_dict = safetensors.torch.load_file(model_weights_path)
                            # ä»æƒé‡æ–‡ä»¶æ¨æ–­ç±»åˆ«æ•°
                            if 'head.head.weight' in state_dict:
                                num_classes = state_dict['head.head.weight'].shape[0]
                                logger.info(f"ğŸ“Š ä»æƒé‡æ–‡ä»¶æ¨æ–­ç±»åˆ«æ•°: {num_classes}")
                        
                        model = P2LModel(
                            config=config,
                            CLS_id=tokenizer.cls_token_id,
                            num_models=num_classes,  # ä½¿ç”¨æ¨æ–­çš„ç±»åˆ«æ•°
                            linear_head_downsize_factor=training_config.get("linear_head_downsize_factor"),
                            head_kwargs=training_config.get("head_kwargs", {})
                        )
                        
                        # åŠ è½½æƒé‡
                        if os.path.exists(model_weights_path):
                            model.load_state_dict(state_dict, strict=False)
                            logger.info("âœ… P2Læ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
                        else:
                            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                        
                        model.to(self.device)
                        model.eval()
                        
                        self.p2l_models[item] = {
                            "tokenizer": tokenizer,
                            "model": model,
                            "path": full_model_path,
                            "model_type": model_type,
                            "loss_type": loss_type,
                            "head_type": head_type,
                            "is_p2l_model": True
                        }
                        logger.info(f"ğŸ‰ P2Lä¸“ç”¨æ¨¡å‹ {item} åŠ è½½æˆåŠŸï¼")
                        return True
                    
                    else:
                        # å°è¯•æ ‡å‡†transformersåŠ è½½
                        logger.info("ğŸ”„ å°è¯•æ ‡å‡†transformersåŠ è½½...")
                        tokenizer = AutoTokenizer.from_pretrained(full_model_path, trust_remote_code=True)
                        model = AutoModelForCausalLM.from_pretrained(
                            full_model_path,
                            torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,
                            device_map=None,
                            trust_remote_code=True
                        )
                        model.to(self.device)
                        model.eval()
                        
                        self.p2l_models[item] = {
                            "tokenizer": tokenizer,
                            "model": model,
                            "path": full_model_path,
                            "is_p2l_model": False
                        }
                        logger.info(f"âœ… æ ‡å‡†æ¨¡å‹ {item} åŠ è½½æˆåŠŸ")
                        return True
                        
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹ {item} åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def _load_p2l_inference_engine(self):
        """åŠ è½½P2Læ¨ç†å¼•æ“ - æŒ‰é…ç½®ä¼˜å…ˆçº§"""
        try:
            logger.info("æ­£åœ¨åŠ è½½P2Læ¨ç†å¼•æ“...")
            
            # å¯¼å…¥é…ç½®å¸¸é‡
            try:
                import sys
                current_dir = os.path.dirname(os.path.abspath(__file__))
                project_root = os.path.dirname(current_dir)
                if project_root not in sys.path:
                    sys.path.insert(0, project_root)
                from constants import DEFAULT_MODEL, MODEL_MAPPING
            except ImportError:
                logger.warning("æ— æ³•å¯¼å…¥é…ç½®å¸¸é‡ï¼Œä½¿ç”¨é»˜è®¤æ‰«ææ–¹å¼")
                DEFAULT_MODEL = None
                MODEL_MAPPING = {}
            
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„
            models_dir = self.config["model_path"]
            p2l_model_path = None
            
            # 1. ä¼˜å…ˆä½¿ç”¨é…ç½®æŒ‡å®šçš„æ¨¡å‹
            if DEFAULT_MODEL and DEFAULT_MODEL in MODEL_MAPPING:
                target_model = MODEL_MAPPING[DEFAULT_MODEL]["local_name"]
                target_path = os.path.join(models_dir, target_model)
                if os.path.exists(target_path):
                    p2l_model_path = target_path
                    logger.info(f"ğŸ¯ æ¨ç†å¼•æ“ä½¿ç”¨é…ç½®æ¨¡å‹: {target_model}")
            
            # 2. å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰«æç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            if not p2l_model_path and os.path.exists(models_dir):
                logger.warning("æ¨ç†å¼•æ“ä½¿ç”¨å¤‡ç”¨æ‰«ææ–¹å¼")
                for item in os.listdir(models_dir):
                    if item.startswith('p2l-') and os.path.isdir(os.path.join(models_dir, item)):
                        p2l_model_path = os.path.join(models_dir, item)
                        logger.warning(f"âš ï¸ æ¨ç†å¼•æ“ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: {item}")
                        break
            
            # ä½¿ç”¨P2Læ¨ç†å¼•æ“
            if p2l_model_path:
                model, tokenizer = load_p2l_model(p2l_model_path, device=str(self.device))
            else:
                # åˆ›å»ºé»˜è®¤æ¨ç†å¼•æ“
                model, tokenizer = load_p2l_model("", device=str(self.device))
            
            if isinstance(model, P2LInferenceEngine):
                self.p2l_inference_engine = model
                logger.info("âœ… P2Læ¨ç†å¼•æ“åŠ è½½æˆåŠŸ")
            else:
                logger.warning("åŠ è½½çš„ä¸æ˜¯P2Læ¨ç†å¼•æ“ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹")
                
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¼•æ“åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºåŸºæœ¬çš„æ¨ç†å¼•æ“ä½œä¸ºåå¤‡
            try:
                self.p2l_inference_engine = P2LInferenceEngine(device=str(self.device))
                logger.info("âœ… åˆ›å»ºäº†åŸºæœ¬P2Læ¨ç†å¼•æ“")
            except Exception as e2:
                logger.error(f"âŒ åŸºæœ¬P2Læ¨ç†å¼•æ“åˆ›å»ºå¤±è´¥: {e2}")
    
    def semantic_analysis(self, prompt: str) -> tuple:
        """ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†æ"""
        if not self.p2l_models:
            logger.warning("ğŸ” P2Læ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ")
            return 0.5, 0.5
        
        try:
            model_name = list(self.p2l_models.keys())[0]
            p2l_model = self.p2l_models[model_name]
            tokenizer = p2l_model["tokenizer"]
            model = p2l_model["model"]
            is_p2l_model = p2l_model.get("is_p2l_model", False)
            
            logger.info(f"ğŸ§  ä½¿ç”¨P2Læ¨¡å‹è¿›è¡Œè¯­ä¹‰å¢å¼ºåˆ†æ (P2Lä¸“ç”¨: {is_p2l_model})...")
            
            if is_p2l_model:
                # ä½¿ç”¨çœŸæ­£çš„P2Læ¨¡å‹è¿›è¡Œæ¨ç†
                logger.info("ğŸ¯ ä½¿ç”¨P2Lä¸“ç”¨æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ")
                
                # å‡†å¤‡P2Læ¨¡å‹è¾“å…¥ - éœ€è¦æ·»åŠ CLS token
                prompt_with_cls = f"{prompt} <|cls|>"
                inputs = tokenizer(
                    prompt_with_cls, 
                    return_tensors="pt", 
                    truncation=True, 
                    padding=True, 
                    max_length=512
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    # P2Læ¨¡å‹å‰å‘ä¼ æ’­
                    outputs = model(**inputs)
                    
                    # è·å–P2Læ¨¡å‹çš„ç³»æ•°è¾“å‡º
                    if hasattr(outputs, 'coefs') and outputs.coefs is not None:
                        coefs = outputs.coefs  # [batch_size, num_models]
                        
                        # ä½¿ç”¨P2Lç³»æ•°è®¡ç®—è¯­ä¹‰ç‰¹å¾
                        coef_values = coefs[0]  # [num_models]
                        
                        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°ï¼šåŸºäºç³»æ•°çš„æ–¹å·®
                        complexity_score = torch.var(coef_values).item()
                        complexity_score = min(max(complexity_score, 0), 1)
                        
                        # è®¡ç®—è¯­è¨€åˆ†æ•°ï¼šåŸºäºç³»æ•°çš„æœ€å¤§å€¼
                        language_score = torch.max(torch.softmax(coef_values, dim=0)).item()
                        
                        # è·å–etaå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        eta_info = ""
                        if hasattr(outputs, 'eta') and outputs.eta is not None:
                            eta_value = outputs.eta[0].item()
                            eta_info = f", eta={eta_value:.3f}"
                            # ä½¿ç”¨etaè°ƒæ•´å¤æ‚åº¦
                            complexity_score = complexity_score * (1 + abs(eta_value) * 0.1)
                            complexity_score = min(complexity_score, 1)
                        
                        logger.info(f"ğŸ¯ P2Læ¨¡å‹è¾“å‡º: coefs_var={torch.var(coef_values).item():.3f}, max_prob={language_score:.3f}{eta_info}")
                        logger.info(f"ğŸ” P2Lè®¡ç®—å¾—åˆ†: complexity_score={complexity_score:.3f}, language_score={language_score:.3f}")
                        
                        return complexity_score, language_score
                    
                    else:
                        logger.warning("âš ï¸ P2Læ¨¡å‹è¾“å‡ºæ ¼å¼å¼‚å¸¸ï¼Œé™çº§åˆ°éšè—çŠ¶æ€åˆ†æ")
                        # é™çº§åˆ°éšè—çŠ¶æ€åˆ†æ
                        if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:
                            hidden_states = outputs.last_hidden_state
                            sentence_embedding = hidden_states.mean(dim=1)[0]
                            
                            feature_mean = sentence_embedding.mean().item()
                            feature_std = sentence_embedding.std().item()
                            
                            complexity_score = min(max(feature_std / (abs(feature_mean) + 1e-6), 0), 1)
                            language_score = min(max(abs(feature_mean), 0), 1)
                            
                            logger.info(f"ğŸ” éšè—çŠ¶æ€åˆ†æ: complexity_score={complexity_score:.3f}, language_score={language_score:.3f}")
                            return complexity_score, language_score
            
            else:
                # æ ‡å‡†transformersæ¨¡å‹åˆ†æ
                logger.info("ğŸ”„ ä½¿ç”¨æ ‡å‡†transformersæ¨¡å‹åˆ†æ")
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model(**inputs)
                    
                    # è·å–éšè—çŠ¶æ€ä½œä¸ºè¯­ä¹‰ç‰¹å¾
                    if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                        hidden_states = outputs.hidden_states[-1]
                        sentence_embedding = hidden_states.mean(dim=1)[0]
                        semantic_features = sentence_embedding
                    elif hasattr(outputs, 'last_hidden_state'):
                        semantic_features = outputs.last_hidden_state.mean(dim=1)[0]
                    else:
                        # ä½¿ç”¨logits
                        logits = outputs.logits
                        semantic_features = logits.mean(dim=(0, 1))
                
                # åŸºäºè¯­ä¹‰ç‰¹å¾è®¡ç®—å¤æ‚åº¦å’Œè¯­è¨€åˆ†æ•°
                feature_mean = semantic_features.mean().item()
                feature_std = semantic_features.std().item()
                feature_max = semantic_features.max().item()
                
                complexity_score = min(max((feature_std / (abs(feature_mean) + 1e-6)), 0), 1)
                language_score = min(max((feature_max / (abs(feature_mean) + 1e-6)), 0), 1)
                
                logger.info(f"ğŸ” æ ‡å‡†æ¨¡å‹åˆ†æ: complexity_score={complexity_score:.3f}, language_score={language_score:.3f}")
            
            return complexity_score, language_score
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return 0.5, 0.5
    
    def inference_engine_analysis(self, prompt: str) -> Optional[Dict]:
        """ä½¿ç”¨P2Læ¨ç†å¼•æ“è¿›è¡Œä»»åŠ¡åˆ†æ"""
        if not self.p2l_inference_engine:
            return None
        
        try:
            logger.info("ä½¿ç”¨P2Læ¨ç†å¼•æ“è¿›è¡Œä»»åŠ¡åˆ†æ...")
            result = self.p2l_inference_engine.analyze_task_complexity(prompt)
            
            # è½¬æ¢P2Læ¨ç†å¼•æ“çš„è¾“å‡ºæ ¼å¼
            task_analysis = {
                "task_type": result.get("task_type", "é€šç”¨"),
                "complexity": result.get("complexity", "ä¸­ç­‰"),
                "language": result.get("language", "ä¸­æ–‡"),
                "length": len(prompt),
                "p2l_scores": {
                    "complexity": result.get("complexity_score", 0.5),
                    "confidence": result.get("confidence", 0.8)
                }
            }
            
            logger.info(f"ğŸ§  P2Læ¨ç†å¼•æ“åˆ†æ: {task_analysis}")
            return task_analysis
            
        except Exception as e:
            logger.warning(f"P2Læ¨ç†å¼•æ“åˆ†æå¤±è´¥: {e}")
            return None
    
    def code_inference(self, code: str, max_length: int = 512, temperature: float = 0.7) -> Dict:
        """P2Lä»£ç æ¨ç† - å°†ä»£ç è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€"""
        logger.info(f"ğŸ§  P2Læ¨ç†è¯·æ±‚: {code[:100]}...")
        
        # ä¼˜å…ˆä½¿ç”¨P2Lä¸“ç”¨æ¨¡å‹
        if self.p2l_models:
            model_name = list(self.p2l_models.keys())[0]
            p2l_model = self.p2l_models[model_name]
            
            if p2l_model.get("is_p2l_model", False):
                try:
                    logger.info("ğŸ¯ ä½¿ç”¨P2Lä¸“ç”¨æ¨¡å‹è¿›è¡Œä»£ç æ¨ç†")
                    
                    tokenizer = p2l_model["tokenizer"]
                    model = p2l_model["model"]
                    
                    # æ„é€ P2Læ¨ç†prompt
                    inference_prompt = f"è¯·åˆ†æä»¥ä¸‹ä»£ç çš„åŠŸèƒ½ï¼š\n{code}\n<|cls|>"
                    
                    inputs = tokenizer(
                        inference_prompt,
                        return_tensors="pt",
                        truncation=True,
                        padding=True,
                        max_length=max_length
                    )
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    start_time = time.time()
                    
                    with torch.no_grad():
                        outputs = model(**inputs)
                        
                        # è·å–P2Læ¨¡å‹çš„æ¨ç†ç»“æœ
                        if hasattr(outputs, 'coefs'):
                            coefs = outputs.coefs[0]  # [num_models]
                            
                            # åŸºäºç³»æ•°ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°
                            model_probs = torch.softmax(coefs, dim=0)
                            top_model_idx = torch.argmax(model_probs).item()
                            confidence = model_probs[top_model_idx].item()
                            
                            # ç®€å•çš„ä»£ç åˆ†æé€»è¾‘
                            if "def " in code or "function" in code:
                                analysis = "è¿™æ˜¯ä¸€ä¸ªå‡½æ•°å®šä¹‰"
                            elif "class " in code:
                                analysis = "è¿™æ˜¯ä¸€ä¸ªç±»å®šä¹‰"
                            elif "import " in code or "from " in code:
                                analysis = "è¿™æ˜¯æ¨¡å—å¯¼å…¥è¯­å¥"
                            elif "for " in code or "while " in code:
                                analysis = "è¿™æ˜¯å¾ªç¯æ§åˆ¶ç»“æ„"
                            elif "if " in code:
                                analysis = "è¿™æ˜¯æ¡ä»¶åˆ¤æ–­è¯­å¥"
                            else:
                                analysis = "è¿™æ˜¯ä¸€æ®µé€šç”¨ä»£ç "
                            
                            processing_time = time.time() - start_time
                            
                            return {
                                "code": code,
                                "natural_language": analysis,
                                "confidence": confidence,
                                "processing_time": processing_time,
                                "model_info": f"P2L-{p2l_model['model_type']}-{p2l_model['loss_type']}",
                                "p2l_coefs": coefs.cpu().tolist(),
                                "used_p2l_model": True
                            }
                            
                except Exception as e:
                    logger.error(f"âŒ P2Lä¸“ç”¨æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # é™çº§åˆ°P2Læ¨ç†å¼•æ“
        if self.p2l_inference_engine:
            try:
                logger.info("ğŸ”„ ä½¿ç”¨P2Læ¨ç†å¼•æ“")
                result = self.p2l_inference_engine.infer(
                    code,
                    max_length=max_length,
                    temperature=temperature
                )
                
                return {
                    "code": code,
                    "natural_language": result["natural_language"],
                    "confidence": result.get("confidence", 0.8),
                    "processing_time": result.get("processing_time", 0.0),
                    "model_info": "P2L-Inference-Engine",
                    "used_p2l_model": False
                }
            except Exception as e:
                logger.error(f"âŒ P2Læ¨ç†å¼•æ“å¤±è´¥: {e}")
        
        # æœ€åçš„é™çº§æ–¹æ¡ˆ
        logger.warning("âš ï¸ æ‰€æœ‰P2Læ¨ç†æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
        return {
            "code": code,
            "natural_language": "ä»£ç åˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨",
            "confidence": 0.1,
            "processing_time": 0.0,
            "model_info": "Fallback",
            "used_p2l_model": False
        }
    
    def get_loaded_models(self) -> Dict:
        """è·å–å·²åŠ è½½çš„P2Læ¨¡å‹ä¿¡æ¯"""
        p2l_model_info = {}
        for name, model_data in self.p2l_models.items():
            p2l_model_info[name] = {
                "is_p2l_model": model_data.get("is_p2l_model", False),
                "model_type": model_data.get("model_type", "unknown"),
                "loss_type": model_data.get("loss_type", "unknown"),
                "head_type": model_data.get("head_type", "unknown")
            }
        
        return {
            "p2l_models": list(self.p2l_models.keys()),
            "p2l_model_details": p2l_model_info,
            "inference_engine_available": self.p2l_inference_engine is not None,
            "p2l_available": P2L_AVAILABLE,
            "total_models_loaded": len(self.p2l_models)
        }