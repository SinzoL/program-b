#!/usr/bin/env python3
"""
P2Lå¼•æ“ - åŠ è½½å¹¶ä½¿ç”¨çœŸå®çš„P2Læ¨¡å‹
åŸºäºä¸‹è½½çš„ p2l-135m-grk æ¨¡å‹è¿›è¡ŒBradley-Terryç³»æ•°è®¡ç®—
"""

import json
import logging
import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import os
import sys
from pathlib import Path

# æ·»åŠ p2lè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = Path(__file__).parent
p2l_project_dir = current_dir.parent / 'p2l'
if str(p2l_project_dir) not in sys.path:
    sys.path.insert(0, str(p2l_project_dir))

logger = logging.getLogger(__name__)

@dataclass
class P2LCoefficients:
    """P2Lç³»æ•°æ•°æ®ç»“æ„"""
    model_coefficients: Dict[str, float]  # Bradley-Terryç³»æ•°
    eta: Optional[float] = None  # å¹³å±€å‚æ•°
    gamma: Optional[float] = None  # è´¨é‡å‚æ•°
    confidence_scores: Optional[Dict[str, float]] = None  # ç½®ä¿¡åº¦åˆ†æ•°
    model_list: List[str] = None  # æ¨¡å‹åˆ—è¡¨

class P2LEngine:
    """P2Lå¼•æ“ - ä½¿ç”¨ä¸‹è½½çš„çœŸå®P2Læ¨¡å‹"""
    
    def __init__(self, model_path: str = None, device: str = "cpu"):
        """
        åˆå§‹åŒ–P2Lå¼•æ“
        
        Args:
            model_path: P2Læ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.is_loaded = False
        
        # è®¾ç½®æ¨¡å‹è·¯å¾„
        if model_path is None:
            self.model_path = current_dir / "model_p2l" / "models" / "p2l-135m-grk"
        else:
            self.model_path = Path(model_path)
        
        logger.info(f"ğŸ” P2Læ¨¡å‹è·¯å¾„: {self.model_path}")
        
        # å°è¯•åŠ è½½P2Læ¨¡å‹
        try:
            self._load_p2l_model()
        except Exception as e:
            logger.warning(f"âš ï¸ P2Læ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {e}")
            self.is_loaded = False
        
    def _load_model_config(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        config_path = self.model_path / "config.json"
        training_config_path = self.model_path / "training_config.json"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                model_config = json.load(f)
            
            # åŠ è½½è®­ç»ƒé…ç½®
            if training_config_path.exists():
                with open(training_config_path, 'r', encoding='utf-8') as f:
                    training_config = json.load(f)
                model_config.update(training_config)
            
            logger.info(f"âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
            return model_config
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹é…ç½®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_model_list(self) -> List[str]:
        """åŠ è½½æ¨¡å‹åˆ—è¡¨"""
        model_list_path = self.model_path / "model_list.json"
        
        try:
            with open(model_list_path, 'r', encoding='utf-8') as f:
                model_list = json.load(f)
            
            logger.info(f"âœ… æ¨¡å‹åˆ—è¡¨åŠ è½½æˆåŠŸï¼Œå…± {len(model_list)} ä¸ªæ¨¡å‹")
            return model_list
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆ—è¡¨åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_p2l_model(self):
        """åŠ è½½P2Læ¨¡å‹å’Œtokenizer"""
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not self.model_path.exists():
            raise FileNotFoundError(f"P2Læ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        # åŠ è½½æ¨¡å‹é…ç½®å’Œæ¨¡å‹åˆ—è¡¨
        self.model_config = self._load_model_config()
        self.model_list = self._load_model_list()
        self.num_models = len(self.model_list)
        
        try:
            # å¯¼å…¥P2Læ¨¡å‹ç›¸å…³æ¨¡å—
            from p2l.model import get_p2l_model
            from transformers import AutoTokenizer
            
            logger.info(f"ğŸ” å¼€å§‹åŠ è½½P2Læ¨¡å‹...")
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
            tokenizer.truncation_side = "left"
            tokenizer.padding_side = "right"
            
            # æ·»åŠ ç‰¹æ®Štoken
            if "pad_token" not in tokenizer.special_tokens_map:
                tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
            if "cls_token" not in tokenizer.special_tokens_map:
                tokenizer.add_special_tokens({"cls_token": "<|cls|>"})
            
            logger.info(f"âœ… TokenizeråŠ è½½æˆåŠŸ")
            
            # è·å–P2Læ¨¡å‹ç±»
            model_type = self.model_config.get("model_type", "llama")
            head_type = self.model_config.get("head_type", "rk")
            loss_type = self.model_config.get("loss_type", "bag")
            
            logger.info(f"ğŸ¯ æ¨¡å‹å‚æ•°: type={model_type}, head={head_type}, loss={loss_type}")
            
            P2LModelClass = get_p2l_model(model_type, loss_type, head_type)
            
            # åŠ è½½æ¨¡å‹
            model = P2LModelClass.from_pretrained(
                str(self.model_path),
                CLS_id=tokenizer.cls_token_id,
                num_models=self.num_models,
                dtype=torch.bfloat16 if self.device != "cpu" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.device != "cuda":
                model = model.to(self.device)
            
            model.eval()
            
            self.model = model
            self.tokenizer = tokenizer
            self.is_loaded = True
            
            logger.info(f"âœ… P2Læ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"ğŸ“Š æ”¯æŒæ¨¡å‹æ•°é‡: {self.num_models}")
            logger.info(f"ğŸ¯ æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
            logger.info(f"ğŸ¯ æ¨¡å‹ç²¾åº¦: {next(model.parameters()).dtype}")
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_bradley_terry_coefficients(self, prompt: str, model_list: List[str]) -> np.ndarray:
        """
        è·å–Bradley-Terryç³»æ•°
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            model_list: è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨
            
        Returns:
            np.ndarray: Bradley-Terryç³»æ•°æ•°ç»„
        """
        print(f"ğŸ¯ ã€P2Lå¼•æ“ã€‘è®¡ç®—Bradley-Terryç³»æ•°...")
        print(f"ğŸ“ æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}")
        print(f"ğŸ“‹ ç›®æ ‡æ¨¡å‹æ•°é‡: {len(model_list)}")
        
        if not self.is_loaded:
            print(f"âš ï¸ P2Læ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°")
            logger.warning("P2Læ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°")
            return self._generate_mock_coefficients(len(model_list))
        
        try:
            # è·å–å®Œæ•´çš„P2Lç³»æ•°å¯¹è±¡
            coefficients = self.get_coefficients_for_prompt(prompt, model_list)
            
            # æå–ç³»æ•°æ•°ç»„
            coef_array = np.array([
                coefficients.model_coefficients.get(model, 0.5) 
                for model in model_list
            ])
            
            print(f"âœ… P2Læ¨ç†æˆåŠŸï¼Œè·å¾—{len(coef_array)}ä¸ªç³»æ•°")
            print(f"ğŸ“Š ç³»æ•°èŒƒå›´: [{coef_array.min():.3f}, {coef_array.max():.3f}]")
            
            return coef_array
            
        except Exception as e:
            print(f"âŒ P2Læ¨ç†å¤±è´¥: {e}")
            logger.error(f"P2Læ¨ç†å¤±è´¥: {e}")
            return self._generate_mock_coefficients(len(model_list))
    
    def get_coefficients_for_prompt(self, prompt: str, models: List[str] = None) -> P2LCoefficients:
        """
        ä½¿ç”¨çœŸå®P2Læ¨¡å‹è®¡ç®—Bradley-Terryç³»æ•°
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            models: è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰€æœ‰æ¨¡å‹
            
        Returns:
            P2LCoefficients: P2Lç³»æ•°å¯¹è±¡
        """
        if not self.is_loaded:
            # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›æ¨¡æ‹Ÿç³»æ•°
            if models is None:
                models = ["gpt-4o", "claude-3.5-sonnet", "gemini-pro"]
            
            mock_coefs = self._generate_mock_coefficients(len(models))
            model_coefficients = {model: float(coef) for model, coef in zip(models, mock_coefs)}
            confidence_scores = {model: 0.5 for model in models}
            
            return P2LCoefficients(
                model_coefficients=model_coefficients,
                eta=0.1,
                gamma=1.0,
                confidence_scores=confidence_scores,
                model_list=models
            )
        
        try:
            logger.info(f"ğŸ” å¼€å§‹P2Læ¨ç†...")
            logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)}")
            
            # å‡†å¤‡è¾“å…¥
            messages = [{"role": "user", "content": prompt}]
            
            # ä½¿ç”¨chat templateæ ¼å¼åŒ–
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False,
                add_special_tokens=False,
            )
            
            # æ·»åŠ CLS token
            formatted_prompt = formatted_prompt + self.tokenizer.cls_token
            
            logger.info(f"ğŸ¯ æ ¼å¼åŒ–æç¤ºè¯: {formatted_prompt[:100]}...")
            
            # Tokenize
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                max_length=8192,
                padding=True,
                truncation=True,
                add_special_tokens=False
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            logger.info(f"ğŸ¯ è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"]
                )
            
            # æå–ç³»æ•°
            coefs = outputs.coefs.cpu().float().numpy()[0]  # [num_models]
            eta = outputs.eta.cpu().float().item() if outputs.eta is not None else None
            gamma = outputs.gamma.cpu().float().item() if outputs.gamma is not None else None
            
            logger.info(f"âœ… P2Læ¨ç†å®Œæˆ")
            logger.info(f"ğŸ¯ ç³»æ•°å½¢çŠ¶: {coefs.shape}")
            logger.info(f"ğŸ¯ Etaå‚æ•°: {eta}")
            logger.info(f"ğŸ¯ Gammaå‚æ•°: {gamma}")
            
            # åˆ›å»ºæ¨¡å‹ç³»æ•°å­—å…¸
            if models is None:
                target_models = self.model_list
            else:
                target_models = models
            
            model_coefficients = {}
            confidence_scores = {}
            
            for model_name in target_models:
                if model_name in self.model_list:
                    model_idx = self.model_list.index(model_name)
                    coef_value = float(coefs[model_idx])
                    model_coefficients[model_name] = coef_value
                    
                    # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆåŸºäºç³»æ•°çš„sigmoidå˜æ¢ï¼‰
                    confidence = float(torch.sigmoid(torch.tensor(coef_value)).item())
                    confidence_scores[model_name] = confidence
                else:
                    logger.warning(f"âš ï¸ æ¨¡å‹ {model_name} ä¸åœ¨P2Læ¨¡å‹åˆ—è¡¨ä¸­")
            
            logger.info(f"ğŸ“Š æˆåŠŸè®¡ç®— {len(model_coefficients)} ä¸ªæ¨¡å‹çš„ç³»æ•°")
            
            # æ˜¾ç¤ºå‰5ä¸ªç³»æ•°ç”¨äºè°ƒè¯•
            sorted_coefs = sorted(model_coefficients.items(), key=lambda x: x[1], reverse=True)
            logger.info(f"ğŸ† å‰5åæ¨¡å‹ç³»æ•°:")
            for i, (model, coef) in enumerate(sorted_coefs[:5]):
                logger.info(f"   {i+1}. {model}: {coef:.4f}")
            
            return P2LCoefficients(
                model_coefficients=model_coefficients,
                eta=eta,
                gamma=gamma,
                confidence_scores=confidence_scores,
                model_list=target_models
            )
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¤±è´¥: {e}")
            raise
    
    def calculate_win_probabilities(self, coefficients: P2LCoefficients, 
                                  model_pairs: List[Tuple[str, str]]) -> Dict[Tuple[str, str], Dict[str, float]]:
        """
        ä½¿ç”¨P2Lç³»æ•°è®¡ç®—æ¨¡å‹å¯¹ä¹‹é—´çš„èƒœç‡æ¦‚ç‡
        ä½¿ç”¨GRK (Generalized Rao-Kupper) æ¨¡å‹
        """
        probabilities = {}
        
        # ä½¿ç”¨çœŸå®çš„etaå‚æ•°
        eta = coefficients.eta if coefficients.eta is not None else 0.1
        theta = np.exp(eta) + 1.000001
        
        for model_a, model_b in model_pairs:
            if model_a in coefficients.model_coefficients and model_b in coefficients.model_coefficients:
                coef_a = coefficients.model_coefficients[model_a]
                coef_b = coefficients.model_coefficients[model_b]
                
                # GRKæ¨¡å‹è®¡ç®—æ¦‚ç‡
                pi_a = np.exp(coef_a)
                pi_b = np.exp(coef_b)
                pi_gamma = 1.0  # bagæ¨¡å‹ä¸­gammaå›ºå®šä¸º1
                
                # è®¡ç®—å„ç§ç»“æœçš„æ¦‚ç‡
                p_win = pi_a / (pi_a + theta * pi_b + pi_gamma)
                p_lose = pi_b / (pi_b + theta * pi_a + pi_gamma)
                p_tie_bb = pi_gamma / (pi_gamma + pi_a + pi_b)  # åŒæ–¹éƒ½ä¸å¥½çš„å¹³å±€
                p_tie = 1.0 - p_win - p_lose - p_tie_bb  # æ­£å¸¸å¹³å±€
                
                probabilities[(model_a, model_b)] = {
                    "win": float(p_win),
                    "lose": float(p_lose),
                    "tie": float(p_tie),
                    "tie_bothbad": float(p_tie_bb)
                }
        
        return probabilities
    
    def get_model_rankings(self, coefficients: P2LCoefficients) -> List[Tuple[str, float]]:
        """è·å–åŸºäºP2Lç³»æ•°çš„æ¨¡å‹æ’å"""
        rankings = [(model, coef) for model, coef in coefficients.model_coefficients.items()]
        rankings.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"ğŸ“Š æ¨¡å‹æ’åè®¡ç®—å®Œæˆï¼Œå‰3å:")
        for i, (model, coef) in enumerate(rankings[:3]):
            logger.info(f"   {i+1}. {model}: {coef:.4f}")
        
        return rankings
    
    def _generate_mock_coefficients(self, num_models: int) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„Bradley-Terryç³»æ•°"""
        print(f"ğŸ² ç”Ÿæˆ{num_models}ä¸ªæ¨¡æ‹ŸBradley-Terryç³»æ•°...")
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
        np.random.seed(42)
        
        # ç”Ÿæˆç¬¦åˆå®é™…åˆ†å¸ƒçš„ç³»æ•°
        # Bradley-Terryç³»æ•°é€šå¸¸åœ¨0.2-1.5ä¹‹é—´ï¼Œå¤§å¤šæ•°åœ¨0.4-1.2ä¹‹é—´
        coefficients = np.random.beta(2, 2, num_models) * 1.0 + 0.2
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§ï¼Œä½†ä¿æŒåˆç†èŒƒå›´
        coefficients += np.random.normal(0, 0.1, num_models)
        coefficients = np.clip(coefficients, 0.2, 1.5)
        
        print(f"ğŸ“Š æ¨¡æ‹Ÿç³»æ•°ç»Ÿè®¡: æœ€å°={coefficients.min():.3f}, æœ€å¤§={coefficients.max():.3f}, å¹³å‡={coefficients.mean():.3f}")
        
        return coefficients
    
    def get_supported_models(self) -> List[str]:
        """è·å–P2Læ¨¡å‹æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹åˆ—è¡¨"""
        if self.is_loaded:
            return self.model_list.copy()
        else:
            return []
    
    def check_model_support(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«P2Læ”¯æŒ"""
        if self.is_loaded:
            return model_name in self.model_list
        else:
            return False
    
    def get_debug_info(self, prompt: str, models: List[str] = None) -> Dict:
        """è·å–è°ƒè¯•ä¿¡æ¯"""
        if not self.is_loaded:
            return {
                "engine_status": "æ¨¡æ‹Ÿæ¨¡å¼",
                "model_loaded": False,
                "prompt_length": len(prompt),
                "target_models": len(models) if models else 0
            }
        
        coefficients = self.get_coefficients_for_prompt(prompt, models)
        
        return {
            "model_path": str(self.model_path),
            "model_type": self.model_config.get("model_type", "unknown"),
            "head_type": self.model_config.get("head_type", "unknown"),
            "loss_type": self.model_config.get("loss_type", "unknown"),
            "total_models_supported": self.num_models,
            "prompt_length": len(prompt),
            "eta": coefficients.eta,
            "gamma": coefficients.gamma,
            "model_count": len(coefficients.model_coefficients),
            "coefficients": coefficients.model_coefficients,
            "confidence_scores": coefficients.confidence_scores,
            "device": str(self.device),
            "model_device": str(next(self.model.parameters()).device) if self.is_loaded else "N/A",
            "model_dtype": str(next(self.model.parameters()).dtype) if self.is_loaded else "N/A"
        }
    
    def get_model_info(self) -> Dict:
        """è·å–P2Læ¨¡å‹ä¿¡æ¯"""
        if not self.is_loaded:
            return {
                "engine_type": "P2L Engine (æ¨¡æ‹Ÿæ¨¡å¼)",
                "model_loaded": False,
                "status": "P2Læ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°"
            }
        
        return {
            "engine_type": "P2L Engine (çœŸå®æ¨¡å¼)",
            "model_path": str(self.model_path),
            "base_model": self.model_config.get("_name_or_path", "unknown"),
            "model_type": self.model_config.get("model_type", "unknown"),
            "head_type": self.model_config.get("head_type", "unknown"),
            "loss_type": self.model_config.get("loss_type", "unknown"),
            "supported_models": self.num_models,
            "device": self.device,
            "parameters": "135M",
            "architecture": "SmolLM2 + P2L Head",
            "features": [
                "çœŸå®Bradley-Terryç³»æ•°è®¡ç®—",
                "GRKæ¦‚ç‡æ¨¡å‹",
                "130+ä¸ªæ¨¡å‹æ”¯æŒ",
                "Rao-Kupperå¤´éƒ¨",
                "BAGæŸå¤±å‡½æ•°"
            ]
        }
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–P2Lå¼•æ“çŠ¶æ€"""
        return {
            "is_loaded": self.is_loaded,
            "model_path": str(self.model_path) if hasattr(self, 'model_path') else None,
            "supported_models": len(self.model_list) if self.is_loaded else 0,
            "device": self.device,
            "model_info": self.get_model_info()
        }
    
    def print_status(self):
        """æ‰“å°P2Lå¼•æ“çŠ¶æ€"""
        status = self.get_status()
        
        print(f"\nğŸ”§ ã€P2Lå¼•æ“çŠ¶æ€ã€‘")
        print(f"âœ… åŠ è½½çŠ¶æ€: {'å·²åŠ è½½' if status['is_loaded'] else 'æœªåŠ è½½'}")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {status['model_path']}")
        print(f"ğŸ“Š æ”¯æŒæ¨¡å‹: {status['supported_models']} ä¸ª")
        print(f"ğŸ–¥ï¸ è®¾å¤‡: {status['device']}")
        
        model_info = status['model_info']
        print(f"ğŸ¯ å¼•æ“ç±»å‹: {model_info['engine_type']}")
        
        if status['is_loaded']:
            print(f"ğŸ—ï¸ æ¶æ„: {model_info['architecture']}")
            print(f"ğŸ¯ ç‰¹æ€§: {', '.join(model_info['features'][:3])}...")

# å…¨å±€P2Lå¼•æ“å®ä¾‹
_p2l_engine = None

def get_p2l_engine() -> P2LEngine:
    """è·å–å…¨å±€P2Lå¼•æ“å®ä¾‹"""
    global _p2l_engine
    if _p2l_engine is None:
        _p2l_engine = P2LEngine()
    return _p2l_engine

def create_p2l_engine(model_path: str = None, device: str = "cpu") -> P2LEngine:
    """åˆ›å»ºæ–°çš„P2Lå¼•æ“å®ä¾‹"""
    return P2LEngine(model_path, device)

# æµ‹è¯•å‡½æ•°
def test_p2l_engine():
    """æµ‹è¯•P2Lå¼•æ“"""
    print("ğŸ§ª æµ‹è¯•P2Lå¼•æ“...")
    
    try:
        engine = P2LEngine()
        engine.print_status()
        
        # æµ‹è¯•æç¤ºè¯
        test_prompts = [
            "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•",
            "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "å¸®æˆ‘ç¿»è¯‘è¿™æ®µè‹±æ–‡ï¼šHello World"
        ]
        
        # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
        test_models = ["gpt-4o-2024-08-06", "claude-3-5-sonnet-20241022", "gemini-1.5-pro-002"]
        
        for prompt in test_prompts:
            print(f"\nğŸ“ æµ‹è¯•æç¤ºè¯: {prompt}")
            print("-" * 50)
            
            # æµ‹è¯•Bradley-Terryç³»æ•°è®¡ç®—
            coefficients_array = engine.get_bradley_terry_coefficients(prompt, test_models)
            print(f"ğŸ“Š Bradley-Terryç³»æ•°: {coefficients_array}")
            
            # æµ‹è¯•å®Œæ•´ç³»æ•°å¯¹è±¡
            if engine.is_loaded:
                coefficients = engine.get_coefficients_for_prompt(prompt, test_models)
                rankings = engine.get_model_rankings(coefficients)
                
                print(f"ğŸ† å‰3åæ¨¡å‹:")
                for i, (model, coef) in enumerate(rankings[:3]):
                    confidence = coefficients.confidence_scores.get(model, 0.5)
                    print(f"   {i+1}. {model}: ç³»æ•°={coef:.4f}, ç½®ä¿¡åº¦={confidence:.3f}")
        
        print(f"\nâœ… P2Lå¼•æ“æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_p2l_engine()