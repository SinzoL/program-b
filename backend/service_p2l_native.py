#!/usr/bin/env python3
"""
P2LåŸç”Ÿåç«¯æœåŠ¡
å®Œå…¨åŸºäºP2Læ¨¡å‹çš„Bradley-Terryç³»æ•°è¿›è¡Œæ™ºèƒ½è·¯ç”±
"""

import os
import sys
import asyncio
import logging
import time
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional

# å¯¼å…¥é¡¹ç›®æ ¸å¿ƒæ¨¡å—
try:
    from model_p2l.p2l_core import DEFAULT_MODEL, MODEL_MAPPING, get_backend_status, print_backend_status
except ImportError:
    # å¤‡ç”¨å¯¼å…¥è·¯å¾„
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    try:
        from p2l_core import DEFAULT_MODEL, MODEL_MAPPING, get_backend_status, print_backend_status
    except ImportError:
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        DEFAULT_MODEL = "p2l-135m-grk-01112025"
        MODEL_MAPPING = {}
        def get_backend_status(): return {"p2l_ready": False}
        def print_backend_status(): print("âš ï¸ P2Læ ¸å¿ƒæ¨¡å—æœªæ‰¾åˆ°")

# é…ç½®æ—¥å¿—
try:
    from .config import get_service_config, load_env_config, get_all_models, get_model_config
except ImportError:
    from config import get_service_config, load_env_config, get_all_models, get_model_config

# åŠ è½½ç¯å¢ƒé…ç½®
load_env_config()

service_config = get_service_config()
logging.basicConfig(
    level=getattr(logging, service_config["logging"]["level"]),
    format=service_config["logging"]["format"]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥P2LåŸç”Ÿæ¨¡å—
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from .p2l_engine import P2LEngine
    from .p2l_model_scorer import P2LModelScorer  # æ–°çš„P2LåŸç”Ÿè¯„åˆ†å™¨
    from .unified_client import UnifiedLLMClient
    logger.info("âœ… P2LåŸç”Ÿæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from p2l_engine import P2LEngine
        from p2l_model_scorer import P2LModelScorer
        from unified_client import UnifiedLLMClient
        logger.info("âœ… P2LåŸç”Ÿæ¨¡å—å¯¼å…¥æˆåŠŸ (ç»å¯¹å¯¼å…¥)")
    except ImportError as e2:
        logger.error(f"âŒ P2LåŸç”Ÿæ¨¡å—å¯¼å…¥å¤±è´¥: {e2}")
        # è®¾ç½®é»˜è®¤å€¼ä»¥é¿å…NameError
        P2LEngine = None
        P2LModelScorer = None
        UnifiedLLMClient = None
        logger.warning("âš ï¸  éƒ¨åˆ†æ¨¡å—å¯¼å…¥å¤±è´¥ï¼ŒæœåŠ¡å¯èƒ½åŠŸèƒ½å—é™")

# è¯·æ±‚æ¨¡å‹
class P2LAnalysisRequest(BaseModel):
    prompt: str
    priority: str = "balanced"
    enabled_models: Optional[List[str]] = None
    budget: Optional[float] = None  # æ–°å¢ï¼šé¢„ç®—çº¦æŸ

class LLMRequest(BaseModel):
    model: str
    prompt: str
    messages: Optional[List[dict]] = None
    max_tokens: Optional[int] = 2000
    temperature: Optional[float] = 0.7

class P2LInferenceRequest(BaseModel):
    code: str
    max_length: int = 512
    temperature: float = 0.7

# P2LåŸç”Ÿåç«¯æœåŠ¡
class P2LNativeBackendService:
    """P2LåŸç”Ÿåç«¯æœåŠ¡ - å®Œå…¨åŸºäºBradley-Terryç³»æ•°çš„æ™ºèƒ½è·¯ç”±"""
    
    def __init__(self):
        # è®¾å¤‡æ£€æµ‹
        self.device = self._detect_device()
        logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.all_models = get_all_models()
        self.p2l_engine = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.p2l_model_scorer = None  # P2LåŸç”Ÿè¯„åˆ†å™¨ï¼Œéœ€è¦p2l_engineåˆå§‹åŒ–ååˆ›å»º
        
        # åˆå§‹åŒ–ç»Ÿä¸€LLMå®¢æˆ·ç«¯
        self.llm_client = None
        
        # æ¨¡å‹åŠ è½½çŠ¶æ€
        self.p2l_loading = False
        self.p2l_loaded = False
        
        logger.info("ğŸš€ P2LåŸç”Ÿåç«¯æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆP2Læ¨¡å‹å°†åœ¨åå°åŠ è½½ï¼‰")
    
    def _detect_device(self) -> torch.device:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("ğŸš€ æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("ğŸš€ æ£€æµ‹åˆ°MPSï¼Œä½¿ç”¨Apple SiliconåŠ é€Ÿ")
        else:
            device = torch.device("cpu")
            logger.info("ğŸ’» ä½¿ç”¨CPUè¿è¡Œ")
        return device
    
    async def _load_p2l_model_async(self):
        """å¼‚æ­¥åŠ è½½P2Læ¨¡å‹"""
        try:
            self.p2l_loading = True
            logger.info("ğŸ”„ å¼€å§‹åå°åŠ è½½P2Læ¨¡å‹...")
            
            # åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
            loop = asyncio.get_event_loop()
            self.p2l_engine = await loop.run_in_executor(
                None, lambda: P2LEngine(self.device)
            )
            
            # åˆå§‹åŒ–P2LåŸç”Ÿè¯„åˆ†å™¨
            self.p2l_model_scorer = P2LModelScorer(
                model_configs=self.all_models,
                p2l_engine=self.p2l_engine
            )
            
            self.p2l_loaded = True
            self.p2l_loading = False
            logger.info("âœ… P2LåŸç”Ÿæ¨¡å‹å’Œè¯„åˆ†å™¨åŠ è½½å®Œæˆ")
            
        except Exception as e:
            self.p2l_loading = False
            self.p2l_loaded = False
            logger.error(f"âŒ P2Læ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.info("ğŸ’¡ æœåŠ¡å°†ä»¥é™çº§æ¨¡å¼è¿è¡Œï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    
    async def _get_llm_client(self) -> UnifiedLLMClient:
        """è·å–ç»Ÿä¸€LLMå®¢æˆ·ç«¯å®ä¾‹"""
        if self.llm_client is None:
            self.llm_client = UnifiedLLMClient()
        return self.llm_client
    
    async def analyze_prompt(self, request: P2LAnalysisRequest) -> Dict:
        """P2LåŸç”Ÿæ™ºèƒ½åˆ†æä¸»æ¥å£"""
        logger.info(f"ğŸ§  æ”¶åˆ°P2LåŸç”Ÿåˆ†æè¯·æ±‚: {request.prompt[:50]}...")
        start_time = time.time()
        
        # æ£€æŸ¥P2Læ¨¡å‹çŠ¶æ€
        if not self.p2l_loaded:
            if self.p2l_loading:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        
        try:
            # ä½¿ç”¨P2LåŸç”Ÿè¯„åˆ†å™¨è¿›è¡Œåˆ†æ
            model_rankings, routing_info = self.p2l_model_scorer.calculate_p2l_scores(
                prompt=request.prompt,
                priority=request.priority,
                enabled_models=request.enabled_models,
                budget=request.budget
            )
            
            # ç”Ÿæˆæ¨èç†ç”±
            if model_rankings:
                best_model = model_rankings[0]
                reasoning = self.p2l_model_scorer.generate_recommendation_reasoning(
                    best_model, routing_info, request.priority
                )
            else:
                reasoning = "æ— å¯ç”¨æ¨¡å‹"
            
            processing_time = round(time.time() - start_time, 3)
            
            # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            recommendations = []
            for ranking in model_rankings:
                recommendations.append({
                    "model": ranking["model"],
                    "score": ranking["score"],
                    "p2l_coefficient": ranking.get("p2l_coefficient", 0),
                    "provider": ranking["provider"],
                    "cost_per_1k": ranking["cost_per_1k"],
                    "avg_response_time": ranking["avg_response_time"],
                    "strengths": ranking["strengths"],
                    "quality_score": ranking["quality_score"]
                })
            
            # æ„å»ºä»»åŠ¡åˆ†æç»“æœï¼ˆå…¼å®¹å‰ç«¯ï¼‰
            task_analysis = {
                "complexity_score": routing_info.get("complexity_score", 0.5),
                "language_score": routing_info.get("language_score", 0.5),
                "task_type": routing_info.get("task_type", "general"),
                "estimated_tokens": routing_info.get("estimated_tokens", len(request.prompt.split()) * 1.3),
                "p2l_strategy": routing_info.get("strategy", "unknown"),
                "routing_explanation": routing_info.get("explanation", "P2LåŸç”Ÿè·¯ç”±")
            }
            
            result = {
                "task_analysis": task_analysis,
                "model_ranking": model_rankings,
                "recommendations": recommendations,
                "recommended_model": model_rankings[0]["model"] if model_rankings else None,
                "confidence": model_rankings[0]["score"] if model_rankings else 0,
                "reasoning": reasoning,
                "processing_time": processing_time,
                "device": str(self.device),
                "p2l_native": True,  # æ ‡è¯†è¿™æ˜¯P2LåŸç”Ÿç»“æœ
                "routing_info": routing_info,  # å®Œæ•´çš„è·¯ç”±ä¿¡æ¯
                # å…¼å®¹æ—§ç‰ˆæœ¬å‰ç«¯
                "recommendation": {
                    "model": model_rankings[0]["model"] if model_rankings else None,
                    "score": model_rankings[0]["score"] if model_rankings else 0,
                    "reasoning": reasoning
                }
            }
            
            logger.info(f"âœ… P2LåŸç”Ÿåˆ†æå®Œæˆï¼Œç­–ç•¥: {routing_info.get('strategy', 'unknown')}, è€—æ—¶: {processing_time}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ P2LåŸç”Ÿåˆ†æå¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"P2LåŸç”Ÿåˆ†æå¤±è´¥: {str(e)}")
    
    async def generate_llm_response(self, request: LLMRequest) -> Dict:
        """LLMå“åº”ç”Ÿæˆæ¥å£ï¼ˆä¿æŒä¸å˜ï¼‰"""
        logger.info(f"ğŸ¤– LLMè¯·æ±‚: {request.model}")
        
        try:
            client = await self._get_llm_client()
            async with client:
                # æ„å»ºkwargså‚æ•°
                kwargs = {
                    'max_tokens': request.max_tokens,
                    'temperature': request.temperature
                }
                
                # å¦‚æœæœ‰messageså‚æ•°ï¼Œä¼ é€’ç»™å®¢æˆ·ç«¯
                if request.messages:
                    kwargs['messages'] = request.messages
                
                response = await client.generate_response(
                    request.model, 
                    request.prompt,
                    **kwargs
                )
                
                return {
                    "content": response.content,
                    "model": response.model,
                    "tokens_used": response.tokens_used,
                    "cost": response.cost,
                    "response_time": response.response_time,
                    "provider": response.provider
                }
            
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            
            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_message = str(e)
            if "timeout" in error_message.lower():
                error_message = f"è¯·æ±‚è¶…æ—¶ï¼š{request.model} æ­£åœ¨å¤„ç†å¤æ‚é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•"
            elif "rate limit" in error_message.lower():
                error_message = f"APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼š{request.model} è¯·ç¨åé‡è¯•"
            elif "quota" in error_message.lower():
                error_message = f"APIé…é¢ä¸è¶³ï¼š{request.model} è¯·æ£€æŸ¥è´¦æˆ·ä½™é¢"
            elif "unauthorized" in error_message.lower():
                error_message = f"APIå¯†é’¥æ— æ•ˆï¼š{request.model} è¯·æ£€æŸ¥é…ç½®"
            else:
                error_message = f"APIæš‚æ—¶ä¸å¯ç”¨: {error_message}"
            
            return {
                "content": error_message,
                "response": error_message,  # å…¼å®¹å‰ç«¯
                "model": request.model,
                "tokens_used": 0,
                "cost": 0.0,
                "response_time": 0.0,
                "provider": "error",
                "error_type": "api_error",
                "original_error": str(e)
            }
    
    async def p2l_inference(self, request: P2LInferenceRequest) -> Dict:
        """P2Læ¨ç†æ¥å£ï¼ˆä¿æŒä¸å˜ï¼‰"""
        logger.info(f"ğŸ§  P2Læ¨ç†è¯·æ±‚")
        
        # æ£€æŸ¥P2Læ¨¡å‹çŠ¶æ€
        if not self.p2l_loaded:
            if self.p2l_loading:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        
        try:
            result = self.p2l_engine.code_inference(
                request.code,
                request.max_length,
                request.temperature
            )
            return result
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"P2Læ¨ç†å¤±è´¥: {str(e)}")
    
    def get_health_status(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        if self.p2l_loaded and self.p2l_engine:
            p2l_models = self.p2l_engine.get_loaded_models()
            p2l_models_count = len(p2l_models["p2l_models"])
            p2l_available = p2l_models["p2l_available"]
        else:
            p2l_models_count = 0
            p2l_available = False
        
        return {
            "status": "healthy",
            "p2l_models_loaded": p2l_models_count,
            "llm_models_available": len(self.all_models),
            "device": str(self.device),
            "p2l_available": p2l_available,
            "p2l_loading": self.p2l_loading,
            "p2l_loaded": self.p2l_loaded,
            "llm_client_available": True,
            "real_api_enabled": True,
            "p2l_native_scorer": self.p2l_model_scorer is not None,
            "service_type": "p2l_native"  # æ ‡è¯†æœåŠ¡ç±»å‹
        }
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return list(self.all_models.keys())

# åˆ›å»ºFastAPIåº”ç”¨
def create_app() -> FastAPI:
    """åˆ›å»ºP2LåŸç”ŸFastAPIåº”ç”¨å®ä¾‹"""
    app = FastAPI(
        title="P2L Native Backend Service", 
        version="4.0.0",
        description="å®Œå…¨åŸºäºP2Læ¨¡å‹Bradley-Terryç³»æ•°çš„æ™ºèƒ½è·¯ç”±æœåŠ¡"
    )
    
    # æ·»åŠ CORSä¸­é—´ä»¶
    cors_config = service_config["cors"]
    app.add_middleware(
        CORSMiddleware,
        allow_origins=cors_config["allow_origins"],
        allow_credentials=cors_config["allow_credentials"],
        allow_methods=cors_config["allow_methods"],
        allow_headers=cors_config["allow_headers"],
    )
    
    # åˆå§‹åŒ–P2LåŸç”ŸæœåŠ¡
    service = P2LNativeBackendService()
    
    # å¯åŠ¨äº‹ä»¶ï¼šå¼€å§‹å¼‚æ­¥åŠ è½½P2Læ¨¡å‹
    @app.on_event("startup")
    async def startup_event():
        """åº”ç”¨å¯åŠ¨æ—¶çš„å¼‚æ­¥ä»»åŠ¡"""
        if service.p2l_engine is None and not service.p2l_loading:
            asyncio.create_task(service._load_p2l_model_async())
    
    # APIè·¯ç”±
    @app.get("/health")
    async def health_check():
        """å¥åº·æ£€æŸ¥æ¥å£"""
        return service.get_health_status()
    
    @app.get("/api/health")
    async def api_health_check():
        """APIå¥åº·æ£€æŸ¥æ¥å£ (å¸¦/apiå‰ç¼€)"""
        return service.get_health_status()
    
    @app.post("/api/p2l/analyze")
    async def analyze_prompt(request: P2LAnalysisRequest):
        """P2LåŸç”Ÿæ™ºèƒ½åˆ†ææ¥å£"""
        return await service.analyze_prompt(request)
    
    @app.post("/api/llm/generate")
    async def generate_response(request: LLMRequest):
        """LLMå“åº”ç”Ÿæˆæ¥å£"""
        return await service.generate_llm_response(request)
    
    @app.post("/api/p2l/inference")
    async def p2l_inference(request: P2LInferenceRequest):
        """P2Læ¨ç†æ¥å£"""
        return await service.p2l_inference(request)
    
    @app.get("/api/models")
    async def get_models():
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return {
            "models": list(service.all_models.keys()),
            "total": len(service.all_models)
        }
    
    @app.get("/api/p2l/model-info")
    async def get_p2l_model_info():
        """è·å–P2Læ¨ç†æ¨¡å‹ä¿¡æ¯"""
        try:
            # è·å–å½“å‰é…ç½®çš„é»˜è®¤æ¨¡å‹ä¿¡æ¯
            current_model = DEFAULT_MODEL
            
            # ä»MODEL_MAPPINGè·å–local_name
            if current_model in MODEL_MAPPING:
                model_local_name = MODEL_MAPPING[current_model]["local_name"]
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ¨¡å‹åç§°æ¨å¯¼local_name
                model_local_name = current_model.replace("-01112025", "")
            
            # è·å–P2Læ¨ç†å¼•æ“å®ä¾‹
            import sys
            import os
            
            # æ·»åŠ P2Læ¨¡å—è·¯å¾„
            p2l_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'p2l')
            if p2l_path not in sys.path:
                sys.path.append(p2l_path)
            
            # å°è¯•å¯¼å…¥P2Læ¨ç†å¼•æ“
            try:
                from p2l.p2l_inference import P2LInferenceEngine
            except ImportError:
                # å¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
                import p2l.p2l_inference
                P2LInferenceEngine = p2l.p2l_inference.P2LInferenceEngine
            
            inference_engine = P2LInferenceEngine()
            
            # è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            model_info = {
                "model_name": model_local_name,
                "model_path": getattr(inference_engine, 'p2l_model_path', 'unknown'),
                "model_type": type(inference_engine.model).__name__ if inference_engine.model else "æœªåŠ è½½",
                "tokenizer_type": type(inference_engine.tokenizer).__name__ if inference_engine.tokenizer else "æœªåŠ è½½",
                "is_loaded": inference_engine.model is not None,
                "device": str(getattr(inference_engine, 'device', 'unknown')),
                "current_model_key": current_model,
                "service_type": "p2l_native",
                "native_scorer_loaded": service.p2l_model_scorer is not None
            }
            
            # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œè·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯
            if inference_engine.model and hasattr(inference_engine.model, 'config'):
                config = inference_engine.model.config
                model_info.update({
                    "architecture": getattr(config, 'architectures', ['æœªçŸ¥'])[0] if hasattr(config, 'architectures') else "æœªçŸ¥",
                    "hidden_size": getattr(config, 'hidden_size', 0),
                    "num_layers": getattr(config, 'num_hidden_layers', 0),
                    "num_attention_heads": getattr(config, 'num_attention_heads', 0),
                    "vocab_size": getattr(config, 'vocab_size', 0),
                    "max_position_embeddings": getattr(config, 'max_position_embeddings', 0)
                })
                
                # è®¡ç®—å‚æ•°é‡
                if hasattr(inference_engine.model, 'parameters'):
                    total_params = sum(p.numel() for p in inference_engine.model.parameters())
                    model_info["total_parameters"] = total_params
                    model_info["parameters_display"] = f"{total_params/1e6:.1f}M" if total_params > 1e6 else f"{total_params/1e3:.1f}K"
            
            return {
                "status": "success",
                "model_info": model_info,
                "timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"è·å–P2Læ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            
            # è·å–å½“å‰é…ç½®çš„é»˜è®¤æ¨¡å‹ä¿¡æ¯ä½œä¸ºå¤‡ç”¨
            current_model = DEFAULT_MODEL
            if current_model in MODEL_MAPPING:
                model_local_name = MODEL_MAPPING[current_model]["local_name"]
            else:
                model_local_name = current_model.replace("-01112025", "")
            
            return {
                "status": "error",
                "error": str(e),
                "model_info": {
                    "model_name": model_local_name,
                    "model_type": "æœªçŸ¥",
                    "is_loaded": False,
                    "device": "unknown",
                    "current_model_key": current_model,
                    "service_type": "p2l_native"
                },
                "timestamp": time.time()
            }
    
    # å…¼å®¹æ€§è·¯ç”± (ä¿æŒå‘åå…¼å®¹)
    @app.post("/analyze")
    async def analyze_prompt_compat(request: P2LAnalysisRequest):
        """P2LåŸç”Ÿæ™ºèƒ½åˆ†ææ¥å£ (å…¼å®¹æ€§)"""
        return await service.analyze_prompt(request)
    
    @app.post("/generate")
    async def generate_response_compat(request: LLMRequest):
        """LLMå“åº”ç”Ÿæˆæ¥å£ (å…¼å®¹æ€§)"""
        return await service.generate_llm_response(request)
    
    @app.get("/models")
    async def get_models_compat():
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (å…¼å®¹æ€§)"""
        return {
            "models": list(service.all_models.keys()),
            "total": len(service.all_models)
        }

    # Nginxä»£ç†è·¯ç”± (å»æ‰/apiå‰ç¼€åçš„è·¯ç”±)
    @app.post("/p2l/analyze")
    async def p2l_analyze_nginx(request: P2LAnalysisRequest):
        """P2LåŸç”Ÿæ™ºèƒ½åˆ†ææ¥å£ (Nginxä»£ç†)"""
        return await service.analyze_prompt(request)

    @app.post("/llm/generate")
    async def llm_generate_nginx(request: LLMRequest):
        """LLMå“åº”ç”Ÿæˆæ¥å£ (Nginxä»£ç†)"""
        return await service.generate_llm_response(request)

    @app.post("/p2l/inference")
    async def p2l_inference_nginx(request: P2LInferenceRequest):
        """P2Læ¨ç†æ¥å£ (Nginxä»£ç†)"""
        return await service.p2l_inference(request)

    @app.get("/p2l/model-info")
    async def p2l_model_info_nginx():
        """è·å–P2Læ¨ç†æ¨¡å‹ä¿¡æ¯ (Nginxä»£ç†)"""
        return await get_p2l_model_info()
    
    return app

# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    import uvicorn
    
    logger.info("ğŸš€ å¯åŠ¨P2LåŸç”Ÿåç«¯æœåŠ¡")
    
    server_config = service_config["server"]
    app = create_app()
    
    uvicorn.run(
        app, 
        host=server_config["host"], 
        port=server_config["port"], 
        log_level=server_config["log_level"],
        reload=server_config["reload"]
    )

if __name__ == "__main__":
    main()