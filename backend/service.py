#!/usr/bin/env python3
"""
P2Låç«¯æœåŠ¡ä¸»æ–‡ä»¶
ç»Ÿä¸€çš„åç«¯æœåŠ¡ï¼Œæ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—
"""

import os
import sys
import asyncio
import logging
import time
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional

# å¯¼å…¥é¡¹ç›®æ ¸å¿ƒæ¨¡å—ï¼ˆå”¯ä¸€ä¾èµ–ï¼‰
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from p2l_core import DEFAULT_MODEL, MODEL_MAPPING, get_backend_status, print_backend_status

# é…ç½®æ—¥å¿—
try:
    from .config import get_service_config, load_env_config
except ImportError:
    from config import get_service_config, load_env_config

# åŠ è½½ç¯å¢ƒé…ç½®
load_env_config()

service_config = get_service_config()
logging.basicConfig(
    level=getattr(logging, service_config["logging"]["level"]),
    format=service_config["logging"]["format"]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥åç«¯æ¨¡å—
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from .config import get_all_models, get_model_config
    from .p2l_engine import P2LEngine
    from .task_analyzer import TaskAnalyzer
    from .model_scorer import ModelScorer
    from .llm_client import LLMClient
    logger.info("âœ… æ‰€æœ‰åç«¯æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥ï¼ˆå…¼å®¹ç›´æ¥è¿è¡Œï¼‰
    try:
        from config import get_all_models, get_model_config
        from p2l_engine import P2LEngine
        from task_analyzer import TaskAnalyzer
        from model_scorer import ModelScorer
        from llm_client import LLMClient
        logger.info("âœ… æ‰€æœ‰åç«¯æ¨¡å—å¯¼å…¥æˆåŠŸ (ç»å¯¹å¯¼å…¥)")
    except ImportError as e2:
        logger.error(f"âŒ åç«¯æ¨¡å—å¯¼å…¥å¤±è´¥: {e2}")
        # è®¾ç½®é»˜è®¤å€¼ä»¥é¿å…NameError
        P2LEngine = None
        TaskAnalyzer = None
        ModelScorer = None
        LLMClient = None
        logger.warning("âš ï¸  éƒ¨åˆ†æ¨¡å—å¯¼å…¥å¤±è´¥ï¼ŒæœåŠ¡å¯èƒ½åŠŸèƒ½å—é™")

# è¯·æ±‚æ¨¡å‹
class P2LAnalysisRequest(BaseModel):
    prompt: str
    priority: str = "balanced"
    enabled_models: Optional[List[str]] = None

class LLMRequest(BaseModel):
    model: str
    prompt: str

class P2LInferenceRequest(BaseModel):
    code: str
    max_length: int = 512
    temperature: float = 0.7

# ä¸»æœåŠ¡ç±»
class P2LBackendService:
    """P2Låç«¯æœåŠ¡ - ç»Ÿä¸€ç‰ˆæœ¬"""
    
    def __init__(self):
        # è®¾å¤‡æ£€æµ‹
        self.device = self._detect_device()
        logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—ï¼ˆä¸åŠ è½½P2Læ¨¡å‹ï¼‰
        self.all_models = get_all_models()
        self.p2l_engine = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.task_analyzer = TaskAnalyzer()
        self.model_scorer = ModelScorer(self.all_models)
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = None
        
        # æ¨¡å‹åŠ è½½çŠ¶æ€
        self.p2l_loading = False
        self.p2l_loaded = False
        
        logger.info("ğŸš€ P2Låç«¯æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆP2Læ¨¡å‹å°†åœ¨åå°åŠ è½½ï¼‰")
        
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œå¯åŠ¨å¼‚æ­¥ä»»åŠ¡ï¼Œè€Œæ˜¯åœ¨FastAPIå¯åŠ¨æ—¶å¤„ç†
    
    def _detect_device(self) -> torch.device:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("ğŸš€ æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("ğŸš€ æ£€æµ‹åˆ°MPSï¼Œä½¿ç”¨Apple SiliconåŠ é€Ÿ")
        else:
            device = torch.device("cpu")
            logger.info("ğŸ’» ä½¿ç”¨CPUè¿è¡Œ")
        return device
    
    async def _load_p2l_model_async(self):
        """å¼‚æ­¥åŠ è½½P2Læ¨¡å‹"""
        try:
            self.p2l_loading = True
            logger.info("ğŸ”„ å¼€å§‹åå°åŠ è½½P2Læ¨¡å‹...")
            
            # åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
            loop = asyncio.get_event_loop()
            self.p2l_engine = await loop.run_in_executor(
                None, lambda: P2LEngine(self.device)
            )
            
            self.p2l_loaded = True
            self.p2l_loading = False
            logger.info("âœ… P2Læ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            self.p2l_loading = False
            self.p2l_loaded = False
            logger.error(f"âŒ P2Læ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.info("ğŸ’¡ æœåŠ¡å°†ä»¥é™çº§æ¨¡å¼è¿è¡Œï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    
    async def _get_llm_client(self) -> LLMClient:
        """è·å–LLMå®¢æˆ·ç«¯å®ä¾‹"""
        if self.llm_client is None:
            self.llm_client = LLMClient()
        return self.llm_client
    
    async def analyze_prompt(self, request: P2LAnalysisRequest) -> Dict:
        """P2Læ™ºèƒ½åˆ†æä¸»æ¥å£"""
        logger.info(f"ğŸ“ æ”¶åˆ°P2Låˆ†æè¯·æ±‚: {request.prompt[:50]}...")
        start_time = time.time()
        
        # æ£€æŸ¥P2Læ¨¡å‹çŠ¶æ€
        if not self.p2l_loaded:
            if self.p2l_loading:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        
        try:
            # 1. P2Lè¯­ä¹‰åˆ†æ
            complexity_score, language_score = self.p2l_engine.semantic_analysis(request.prompt)
            
            # 2. ä»»åŠ¡åˆ†æ
            task_analysis = self.task_analyzer.analyze_task(
                request.prompt, 
                complexity_score, 
                language_score
            )
            
            # 3. æ¨¡å‹è¯„åˆ†å’Œæ’åº
            model_scores = self.model_scorer.calculate_model_scores(
                task_analysis, 
                request.priority, 
                request.enabled_models
            )
            
            # 4. ç”Ÿæˆæ¨èç†ç”±
            if model_scores:
                best_model = model_scores[0]
                reasoning = self.model_scorer.generate_recommendation_reasoning(
                    best_model, task_analysis, request.priority
                )
            else:
                reasoning = "æ— å¯ç”¨æ¨¡å‹"
            
            processing_time = round(time.time() - start_time, 3)
            
            # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            recommendations = []
            for score_data in model_scores:
                model_config = get_model_config(score_data["model"])
                recommendations.append({
                    "model": score_data["model"],
                    "score": score_data["score"],
                    "provider": model_config["provider"],
                    "cost_per_1k": model_config["cost_per_1k"],
                    "avg_response_time": model_config["avg_response_time"],
                    "strengths": model_config["strengths"],
                    "quality_score": model_config["quality_score"]
                })
            
            result = {
                "task_analysis": task_analysis,
                "model_ranking": model_scores,
                "recommendations": recommendations,  # å‰ç«¯æœŸæœ›çš„å­—æ®µ
                "recommended_model": model_scores[0]["model"] if model_scores else None,
                "confidence": model_scores[0]["score"] if model_scores else 0,
                "reasoning": reasoning,
                "processing_time": processing_time,
                "device": str(self.device),
                # å…¼å®¹æ—§ç‰ˆæœ¬å‰ç«¯
                "recommendation": {
                    "model": model_scores[0]["model"] if model_scores else None,
                    "score": model_scores[0]["score"] if model_scores else 0,
                    "reasoning": reasoning
                }
            }
            
            logger.info(f"âœ… P2Låˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ P2Låˆ†æå¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"P2Låˆ†æå¤±è´¥: {str(e)}")
    
    async def generate_llm_response(self, request: LLMRequest) -> Dict:
        """LLMå“åº”ç”Ÿæˆæ¥å£"""
        logger.info(f"ğŸ¤– LLMè¯·æ±‚: {request.model}")
        
        try:
            client = await self._get_llm_client()
            async with client:
                response = await client.generate_response(
                    request.model, 
                    request.prompt
                )
                
                return {
                    "content": response.content,
                    "model": response.model,
                    "tokens_used": response.tokens_used,
                    "cost": response.cost,
                    "response_time": response.response_time,
                    "provider": response.provider
                }
            
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return {
                "content": f"APIæš‚æ—¶ä¸å¯ç”¨: {str(e)}",
                "model": request.model,
                "tokens_used": 0,
                "cost": 0.0,
                "response_time": 0.0,
                "provider": "error"
            }
    
    async def p2l_inference(self, request: P2LInferenceRequest) -> Dict:
        """P2Læ¨ç†æ¥å£"""
        logger.info(f"ğŸ§  P2Læ¨ç†è¯·æ±‚")
        
        # æ£€æŸ¥P2Læ¨¡å‹çŠ¶æ€
        if not self.p2l_loaded:
            if self.p2l_loading:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise HTTPException(status_code=503, detail="P2Læ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        
        try:
            result = self.p2l_engine.code_inference(
                request.code,
                request.max_length,
                request.temperature
            )
            return result
            
        except Exception as e:
            logger.error(f"âŒ P2Læ¨ç†å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"P2Læ¨ç†å¤±è´¥: {str(e)}")
    
    def get_health_status(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        if self.p2l_loaded and self.p2l_engine:
            p2l_models = self.p2l_engine.get_loaded_models()
            p2l_models_count = len(p2l_models["p2l_models"])
            p2l_available = p2l_models["p2l_available"]
        else:
            p2l_models_count = 0
            p2l_available = False
        
        return {
            "status": "healthy",
            "p2l_models_loaded": p2l_models_count,
            "llm_models_available": len(self.all_models),
            "device": str(self.device),
            "p2l_available": p2l_available,
            "p2l_loading": self.p2l_loading,
            "p2l_loaded": self.p2l_loaded,
            "llm_client_available": True,
            "real_api_enabled": True
        }

# åˆ›å»ºFastAPIåº”ç”¨
def create_app() -> FastAPI:
    """åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹"""
    app = FastAPI(title="P2L Backend Service - Unified", version="3.0.0")
    
    # æ·»åŠ CORSä¸­é—´ä»¶
    cors_config = service_config["cors"]
    app.add_middleware(
        CORSMiddleware,
        allow_origins=cors_config["allow_origins"],
        allow_credentials=cors_config["allow_credentials"],
        allow_methods=cors_config["allow_methods"],
        allow_headers=cors_config["allow_headers"],
    )
    
    # åˆå§‹åŒ–æœåŠ¡
    service = P2LBackendService()
    
    # å¯åŠ¨äº‹ä»¶ï¼šå¼€å§‹å¼‚æ­¥åŠ è½½P2Læ¨¡å‹
    @app.on_event("startup")
    async def startup_event():
        """åº”ç”¨å¯åŠ¨æ—¶çš„å¼‚æ­¥ä»»åŠ¡"""
        if service.p2l_engine is None and not service.p2l_loading:
            asyncio.create_task(service._load_p2l_model_async())
    
    # APIè·¯ç”±
    @app.get("/health")
    async def health_check():
        """å¥åº·æ£€æŸ¥æ¥å£"""
        return service.get_health_status()
    
    @app.get("/api/health")
    async def api_health_check():
        """APIå¥åº·æ£€æŸ¥æ¥å£ (å¸¦/apiå‰ç¼€)"""
        return service.get_health_status()
    
    @app.post("/api/p2l/analyze")
    async def analyze_prompt(request: P2LAnalysisRequest):
        """P2Læ™ºèƒ½åˆ†ææ¥å£"""
        return await service.analyze_prompt(request)
    
    @app.post("/api/llm/generate")
    async def generate_response(request: LLMRequest):
        """LLMå“åº”ç”Ÿæˆæ¥å£"""
        return await service.generate_llm_response(request)
    
    @app.post("/api/p2l/inference")
    async def p2l_inference(request: P2LInferenceRequest):
        """P2Læ¨ç†æ¥å£"""
        return await service.p2l_inference(request)
    
    @app.get("/api/models")
    async def get_models():
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return {
            "models": list(service.all_models.keys()),
            "total": len(service.all_models)
        }
    
    @app.get("/api/p2l/model-info")
    async def get_p2l_model_info():
        """è·å–P2Læ¨ç†æ¨¡å‹ä¿¡æ¯"""
        try:
            # è·å–å½“å‰é…ç½®çš„é»˜è®¤æ¨¡å‹ä¿¡æ¯
            current_model = DEFAULT_MODEL
            
            # ä»MODEL_MAPPINGè·å–local_name
            if current_model in MODEL_MAPPING:
                model_local_name = MODEL_MAPPING[current_model]["local_name"]
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ¨¡å‹åç§°æ¨å¯¼local_name
                model_local_name = current_model.replace("-01112025", "")
            
            # è·å–P2Læ¨ç†å¼•æ“å®ä¾‹
            import sys
            import os
            
            # æ·»åŠ P2Læ¨¡å—è·¯å¾„
            p2l_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'p2l')
            if p2l_path not in sys.path:
                sys.path.append(p2l_path)
            
            # å°è¯•å¯¼å…¥P2Læ¨ç†å¼•æ“
            try:
                from p2l.p2l_inference import P2LInferenceEngine
            except ImportError:
                # å¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
                import p2l.p2l_inference
                P2LInferenceEngine = p2l.p2l_inference.P2LInferenceEngine
            
            inference_engine = P2LInferenceEngine()
            
            # è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            model_info = {
                "model_name": model_local_name,  # ç›´æ¥ä½¿ç”¨local_name
                "model_path": getattr(inference_engine, 'p2l_model_path', 'unknown'),
                "model_type": type(inference_engine.model).__name__ if inference_engine.model else "æœªåŠ è½½",
                "tokenizer_type": type(inference_engine.tokenizer).__name__ if inference_engine.tokenizer else "æœªåŠ è½½",
                "is_loaded": inference_engine.model is not None,
                "device": str(getattr(inference_engine, 'device', 'unknown')),
                "current_model_key": current_model
            }
            
            # è°ƒè¯•ä¿¡æ¯
            logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - è®¾ç½®çš„model_name: {model_local_name}")
            
            # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œè·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯
            if inference_engine.model and hasattr(inference_engine.model, 'config'):
                config = inference_engine.model.config
                model_info.update({
                    "architecture": getattr(config, 'architectures', ['æœªçŸ¥'])[0] if hasattr(config, 'architectures') else "æœªçŸ¥",
                    "hidden_size": getattr(config, 'hidden_size', 0),
                    "num_layers": getattr(config, 'num_hidden_layers', 0),
                    "num_attention_heads": getattr(config, 'num_attention_heads', 0),
                    "vocab_size": getattr(config, 'vocab_size', 0),
                    "max_position_embeddings": getattr(config, 'max_position_embeddings', 0)
                })
                
                # è®¡ç®—å‚æ•°é‡ï¼ˆä½†ä¸ç”¨äºæ˜¾ç¤ºåç§°ï¼‰
                if hasattr(inference_engine.model, 'parameters'):
                    total_params = sum(p.numel() for p in inference_engine.model.parameters())
                    model_info["total_parameters"] = total_params
                    model_info["parameters_display"] = f"{total_params/1e6:.1f}M" if total_params > 1e6 else f"{total_params/1e3:.1f}K"
            
            # ç¡®ä¿model_nameå§‹ç»ˆä½¿ç”¨local_nameï¼Œä¸è¢«å…¶ä»–é€»è¾‘è¦†ç›–
            model_info["model_name"] = model_local_name
            logger.info(f"ğŸ” æœ€ç»ˆè®¾ç½®çš„model_name: {model_local_name}")
            
            return {
                "status": "success",
                "model_info": model_info,
                "timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"è·å–P2Læ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            
            # è·å–å½“å‰é…ç½®çš„é»˜è®¤æ¨¡å‹ä¿¡æ¯ä½œä¸ºå¤‡ç”¨
            current_model = DEFAULT_MODEL
            if current_model in MODEL_MAPPING:
                model_local_name = MODEL_MAPPING[current_model]["local_name"]
            else:
                model_local_name = current_model.replace("-01112025", "")
            
            return {
                "status": "error",
                "error": str(e),
                "model_info": {
                    "model_name": model_local_name,  # ç›´æ¥ä½¿ç”¨local_name
                    "model_type": "æœªçŸ¥",
                    "is_loaded": False,
                    "device": "unknown",
                    "current_model_key": current_model
                },
                "timestamp": time.time()
            }
    
    # å…¼å®¹æ€§è·¯ç”± (ä¿æŒå‘åå…¼å®¹)
    @app.post("/analyze")
    async def analyze_prompt_compat(request: P2LAnalysisRequest):
        """P2Læ™ºèƒ½åˆ†ææ¥å£ (å…¼å®¹æ€§)"""
        return await service.analyze_prompt(request)
    
    @app.post("/generate")
    async def generate_response_compat(request: LLMRequest):
        """LLMå“åº”ç”Ÿæˆæ¥å£ (å…¼å®¹æ€§)"""
        return await service.generate_llm_response(request)
    
    @app.get("/models")
    async def get_models_compat():
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (å…¼å®¹æ€§)"""
        return {
            "models": list(service.all_models.keys()),
            "total": len(service.all_models)
        }

    # Nginxä»£ç†è·¯ç”± (å»æ‰/apiå‰ç¼€åçš„è·¯ç”±)
    @app.post("/p2l/analyze")
    async def p2l_analyze_nginx(request: P2LAnalysisRequest):
        """P2Læ™ºèƒ½åˆ†ææ¥å£ (Nginxä»£ç†)"""
        return await service.analyze_prompt(request)

    @app.post("/llm/generate")
    async def llm_generate_nginx(request: LLMRequest):
        """LLMå“åº”ç”Ÿæˆæ¥å£ (Nginxä»£ç†)"""
        return await service.generate_llm_response(request)

    @app.post("/p2l/inference")
    async def p2l_inference_nginx(request: P2LInferenceRequest):
        """P2Læ¨ç†æ¥å£ (Nginxä»£ç†)"""
        return await service.p2l_inference(request)

    @app.get("/p2l/model-info")
    async def p2l_model_info_nginx():
        """è·å–P2Læ¨ç†æ¨¡å‹ä¿¡æ¯ (Nginxä»£ç†)"""
        return await get_p2l_model_info()
    
    return app

# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    import uvicorn
    
    logger.info("ğŸš€ å¯åŠ¨P2Låç«¯æœåŠ¡ (ç»Ÿä¸€ç‰ˆæœ¬)")
    
    server_config = service_config["server"]
    app = create_app()
    
    uvicorn.run(
        app, 
        host=server_config["host"], 
        port=server_config["port"], 
        log_level=server_config["log_level"],
        reload=server_config["reload"]
    )

if __name__ == "__main__":
    main()