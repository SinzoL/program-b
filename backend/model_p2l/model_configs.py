#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡åž‹é…ç½®æ–‡ä»¶ - é¡¹ç›®å¤–å±‚é…ç½®
åŒ…å«æ‰€æœ‰æ¨¡åž‹çš„è¯¦ç»†é…ç½®ä¿¡æ¯ï¼Œä¾›backendå’Œp2læ¨¡å—å…±åŒä½¿ç”¨
åŸºäºŽAPIæµ‹è¯•ç»“æžœæ›´æ–° (2025-01-06)
"""

# ================== æ¨¡åž‹é…ç½® ==================
MODEL_CONFIGS = {
    # ================== OpenAI æ¨¡åž‹ ==================
    "gpt-4o-2024-08-06": {
        "provider": "openai",
        "request_name": "gpt-4o-2024-08-06",
        "cost_per_1k": 0.0015,
        "max_tokens": 16384,
        "context_window": 128000,
        "avg_response_time": 1.2
    },
    "gpt-3.5-turbo-0125": {
        "provider": "openai",
        "request_name": "gpt-3.5-turbo-0125",
        "cost_per_1k": 0.001,
        "max_tokens": 4096,
        "context_window": 16385,
        "avg_response_time": 1.2,
        "verified": True
    },
    "gpt-4o-mini-2024-07-18": {
        "provider": "openai",
        "request_name": "gpt-4o-mini-2024-07-18",
        "cost_per_1k": 0.0015,
        "max_tokens": 16384,
        "context_window": 128000,
        "avg_response_time": 1.0,
        "verified": True
    },
    "gpt-4-turbo-2024-04-09": {
        "provider": "openai",
        "request_name": "gpt-4-turbo-2024-04-09",
        "cost_per_1k": 0.02,
        "max_tokens": 4096,
        "context_window": 128000,
        "avg_response_time": 2.5,
        "verified": True
    },
    
    # ================== Anthropic æ¨¡åž‹ ==================
    "claude-3-5-sonnet-20241022": {
        "provider": "anthropic",
        "request_name": "claude-3-5-sonnet-20241022",
        "cost_per_1k": 0.025,
        "max_tokens": 4096,
        "context_window": 200000,
        "avg_response_time": 2.8,
        "verified": True
    },
    "claude-3-5-haiku-20241022": {
        "provider": "anthropic",
        "request_name": "claude-3-5-haiku-20241022",
        "cost_per_1k": 0.008,
        "max_tokens": 4096,
        "context_window": 200000,
        "avg_response_time": 1.5,
        "verified": True
    },
    "claude-3-5-sonnet-20240620": {
        "provider": "anthropic",
        "request_name": "claude-3-5-sonnet-20240620",
        "cost_per_1k": 0.025,
        "max_tokens": 4096,
        "context_window": 200000,
        "avg_response_time": 2.8
    },

    
    # ================== Google æ¨¡åž‹ ==================
    "gemini-1.5-flash-001": {
        "provider": "google",
        "request_name": "gemini-1.5-flash-latest",
        "cost_per_1k": 0.015,
        "max_tokens": 8192,
        "context_window": 1000000,
        "avg_response_time": 5.3,
        "verified": True
    },
    "gemini-1.5-pro-001": {
        "provider": "google",
        "request_name": "gemini-1.5-pro",
        "cost_per_1k": 0.025,
        "max_tokens": 8192,
        "context_window": 1000000,
        "avg_response_time": 5.5,
        "verified": True
    },
    "gemini-1.5-pro-002": {
        "provider": "google",
        "request_name": "gemini-1.5-pro-latest",
        "cost_per_1k": 0.025,
        "max_tokens": 8192,
        "context_window": 1000000,
        "avg_response_time": 5.3,
        "verified": True
    },
    
    # ================== DeepSeek æ¨¡åž‹ ==================
    "deepseek-v2.5": {
        "provider": "deepseek",
        "request_name": "deepseek-chat",
        "cost_per_1k": 0.002,
        "max_tokens": 4096,
        "context_window": 32000,
        "avg_response_time": 1.8
    },
    "deepseek-v3": {
        "provider": "deepseek",
        "request_name": "deepseek-v3",
        "cost_per_1k": 0.002,
        "max_tokens": 4096,
        "context_window": 32000,
        "avg_response_time": 1.8,
        "verified": True
    },
    
    # ================== DashScope (é˜¿é‡Œäº‘) æ¨¡åž‹ ==================
    "qwen-max-0428": {
        "provider": "dashscope",
        "request_name": "qwen-max-0428",
        "cost_per_1k": 0.02,
        "max_tokens": 8192,
        "context_window": 200000,
        "avg_response_time": 2.5
    },
    "qwen-max-0919": {
        "provider": "dashscope",
        "request_name": "qwen-max-0919",
        "cost_per_1k": 0.02,
        "max_tokens": 8192,
        "context_window": 200000,
        "avg_response_time": 2.3
    },
    "qwen1.5-110b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-110b-chat",
        "cost_per_1k": 0.008,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 3.2
    },
    "qwen1.5-14b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-14b-chat",
        "cost_per_1k": 0.001,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 1.5
    },
    "qwen1.5-32b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-32b-chat",
        "cost_per_1k": 0.002,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 2.0
    },
    "qwen1.5-72b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-72b-chat",
        "cost_per_1k": 0.004,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 2.3
    },
    "qwen2-72b-instruct": {
        "provider": "dashscope",
        "request_name": "qwen2-72b-instruct",
        "cost_per_1k": 0.003,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 2.2
    },
    "qwen2.5-72b-instruct": {
        "provider": "dashscope",
        "request_name": "qwen2.5-72b-instruct",
        "cost_per_1k": 0.002,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 2.0
    },
    "qwen2.5-coder-32b-instruct": {
        "provider": "dashscope",
        "request_name": "qwen2.5-coder-32b-instruct",
        "cost_per_1k": 0.002,
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 1.8
    }
}

# ================== å·¥å…·å‡½æ•° ==================
def get_model_config(model_name: str):
    """èŽ·å–æŒ‡å®šæ¨¡åž‹çš„é…ç½®"""
    return MODEL_CONFIGS.get(model_name, {})

def get_all_models():
    """èŽ·å–æ‰€æœ‰æ¨¡åž‹é…ç½®"""
    return MODEL_CONFIGS

def get_models_by_provider(provider: str):
    """æ ¹æ®æä¾›å•†èŽ·å–æ¨¡åž‹"""
    return {
        name: config for name, config in MODEL_CONFIGS.items()
        if config.get("provider") == provider
    }

def get_verified_models():
    """èŽ·å–å·²éªŒè¯å¯ç”¨çš„æ¨¡åž‹"""
    return {
        name: config for name, config in MODEL_CONFIGS.items()
        if config.get("verified", False)
    }

def get_model_count():
    """èŽ·å–æ¨¡åž‹æ€»æ•°"""
    return len(MODEL_CONFIGS)

def get_model_names():
    """èŽ·å–æ‰€æœ‰æ¨¡åž‹åç§°åˆ—è¡¨"""
    return list(MODEL_CONFIGS.keys())

def get_request_name(model_name: str):
    """èŽ·å–æ¨¡åž‹çš„APIè¯·æ±‚åç§°"""
    config = MODEL_CONFIGS.get(model_name, {})
    return config.get("request_name", model_name)  # å¦‚æžœæ²¡æœ‰é…ç½®request_nameï¼Œåˆ™ä½¿ç”¨æ¨¡åž‹åæœ¬èº«

def get_model_with_request_info(model_name: str):
    """èŽ·å–æ¨¡åž‹é…ç½®å’Œè¯·æ±‚ä¿¡æ¯"""
    config = get_model_config(model_name)
    if config:
        return {
            "display_name": model_name,  # å±•ç¤ºç”¨çš„æ¨¡åž‹å
            "request_name": get_request_name(model_name),  # APIè¯·æ±‚ç”¨çš„æ¨¡åž‹å
            "provider": config.get("provider", "unknown"),
            "config": config
        }
    return None

def get_model_provider_info(model_name: str):
    """èŽ·å–æ¨¡åž‹çš„æä¾›å•†ä¿¡æ¯"""
    config = MODEL_CONFIGS.get(model_name, {})
    provider = config.get("provider", "unknown")
    return {
        "provider": provider,
        "request_name": config.get("request_name", model_name),
        "display_name": model_name
    }

def update_model_providers():
    """æ›´æ–°æ‰€æœ‰æ¨¡åž‹çš„æä¾›å•†ä¿¡æ¯ï¼ˆå ä½å‡½æ•°ï¼‰"""
    pass

if __name__ == "__main__":
    print(f"âœ… æ¨¡åž‹é…ç½®åŠ è½½å®Œæˆï¼Œå…± {get_model_count()} ä¸ªæ¨¡åž‹")
    
    # ç»Ÿè®¡å·²éªŒè¯çš„æ¨¡åž‹
    verified_models = get_verified_models()
    print(f"ðŸ” å·²éªŒè¯å¯ç”¨æ¨¡åž‹: {len(verified_models)} ä¸ª")
    
    print(f"\nðŸ“‹ æä¾›å•†åˆ†å¸ƒ:")
    providers = {}
    
    for config in MODEL_CONFIGS.values():
        provider = config.get("provider", "unknown")
        providers[provider] = providers.get(provider, 0) + 1
    
    for provider, count in providers.items():
        print(f"  - {provider}: {count} ä¸ªæ¨¡åž‹")
    
    print(f"\nðŸ§ª æµ‹è¯•request_nameåŠŸèƒ½:")
    test_models = ['deepseek-v2.5', 'qwen-max-0428', 'gpt-3.5-turbo-0125']
    for model in test_models:
        info = get_model_with_request_info(model)
        if info:
            print(f"  {model}:")
            print(f"    å±•ç¤ºå: {info['display_name']}")
            print(f"    è¯·æ±‚å: {info['request_name']}")
            print(f"    æä¾›å•†: {info['provider']}")
        else:
            print(f"  {model}: æœªæ‰¾åˆ°é…ç½®")
    
    print(f"\nâœ… DashScopeæ¨¡åž‹:")
    dashscope_models = get_models_by_provider("dashscope")
    for name, config in dashscope_models.items():
        request_name = config.get("request_name", name)
        print(f"  - {name} -> {request_name}")