#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡åž‹é…ç½®æ–‡ä»¶ - é¡¹ç›®å¤–å±‚é…ç½®
åŒ…å«æ‰€æœ‰æ¨¡åž‹çš„è¯¦ç»†é…ç½®ä¿¡æ¯ï¼Œä¾›backendå’Œp2læ¨¡å—å…±åŒä½¿ç”¨
åŸºäºŽAPIæµ‹è¯•ç»“æžœæ›´æ–° (2025-01-06)
æŒ‰P2Lé‡‡æ ·æƒé‡é‡æ–°æŽ’åº (æƒé‡è¶Šé«˜æŽ’åœ¨è¶Šå‰é¢)
"""

# ================== æŒ‰é‡‡æ ·æƒé‡æŽ’åºçš„æ¨¡åž‹é…ç½® ==================
MODEL_CONFIGS = {
    # ================== æƒé‡6: é¡¶çº§æ¨¡åž‹ ==================
    "gpt-4o-2024-08-06": {
        "provider": "openai",
        "request_name": "gpt-4o-2024-08-06",
        "cost_per_1k": 0.040,  # å¾ˆé«˜æˆæœ¬ï¼Œä½“çŽ°é¡¶çº§æ€§èƒ½
        "max_tokens": 16384,
        "context_window": 128000,
        "avg_response_time": 5.0,  # æ…¢ï¼Œä½†è´¨é‡æœ€é«˜
        "verified": True,
        "sampling_weight": 6
    },
    "claude-3-5-sonnet-20241022": {
        "provider": "anthropic",
        "request_name": "claude-3-5-sonnet-20241022",
        "cost_per_1k": 0.050,  # æœ€é«˜æˆæœ¬ï¼Œæœ€é«˜è´¨é‡
        "max_tokens": 4096,
        "context_window": 200000,
        "avg_response_time": 6.0,  # æœ€æ…¢ï¼Œä½†æŽ¨ç†èƒ½åŠ›æœ€å¼º
        "verified": True,
        "sampling_weight": 6
    },
    
    # ================== æƒé‡5: é«˜æ€§èƒ½æ¨¡åž‹ ==================
    "gpt-4-turbo-2024-04-09": {
        "provider": "openai",
        "request_name": "gpt-4-turbo-2024-04-09",
        "cost_per_1k": 0.025,  # ä¸­é«˜æˆæœ¬
        "max_tokens": 4096,
        "context_window": 128000,
        "avg_response_time": 3.2,  # ä¸­ç­‰é€Ÿåº¦ï¼Œå¹³è¡¡æ€§èƒ½
        "verified": True,
        "sampling_weight": 5
    },
    "claude-3-5-sonnet-20240620": {
        "provider": "anthropic",
        "request_name": "claude-3-5-sonnet-20240620",
        "cost_per_1k": 0.028,  # ç¨é«˜æˆæœ¬
        "max_tokens": 4096,
        "context_window": 200000,
        "avg_response_time": 3.8,  # è¾ƒæ…¢ä½†è´¨é‡é«˜
        "verified": True,
        "sampling_weight": 5
    },
    "gemini-1.5-pro-001": {
        "provider": "google",
        "request_name": "gemini-1.5-pro",
        "cost_per_1k": 0.020,  # ä¸­ç­‰æˆæœ¬
        "max_tokens": 8192,
        "context_window": 1000000,
        "avg_response_time": 6.5,  # æ…¢ä½†ä¸Šä¸‹æ–‡çª—å£å¤§
        "verified": True,
        "sampling_weight": 5
    },
    "gemini-1.5-pro-002": {
        "provider": "google",
        "request_name": "gemini-1.5-pro-latest",
        "cost_per_1k": 0.022,  # ç¨é«˜æˆæœ¬
        "max_tokens": 8192,
        "context_window": 1000000,
        "avg_response_time": 6.0,  # ç¨å¿«ç‰ˆæœ¬
        "verified": True,
        "sampling_weight": 5
    },
    "qwen-max-0428": {
        "provider": "dashscope",
        "request_name": "qwen-max-0428",
        "cost_per_1k": 0.018,  # ä¸­ç­‰æˆæœ¬
        "max_tokens": 8192,
        "context_window": 200000,
        "avg_response_time": 2.5,  # ä¸­ç­‰é€Ÿåº¦
        "verified": True,
        "sampling_weight": 5
    },
    "qwen-max-0919": {
        "provider": "dashscope",
        "request_name": "qwen-max-0919",
        "cost_per_1k": 0.016,  # ç¨ä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 200000,
        "avg_response_time": 2.2,  # ç¨å¿«é€Ÿåº¦
        "verified": True,
        "sampling_weight": 5
    },
    
    # ================== æƒé‡4: é«˜æ€§ä»·æ¯”æ¨¡åž‹ ==================
    "gpt-4o-mini-2024-07-18": {
        "provider": "openai",
        "request_name": "gpt-4o-mini-2024-07-18",
        "cost_per_1k": 0.008,  # æé«˜æˆæœ¬ï¼Œä¸å†æ˜¯ç»å¯¹æœ€ä¾¿å®œ
        "max_tokens": 16384,
        "context_window": 128000,
        "avg_response_time": 2.8,  # é™ä½Žé€Ÿåº¦ä¼˜åŠ¿
        "verified": True,
        "sampling_weight": 4
    },
    "claude-3-5-haiku-20241022": {
        "provider": "anthropic",
        "request_name": "claude-3-5-haiku-20241022",
        "cost_per_1k": 0.015,  # ä¸­ç­‰æˆæœ¬
        "max_tokens": 4096,
        "context_window": 200000,
        "avg_response_time": 0.5,  # æžé€Ÿï¼Œé€Ÿåº¦ä¼˜å…ˆçš„æ˜Žç¡®é€‰æ‹©
        "verified": True,
        "sampling_weight": 4
    },
    "deepseek-v3": {
        "provider": "deepseek",
        "request_name": "deepseek-v3",
        "cost_per_1k": 0.003,  # ä½Žæˆæœ¬ä¼˜åŠ¿
        "max_tokens": 4096,
        "context_window": 32000,
        "avg_response_time": 1.2,  # å¿«é€Ÿä¸”ä¾¿å®œ
        "verified": True,
        "sampling_weight": 4
    },
    "qwen1.5-110b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-110b-chat",
        "cost_per_1k": 0.015,  # è¾ƒé«˜æˆæœ¬ï¼Œä½†æ€§èƒ½å¼º
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 4.2,  # è¾ƒæ…¢ä½†è´¨é‡é«˜
        "verified": True,
        "sampling_weight": 4
    },
    "qwen1.5-72b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-72b-chat",
        "cost_per_1k": 0.010,  # ä¸­ç­‰æˆæœ¬
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 3.0,  # ä¸­ç­‰é€Ÿåº¦
        "verified": True,
        "sampling_weight": 4
    },
    "qwen2-72b-instruct": {
        "provider": "dashscope",
        "request_name": "qwen2-72b-instruct",
        "cost_per_1k": 0.008,  # è¾ƒä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 2.5,  # è¾ƒå¿«é€Ÿåº¦
        "verified": True,
        "sampling_weight": 4
    },
    "qwen2.5-72b-instruct": {
        "provider": "dashscope",
        "request_name": "qwen2.5-72b-instruct",
        "cost_per_1k": 0.006,  # ä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 1.8,  # å¿«é€Ÿ
        "verified": True,
        "sampling_weight": 4
    },
    
    # ================== æƒé‡3: å®žç”¨æ¨¡åž‹ ==================
    "gpt-3.5-turbo-0125": {
        "provider": "openai",
        "request_name": "gpt-3.5-turbo-0125",
        "cost_per_1k": 0.001,  # æžä½Žæˆæœ¬ï¼Œæˆæœ¬ä¼˜å…ˆçš„æ˜Žç¡®é€‰æ‹©
        "max_tokens": 4096,
        "context_window": 16385,
        "avg_response_time": 2.0,  # é€Ÿåº¦ä¸€èˆ¬ï¼Œä½†æˆæœ¬æžä½Ž
        "verified": True,
        "sampling_weight": 3
    },
    "gemini-1.5-flash-001": {
        "provider": "google",
        "request_name": "gemini-1.5-flash-latest",
        "cost_per_1k": 0.005,  # ä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 1000000,
        "avg_response_time": 1.5,  # å¿«é€Ÿä¸”å¤§ä¸Šä¸‹æ–‡
        "verified": True,
        "sampling_weight": 3
    },
    "deepseek-v2.5": {
        "provider": "deepseek",
        "request_name": "deepseek-chat",
        "cost_per_1k": 0.004,  # ä½Žæˆæœ¬
        "max_tokens": 4096,
        "context_window": 32000,
        "avg_response_time": 1.3,  # å¿«é€Ÿ
        "verified": True,
        "sampling_weight": 3
    },
    "qwen1.5-32b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-32b-chat",
        "cost_per_1k": 0.005,  # ä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 1.8,  # è¾ƒå¿«
        "verified": True,
        "sampling_weight": 3
    },
    "qwen2.5-coder-32b-instruct": {
        "provider": "dashscope",
        "request_name": "qwen2.5-coder-32b-instruct",
        "cost_per_1k": 0.004,  # ä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 1.6,  # å¿«é€Ÿï¼Œç¼–ç¨‹ä¸“ç”¨
        "verified": True,
        "sampling_weight": 3
    },
    
    # ================== æƒé‡2: ç»æµŽæ¨¡åž‹ ==================
    "qwen1.5-14b-chat": {
        "provider": "dashscope",
        "request_name": "qwen1.5-14b-chat",
        "cost_per_1k": 0.0005,  # æžä½Žæˆæœ¬
        "max_tokens": 8192,
        "context_window": 32000,
        "avg_response_time": 0.3,  # æžé€Ÿ
        "verified": True,
        "sampling_weight": 2
    }
}

# ================== å·¥å…·å‡½æ•° ==================
def get_model_config(model_name: str):
    """èŽ·å–æŒ‡å®šæ¨¡åž‹çš„é…ç½®"""
    return MODEL_CONFIGS.get(model_name, {})

def get_all_models():
    """èŽ·å–æ‰€æœ‰æ¨¡åž‹é…ç½®"""
    return MODEL_CONFIGS

def get_models_by_provider(provider: str):
    """æ ¹æ®æä¾›å•†èŽ·å–æ¨¡åž‹"""
    return {
        name: config for name, config in MODEL_CONFIGS.items()
        if config.get("provider") == provider
    }

def get_verified_models():
    """èŽ·å–å·²éªŒè¯å¯ç”¨çš„æ¨¡åž‹"""
    return {
        name: config for name, config in MODEL_CONFIGS.items()
        if config.get("verified", False)
    }

def get_model_count():
    """èŽ·å–æ¨¡åž‹æ€»æ•°"""
    return len(MODEL_CONFIGS)

def get_model_names():
    """èŽ·å–æ‰€æœ‰æ¨¡åž‹åç§°åˆ—è¡¨"""
    return list(MODEL_CONFIGS.keys())

def get_request_name(model_name: str):
    """èŽ·å–æ¨¡åž‹çš„APIè¯·æ±‚åç§°"""
    config = MODEL_CONFIGS.get(model_name, {})
    return config.get("request_name", model_name)  # å¦‚æžœæ²¡æœ‰é…ç½®request_nameï¼Œåˆ™ä½¿ç”¨æ¨¡åž‹åæœ¬èº«

def get_model_with_request_info(model_name: str):
    """èŽ·å–æ¨¡åž‹é…ç½®å’Œè¯·æ±‚ä¿¡æ¯"""
    config = get_model_config(model_name)
    if config:
        return {
            "display_name": model_name,  # å±•ç¤ºç”¨çš„æ¨¡åž‹å
            "request_name": get_request_name(model_name),  # APIè¯·æ±‚ç”¨çš„æ¨¡åž‹å
            "provider": config.get("provider", "unknown"),
            "config": config
        }
    return None

def get_model_provider_info(model_name: str):
    """èŽ·å–æ¨¡åž‹çš„æä¾›å•†ä¿¡æ¯"""
    config = MODEL_CONFIGS.get(model_name, {})
    provider = config.get("provider", "unknown")
    return {
        "provider": provider,
        "request_name": config.get("request_name", model_name),
        "display_name": model_name
    }

def update_model_providers():
    """æ›´æ–°æ‰€æœ‰æ¨¡åž‹çš„æä¾›å•†ä¿¡æ¯ï¼ˆå ä½å‡½æ•°ï¼‰"""
    pass

if __name__ == "__main__":
    print(f"âœ… æ¨¡åž‹é…ç½®åŠ è½½å®Œæˆï¼Œå…± {get_model_count()} ä¸ªæ¨¡åž‹")
    
    # ç»Ÿè®¡å·²éªŒè¯çš„æ¨¡åž‹
    verified_models = get_verified_models()
    print(f"ðŸ” å·²éªŒè¯å¯ç”¨æ¨¡åž‹: {len(verified_models)} ä¸ª")
    
    print(f"\nðŸ“‹ æä¾›å•†åˆ†å¸ƒ:")
    providers = {}
    
    for config in MODEL_CONFIGS.values():
        provider = config.get("provider", "unknown")
        providers[provider] = providers.get(provider, 0) + 1
    
    for provider, count in providers.items():
        print(f"  - {provider}: {count} ä¸ªæ¨¡åž‹")
    
    print(f"\nðŸ§ª æµ‹è¯•request_nameåŠŸèƒ½:")
    test_models = ['deepseek-v2.5', 'qwen-max-0428', 'gpt-3.5-turbo-0125']
    for model in test_models:
        info = get_model_with_request_info(model)
        if info:
            print(f"  {model}:")
            print(f"    å±•ç¤ºå: {info['display_name']}")
            print(f"    è¯·æ±‚å: {info['request_name']}")
            print(f"    æä¾›å•†: {info['provider']}")
        else:
            print(f"  {model}: æœªæ‰¾åˆ°é…ç½®")
    
    print(f"\nâœ… DashScopeæ¨¡åž‹:")
    dashscope_models = get_models_by_provider("dashscope")
    for name, config in dashscope_models.items():
        request_name = config.get("request_name", name)
        print(f"  - {name} -> {request_name}")