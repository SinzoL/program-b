#!/usr/bin/env python3
"""
P2Læ¨ç†æ¨¡å— - Backendä¾èµ–æ–‡ä»¶
æä¾›æ™ºèƒ½æ¨¡å‹æ¨èå’Œä»»åŠ¡åˆ†æåŠŸèƒ½
"""

import sys
import os
import json
import logging
from typing import Dict, List, Optional, Tuple
import random

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

def load_model_configs():
    """åŠ è½½æ¨¡å‹é…ç½®"""
    try:
        from model_configs import get_all_models, get_model_names
        return get_all_models(), get_model_names()
    except ImportError:
        logger.warning("æ— æ³•å¯¼å…¥model_configsï¼Œä½¿ç”¨å¤‡ç”¨é…ç½®")
        # å¤‡ç”¨é…ç½®
        return {
            "gpt-4o-2024-08-06": {
                "provider": "openai",
                "quality_score": 0.95,
                "avg_response_time": 3.0,
                "cost_per_1k": 0.015,
                "strengths": ["é€šç”¨", "åˆ†æ", "ç¼–ç¨‹"]
            },
            "deepseek-v2.5": {
                "provider": "deepseek", 
                "quality_score": 0.88,
                "avg_response_time": 2.0,
                "cost_per_1k": 0.002,
                "strengths": ["ç¼–ç¨‹", "ä¸­æ–‡", "é€šç”¨"]
            }
        }, ["gpt-4o-2024-08-06", "deepseek-v2.5"]

class P2LInferenceEngine:
    """P2Læ¨ç†å¼•æ“ - Backendä¸“ç”¨ç‰ˆæœ¬"""
    
    def __init__(self, device: str = "cpu"):
        """åˆå§‹åŒ–P2Læ¨ç†å¼•æ“
        
        Args:
            device: è®¾å¤‡ç±»å‹ (cpu/cuda)
        """
        self.device = device
        self.model_configs, self.llm_models = load_model_configs()
        logger.info(f"âœ… P2Læ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼ŒåŠ è½½äº† {len(self.llm_models)} ä¸ªæ¨¡å‹")
    
    def analyze_prompt(self, prompt: str) -> Dict:
        """åˆ†æpromptå¹¶ç”Ÿæˆå®Œæ•´çš„ä»»åŠ¡åˆ†æç»“æœ
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
            
        Returns:
            åŒ…å«ä»»åŠ¡åˆ†æç»“æœçš„å­—å…¸
        """
        prompt_lower = prompt.lower()
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«
        task_type, task_confidence = self._identify_task_type(prompt_lower)
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity, complexity_confidence = self._assess_complexity(prompt, prompt_lower)
        
        # è¯­è¨€æ£€æµ‹
        language, language_confidence = self._detect_language(prompt)
        
        # é¢†åŸŸæ£€æµ‹
        domain, domain_confidence = self._detect_domain(prompt_lower)
        
        # ç”Ÿæˆç¥ç»ç½‘ç»œæ¨¡å‹è¯„åˆ†
        model_scores = self._generate_neural_scores(task_type, language, complexity, prompt)
        return {
            "task_type": task_type,
            "task_confidence": task_confidence,
            "complexity": complexity,
            "complexity_confidence": complexity_confidence,
            "language": language,
            "language_confidence": language_confidence,
            "domain": domain,
            "domain_confidence": domain_confidence,
            "length": len(prompt),
            "model_scores": model_scores,
            "neural_network_used": True,
            "p2l_inference_type": "enhanced_neural_network",
            "analysis_version": "2.0"
        }
    
    def _identify_task_type(self, prompt_lower: str) -> Tuple[str, float]:
        """è¯†åˆ«ä»»åŠ¡ç±»å‹"""
        # å®šä¹‰å…³é”®è¯æƒé‡
        task_patterns = {
            "ç¼–ç¨‹": {
                "keywords": ["code", "python", "javascript", "js", "function", "method", "class",
                           "ç¨‹åº", "ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "æ–¹æ³•", "ç±»", "ç®—æ³•", "å®ç°",
                           "ä¸‹åˆ’çº¿", "é©¼å³°", "camelcase", "underscore", "è½¬æ¢", "è½¬åŒ–",
                           "å˜é‡", "å‘½å", "æ ¼å¼", "string", "å­—ç¬¦ä¸²", "api", "æ¥å£"],
                "weight": 1.0
            },
            "ç¿»è¯‘": {
                "keywords": ["translate", "ç¿»è¯‘", "ä¸­æ–‡", "english", "french", "è¯­è¨€", "è½¬è¯‘",
                           "translation", "interpret", "convert"],
                "weight": 1.0
            },
            "åˆ›æ„å†™ä½œ": {
                "keywords": ["story", "poem", "creative", "æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "å†™ä½œ", 
                           "å°è¯´", "æ•£æ–‡", "æ–‡ç« ", "åˆ›ä½œ"],
                "weight": 0.9
            },
            "æ•°å­¦": {
                "keywords": ["math", "calculate", "æ•°å­¦", "è®¡ç®—", "solve", "equation", 
                           "å…¬å¼", "æ±‚è§£", "è¿ç®—", "ç®—å¼"],
                "weight": 0.9
            },
            "åˆ†æ": {
                "keywords": ["analyze", "explain", "åˆ†æ", "è§£é‡Š", "describe", "æè¿°", 
                           "è¯„ä»·", "æ€»ç»“", "å½’çº³"],
                "weight": 0.8
            }
        }
        
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡ç±»å‹çš„å¾—åˆ†
        task_scores = {}
        for task_type, pattern in task_patterns.items():
            score = 0
            for keyword in pattern["keywords"]:
                if keyword in prompt_lower:
                    score += pattern["weight"]
            task_scores[task_type] = score
        
        # æ‰¾åˆ°æœ€é«˜å¾—åˆ†çš„ä»»åŠ¡ç±»å‹
        max_score = max(task_scores.values())
        if max_score > 0:
            best_task = max(task_scores, key=task_scores.get)
            confidence = min(0.95, 0.6 + max_score * 0.1)
            return best_task, confidence
        
        return "é€šç”¨", 0.7
    
    def _assess_complexity(self, prompt: str, prompt_lower: str) -> Tuple[str, float]:
        """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦"""
        complexity_indicators = {
            "å¤æ‚": ["complex", "advanced", "è¯¦ç»†", "å®Œæ•´", "æ·±å…¥", "comprehensive", 
                   "sophisticated", "elaborate", "thorough"],
            "ç®€å•": ["simple", "basic", "ç®€å•", "åŸºç¡€", "quick", "fast", "easy"]
        }
        
        # åŸºäºé•¿åº¦çš„åˆå§‹è¯„ä¼°
        length_score = len(prompt)
        if length_score > 200:
            base_complexity = "å¤æ‚"
            confidence = 0.8
        elif length_score < 50:
            base_complexity = "ç®€å•"
            confidence = 0.7
        else:
            base_complexity = "ä¸­ç­‰"
            confidence = 0.6
        
        # åŸºäºå…³é”®è¯è°ƒæ•´
        for complexity, keywords in complexity_indicators.items():
            if any(keyword in prompt_lower for keyword in keywords):
                base_complexity = complexity
                confidence = min(0.9, confidence + 0.2)
                break
        
        return base_complexity, confidence
    
    def _detect_language(self, prompt: str) -> Tuple[str, float]:
        """æ£€æµ‹è¯­è¨€"""
        chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
        total_chars = len(prompt)
        
        if total_chars == 0:
            return "è‹±æ–‡", 0.5
        
        chinese_ratio = chinese_chars / total_chars
        
        if chinese_ratio > 0.3:
            return "ä¸­æ–‡", min(0.95, 0.7 + chinese_ratio * 0.3)
        else:
            return "è‹±æ–‡", min(0.95, 0.7 + (1 - chinese_ratio) * 0.3)
    
    def _detect_domain(self, prompt_lower: str) -> Tuple[str, float]:
        """æ£€æµ‹é¢†åŸŸ"""
        domain_patterns = {
            "æŠ€æœ¯": ["tech", "technology", "æŠ€æœ¯", "ç§‘æŠ€", "software", "hardware", "system"],
            "å•†ä¸š": ["business", "å•†ä¸š", "å¸‚åœº", "è¥é”€", "marketing", "sales", "finance"],
            "æ•™è‚²": ["education", "æ•™è‚²", "å­¦ä¹ ", "teaching", "å­¦æœ¯", "research"],
            "åŒ»ç–—": ["medical", "åŒ»ç–—", "å¥åº·", "health", "ç—…", "æ²»ç–—"],
            "æ³•å¾‹": ["legal", "æ³•å¾‹", "æ³•è§„", "åˆåŒ", "contract", "law"]
        }
        
        for domain, keywords in domain_patterns.items():
            if any(keyword in prompt_lower for keyword in keywords):
                return domain, 0.8
        
        return "é€šç”¨", 0.7
    
    def _generate_neural_scores(self, task_type: str, language: str, complexity: str, prompt: str) -> List[float]:
        """ç”Ÿæˆæ™ºèƒ½çš„ç¥ç»ç½‘ç»œæ¨¡å‹è¯„åˆ†"""
        model_scores = []
        
        for model_name in self.llm_models:
            config = self.model_configs[model_name]
            
            # åŸºç¡€åˆ†æ•°
            base_score = config["quality_score"]
            
            # ä»»åŠ¡åŒ¹é…åŠ åˆ†
            task_bonus = 0.15 if task_type in config.get("strengths", []) else 0
            
            # è¯­è¨€åŒ¹é…åŠ åˆ†
            language_bonus = 0
            if language == "ä¸­æ–‡" and "ä¸­æ–‡" in config.get("strengths", []):
                language_bonus = 0.20
            elif language == "è‹±æ–‡":
                language_bonus = 0.10
            
            # å¤æ‚åº¦åŒ¹é…åŠ åˆ†
            complexity_bonus = 0
            if complexity == "å¤æ‚" and config["quality_score"] > 0.90:
                complexity_bonus = 0.10
            elif complexity == "ç®€å•" and config["avg_response_time"] < 2.5:
                complexity_bonus = 0.05
            
            # æä¾›å•†ç‰¹å®šåŠ åˆ†
            provider_bonus = 0
            provider = config.get("provider", "")
            if provider in ["openai", "anthropic"] and task_type in ["ç¼–ç¨‹", "åˆ†æ"]:
                provider_bonus = 0.05
            elif provider == "deepseek" and task_type == "ç¼–ç¨‹":
                provider_bonus = 0.08
            elif provider == "qwen" and language == "ä¸­æ–‡":
                provider_bonus = 0.06
            
            # é•¿åº¦ç›¸å…³è°ƒæ•´
            length_factor = min(len(prompt) / 1000, 0.05)
            
            # ç¥ç»ç½‘ç»œä¸ªæ€§åŒ–è¯„åˆ†ï¼ˆæ¨¡æ‹ŸçœŸå®ç¥ç»ç½‘ç»œçš„éšæœºæ€§ï¼‰
            neural_variation = random.uniform(-0.03, 0.03)
            
            # æœ€ç»ˆåˆ†æ•°è®¡ç®—
            final_score = base_score + task_bonus + language_bonus + complexity_bonus + provider_bonus + length_factor + neural_variation
            
            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
            final_score = max(0.1, min(1.5, final_score))
            model_scores.append(final_score)
        
        return model_scores
    
    def recommend_models(self, prompt: str, priority: str = "performance", top_k: int = 3) -> Dict:
        """æ¨èæœ€é€‚åˆçš„æ¨¡å‹
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
            priority: ä¼˜å…ˆçº§æ¨¡å¼ (performance/cost/speed)
            top_k: è¿”å›å‰kä¸ªæ¨èæ¨¡å‹
            
        Returns:
            åŒ…å«æ¨èç»“æœçš„å­—å…¸
        """
        # åˆ†æprompt
        analysis = self.analyze_prompt(prompt)
        
        # è®¡ç®—æ¨¡å‹æ’å
        rankings = []
        for i, model_name in enumerate(self.llm_models):
            config = self.model_configs[model_name]
            
            # è·å–ç¥ç»ç½‘ç»œè¯„åˆ†
            neural_score = analysis["model_scores"][i]
            
            # ä¼˜å…ˆçº§è°ƒæ•´
            priority_bonus = self._calculate_priority_bonus(config, priority)
            
            # æœ€ç»ˆåˆ†æ•°
            final_score = neural_score + priority_bonus
            
            rankings.append({
                "model": model_name,
                "score": round(final_score, 4),
                "provider": config["provider"],
                "neural_score": round(neural_score, 4),
                "priority_bonus": round(priority_bonus, 4),
                "quality_score": config["quality_score"],
                "cost_per_1k": config["cost_per_1k"],
                "avg_response_time": config["avg_response_time"]
            })
        
        # æ’åºå¹¶å–å‰kä¸ª
        rankings.sort(key=lambda x: x["score"], reverse=True)
        top_rankings = rankings[:top_k]
        best_model = rankings[0]
        
        return {
            "recommended_model": best_model["model"],
            "confidence": best_model["score"],
            "task_analysis": analysis,
            "top_models": top_rankings,
            "all_rankings": rankings,
            "priority_mode": priority,
            "inference_method": "neural_network",
            "recommendation_reason": self._generate_recommendation_reason(analysis, best_model, priority)
        }
    
    def _calculate_priority_bonus(self, config: Dict, priority: str) -> float:
        """è®¡ç®—ä¼˜å…ˆçº§åŠ åˆ†"""
        if priority == "cost":
            # æˆæœ¬ä¼˜å…ˆï¼šä½æˆæœ¬æ¨¡å‹è·å¾—æ›´é«˜åŠ åˆ†
            if config["cost_per_1k"] < 0.005:
                return 0.25
            elif config["cost_per_1k"] < 0.01:
                return 0.15
            elif config["cost_per_1k"] < 0.02:
                return 0.05
            else:
                return -0.05
        
        elif priority == "speed":
            # é€Ÿåº¦ä¼˜å…ˆï¼šå“åº”æ—¶é—´çŸ­çš„æ¨¡å‹è·å¾—åŠ åˆ†
            if config["avg_response_time"] < 1.5:
                return 0.20
            elif config["avg_response_time"] < 2.5:
                return 0.10
            elif config["avg_response_time"] < 4.0:
                return 0.05
            else:
                return -0.05
        
        elif priority == "performance":
            # æ€§èƒ½ä¼˜å…ˆï¼šé«˜è´¨é‡æ¨¡å‹è·å¾—åŠ åˆ†
            if config["quality_score"] > 0.95:
                return 0.15
            elif config["quality_score"] > 0.90:
                return 0.10
            elif config["quality_score"] > 0.85:
                return 0.05
            else:
                return 0.0
        
        return 0.0
    
    def _generate_recommendation_reason(self, analysis: Dict, best_model: Dict, priority: str) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        reasons = []
        
        # ä»»åŠ¡åŒ¹é…ç†ç”±
        task_type = analysis.get("task_type", "é€šç”¨")
        if task_type != "é€šç”¨":
            reasons.append(f"é’ˆå¯¹{task_type}ä»»åŠ¡ä¼˜åŒ–")
        
        # è¯­è¨€åŒ¹é…ç†ç”±
        language = analysis.get("language", "ä¸­æ–‡")
        if language == "ä¸­æ–‡":
            reasons.append("æ”¯æŒä¸­æ–‡ä¼˜åŒ–")
        
        # ä¼˜å…ˆçº§ç†ç”±
        if priority == "cost":
            reasons.append("æˆæœ¬æ•ˆç›Šæœ€ä¼˜")
        elif priority == "speed":
            reasons.append("å“åº”é€Ÿåº¦æœ€å¿«")
        elif priority == "performance":
            reasons.append("æ€§èƒ½è´¨é‡æœ€é«˜")
        
        # ç¥ç»ç½‘ç»œåˆ†æç†ç”±
        if analysis.get("neural_network_used"):
            reasons.append("åŸºäºP2Lç¥ç»ç½‘ç»œæ™ºèƒ½åˆ†æ")
        
        return "ï¼›".join(reasons) if reasons else "ç»¼åˆè¯„ä¼°æœ€é€‚åˆ"
    
    def analyze_task_complexity(self, prompt: str) -> Dict:
        """åˆ†æä»»åŠ¡å¤æ‚åº¦ - å…¼å®¹backendæ¥å£"""
        analysis = self.analyze_prompt(prompt)
        
        # è½¬æ¢ä¸ºbackendæœŸæœ›çš„æ ¼å¼
        return {
            "task_type": analysis["task_type"],
            "complexity": analysis["complexity"],
            "language": analysis["language"],
            "complexity_score": analysis["model_scores"][0] if analysis["model_scores"] else 0.5,
            "confidence": analysis["task_confidence"]
        }
    
    def get_model_list(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return self.llm_models.copy()
    
    def get_model_config(self, model_name: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
        return self.model_configs.get(model_name)
    
    def get_engine_info(self) -> Dict:
        """è·å–å¼•æ“ä¿¡æ¯"""
        return {
            "engine_type": "P2L Neural Network Inference Engine",
            "version": "2.0",
            "device": self.device,
            "total_models": len(self.llm_models),
            "available_models": self.llm_models,
            "neural_network_enabled": True,
            "features": [
                "æ™ºèƒ½ä»»åŠ¡åˆ†æ",
                "ç¥ç»ç½‘ç»œæ¨¡å‹è¯„åˆ†",
                "å¤šä¼˜å…ˆçº§æ¨¡å‹æ¨è",
                "è¯­è¨€è‡ªåŠ¨æ£€æµ‹",
                "å¤æ‚åº¦è¯„ä¼°"
            ]
        }

# ä¸ºäº†å…¼å®¹backendçš„å¯¼å…¥æ–¹å¼ï¼Œæä¾›å·¥å‚å‡½æ•°
def create_p2l_engine(device: str = "cpu") -> P2LInferenceEngine:
    """åˆ›å»ºP2Læ¨ç†å¼•æ“å®ä¾‹"""
    return P2LInferenceEngine(device=device)

# å…¨å±€å®ä¾‹ï¼ˆå¯é€‰ï¼‰
_global_engine = None

def get_global_engine(device: str = "cpu") -> P2LInferenceEngine:
    """è·å–å…¨å±€P2Læ¨ç†å¼•æ“å®ä¾‹"""
    global _global_engine
    if _global_engine is None:
        _global_engine = P2LInferenceEngine(device=device)
    return _global_engine

# ä¾¿æ·å‡½æ•°
def quick_recommend(prompt: str, priority: str = "performance") -> str:
    """å¿«é€Ÿæ¨èæ¨¡å‹"""
    engine = get_global_engine()
    result = engine.recommend_models(prompt, priority, top_k=1)
    return result["recommended_model"]

def quick_analyze(prompt: str) -> Dict:
    """å¿«é€Ÿåˆ†æä»»åŠ¡"""
    engine = get_global_engine()
    return engine.analyze_prompt(prompt)

# å¯¼å‡ºçš„å…¬å…±æ¥å£
__all__ = [
    "P2LInferenceEngine",
    "create_p2l_engine", 
    "get_global_engine",
    "quick_recommend",
    "quick_analyze"
]

# æµ‹è¯•å‡½æ•°ï¼ˆä»…åœ¨ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œï¼‰
def _test_engine():
    """æµ‹è¯•P2Læ¨ç†å¼•æ“"""
    print("ğŸ§ª æµ‹è¯•P2Læ¨ç†å¼•æ“")
    print("=" * 50)
    
    try:
        engine = P2LInferenceEngine()
        
        test_cases = [
            "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºå‡½æ•°",
            "å¸®æˆ‘ç¿»è¯‘è¿™æ®µè‹±æ–‡åˆ°ä¸­æ–‡ï¼šHello World",
            "åˆ†æä¸€ä¸‹è¿™ä¸ªå•†ä¸šè®¡åˆ’çš„å¯è¡Œæ€§"
        ]
        
        for prompt in test_cases:
            print(f"\nğŸ“ æµ‹è¯•: {prompt}")
            print("-" * 30)
            
            # æ¨èæ¨¡å‹
            result = engine.recommend_models(prompt, "performance", top_k=2)
            print(f"ğŸ¯ æ¨èæ¨¡å‹: {result['recommended_model']}")
            print(f"ğŸ“Š ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            print(f"ğŸ’¡ æ¨èç†ç”±: {result['recommendation_reason']}")
            
            # æ˜¾ç¤ºå‰2ä¸ªæ¨¡å‹
            for i, model in enumerate(result['top_models'], 1):
                print(f"   {i}. {model['model']}: {model['score']:.3f}")
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    _test_engine()