#!/usr/bin/env python3
"""
P2LåŸç”Ÿæ¨¡å‹è¯„åˆ†å™¨
å®Œå…¨åŸºäºP2Læ¨¡å‹çš„Bradley-Terryç³»æ•°è¿›è¡Œè¯„åˆ†å’Œè·¯ç”±
"""

from typing import Dict, List, Optional, Tuple
import logging
import numpy as np

try:
    from .config import get_task_config, get_model_config
    from .p2l_router import P2LRouter
except ImportError:
    from config import get_task_config, get_model_config
    from p2l_router import P2LRouter

logger = logging.getLogger(__name__)

class P2LModelScorer:
    """P2LåŸç”Ÿæ¨¡å‹è¯„åˆ†å™¨"""
    
    def __init__(self, model_configs: Dict, p2l_engine=None):
        self.model_configs = model_configs
        self.task_config = get_task_config()
        self.p2l_router = P2LRouter()
        
        # æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰å›ºå®šé¡ºåºï¼‰
        self.model_list = list(model_configs.keys())
        logger.info(f"ğŸ¯ P2Lè¯„åˆ†å™¨åˆå§‹åŒ–ï¼Œæ”¯æŒæ¨¡å‹: {self.model_list}")
        
        # åˆå§‹åŒ–P2Lå¼•æ“
        if p2l_engine is None:
            try:
                # æ·»åŠ è·¯å¾„ä»¥ç¡®ä¿èƒ½æ‰¾åˆ°P2Læ¨¡å—
                import sys
                import os
                from pathlib import Path
                
                # æ·»åŠ p2lé¡¹ç›®è·¯å¾„
                current_dir = Path(__file__).parent
                p2l_project_dir = current_dir.parent / 'p2l'
                if str(p2l_project_dir) not in sys.path:
                    sys.path.insert(0, str(p2l_project_dir))
                
                from p2l_engine import P2LEngine
                self.p2l_engine = P2LEngine(device='cpu')  # æ˜ç¡®æŒ‡å®šè®¾å¤‡
                logger.info(f"âœ… P2Lå¼•æ“åˆ›å»ºæˆåŠŸï¼ŒåŠ è½½çŠ¶æ€: {self.p2l_engine.is_loaded}")
                
                if self.p2l_engine.is_loaded:
                    logger.info(f"ğŸ‰ çœŸå®P2Læ¨¡å‹å·²åŠ è½½ï¼Œæ”¯æŒ{len(self.p2l_engine.model_list)}ä¸ªæ¨¡å‹")
                else:
                    logger.warning(f"âš ï¸ P2Læ¨¡å‹æœªåŠ è½½ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°")
                    
            except Exception as e:
                logger.error(f"âŒ P2Lå¼•æ“åˆ›å»ºå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                self.p2l_engine = None
        else:
            self.p2l_engine = p2l_engine
    
    def calculate_p2l_scores(
        self, 
        prompt: str, 
        priority: str, 
        enabled_models: Optional[List[str]] = None,
        budget: Optional[float] = None
    ) -> Tuple[List[Dict], Dict]:
        """
        ä½¿ç”¨P2Læ¨¡å‹è®¡ç®—åŸç”Ÿè¯„åˆ†
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
            priority: ä¼˜å…ˆçº§æ¨¡å¼ (performance/cost/speed/balanced)
            enabled_models: å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
            budget: é¢„ç®—çº¦æŸï¼ˆå¯é€‰ï¼‰
        
        Returns:
            (rankings, routing_info)
        """
        print("\n" + "="*80)
        print(f"ğŸ§  ã€P2Lè¯„åˆ†å¼€å§‹ã€‘")
        print(f"ğŸ“ æç¤ºè¯: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
        print(f"ğŸ¯ ä¼˜å…ˆçº§æ¨¡å¼: {priority}")
        print(f"ğŸ”§ å¯ç”¨æ¨¡å‹: {enabled_models}")
        print(f"ğŸ’° é¢„ç®—çº¦æŸ: {budget}")
        print("="*80)
        
        logger.info(f"ğŸ§  å¼€å§‹P2LåŸç”Ÿè¯„åˆ†: æ¨¡å¼={priority}")
        
        try:
            # 1. è·å–P2Læ¨¡å‹çš„Bradley-Terryç³»æ•°
            print(f"\nğŸ” ã€æ­¥éª¤1ã€‘è·å–P2Læ¨¡å‹çš„Bradley-Terryç³»æ•°...")
            p2l_coefficients = self._get_p2l_coefficients(prompt)
            print(f"ğŸ“Š Bradley-Terryç³»æ•°: {p2l_coefficients}")
            print(f"ğŸ“ˆ ç³»æ•°ç»Ÿè®¡: æœ€å¤§={p2l_coefficients.max():.3f}, æœ€å°={p2l_coefficients.min():.3f}, å¹³å‡={p2l_coefficients.mean():.3f}")
            
            # 2. ä½¿ç”¨P2Lè·¯ç”±å™¨è¿›è¡Œæ™ºèƒ½è·¯ç”±
            print(f"\nğŸ¯ ã€æ­¥éª¤2ã€‘P2Lè·¯ç”±å™¨æ™ºèƒ½è·¯ç”±...")
            print(f"ğŸ”„ è·¯ç”±æ¨¡å¼: {priority}")
            selected_model, routing_info = self.p2l_router.route_models(
                p2l_coefficients=p2l_coefficients,
                model_list=self.model_list,
                model_configs=self.model_configs,
                mode=priority,
                budget=budget,
                enabled_models=enabled_models
            )
            print(f"ğŸ† è·¯ç”±ç»“æœ: {selected_model}")
            print(f"ğŸ“‹ è·¯ç”±ä¿¡æ¯: {routing_info}")
            
            # 3. ç”Ÿæˆå®Œæ•´çš„æ¨¡å‹æ’åï¼ˆæ ¹æ®ä¼˜å…ˆæ¨¡å¼è°ƒæ•´ï¼‰
            print(f"\nğŸ“Š ã€æ­¥éª¤3ã€‘ç”Ÿæˆå®Œæ•´æ¨¡å‹æ’å...")
            rankings = self.p2l_router.generate_model_ranking(
                p2l_coefficients=p2l_coefficients,
                model_list=self.model_list,
                model_configs=self.model_configs,
                mode=priority,  # ä¼ é€’ä¼˜å…ˆæ¨¡å¼
                enabled_models=enabled_models
            )
            print(f"ğŸ“ˆ æ’åç”Ÿæˆå®Œæˆï¼Œå…±{len(rankings)}ä¸ªæ¨¡å‹")
            
            # æ‰“å°è¯¦ç»†æ’å
            print(f"\nğŸ… ã€æ¨¡å‹æ’åè¯¦æƒ…ã€‘")
            for i, ranking in enumerate(rankings[:5], 1):  # åªæ˜¾ç¤ºå‰5å
                print(f"  {i}. {ranking['model']}: è¯„åˆ†={ranking['score']:.2f}, P2Lç³»æ•°={ranking.get('p2l_coefficient', 0):.3f}")
            
            # 4. æ·»åŠ è·¯ç”±è§£é‡Š
            routing_info["explanation"] = self.p2l_router.get_routing_explanation(routing_info)
            routing_info["prompt_length"] = len(prompt)
            
            print(f"\nâœ… ã€P2Lè¯„åˆ†å®Œæˆã€‘")
            print(f"ğŸ¯ æ¨èæ¨¡å‹: {selected_model}")
            print(f"ğŸ“Š æ€»æ’åæ•°: {len(rankings)}")
            print(f"ğŸ” è·¯ç”±ç­–ç•¥: {routing_info.get('strategy', 'unknown')}")
            print(f"ğŸ’¡ è§£é‡Š: {routing_info.get('explanation', 'N/A')}")
            print("="*80)
            
            logger.info(f"âœ… P2Lè¯„åˆ†å®Œæˆ: æ¨èæ¨¡å‹={selected_model}, æ€»æ’å={len(rankings)}")
            return rankings, routing_info
            
        except Exception as e:
            print(f"\nâŒ ã€P2Lè¯„åˆ†å¤±è´¥ã€‘: {e}")
            print(f"ğŸ”„ å¯ç”¨é™çº§è¯„åˆ†...")
            logger.error(f"âŒ P2Lè¯„åˆ†å¤±è´¥: {e}")
            # é™çº§åˆ°åŸºç¡€è¯„åˆ†
            fallback_result = self._fallback_scoring(enabled_models)
            print(f"âœ… é™çº§è¯„åˆ†å®Œæˆï¼Œå…±{len(fallback_result)}ä¸ªæ¨¡å‹")
            return fallback_result, {
                "strategy": "fallback",
                "error": str(e),
                "explanation": "P2Lè¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨é™çº§è¯„åˆ†"
            }
    
    def _get_p2l_coefficients(self, prompt: str) -> np.ndarray:
        """è·å–P2Læ¨¡å‹çš„Bradley-Terryç³»æ•°"""
        print(f"\nğŸ” ã€è·å–P2Lç³»æ•°ã€‘")
        print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        if not self.p2l_engine:
            print(f"âš ï¸ P2Lå¼•æ“æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°")
            logger.warning("âš ï¸ P2Lå¼•æ“æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°")
            coefficients = self._generate_mock_coefficients()
            print(f"ğŸ² æ¨¡æ‹Ÿç³»æ•°: {coefficients}")
            return coefficients
        
        try:
            print(f"ğŸ¯ è°ƒç”¨P2Lå¼•æ“è·å–Bradley-Terryç³»æ•°...")
            logger.info("ğŸ¯ è°ƒç”¨P2Læ¨¡å‹è·å–Bradley-Terryç³»æ•°")
            
            # ä½¿ç”¨P2Lå¼•æ“è®¡ç®—ç³»æ•°
            coefficients = self.p2l_engine.get_bradley_terry_coefficients(
                prompt=prompt,
                model_list=self.model_list
            )
            
            print(f"âœ… P2Lå¼•æ“è¿”å›{len(coefficients)}ä¸ªç³»æ•°")
            return coefficients
            
        except Exception as e:
            print(f"âŒ P2Lç³»æ•°è·å–å¤±è´¥: {e}")
            print(f"ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿç³»æ•°ä½œä¸ºå¤‡ç”¨...")
            logger.error(f"âŒ P2Lç³»æ•°è·å–å¤±è´¥: {e}")
            coefficients = self._generate_mock_coefficients()
            print(f"ğŸ² å¤‡ç”¨æ¨¡æ‹Ÿç³»æ•°: {coefficients}")
            return coefficients
    
    def _generate_mock_coefficients(self) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„Bradley-Terryç³»æ•°ï¼ˆç”¨äºæµ‹è¯•å’Œé™çº§ï¼‰"""
        print(f"ğŸ² ã€ç”Ÿæˆæ¨¡æ‹ŸP2Lç³»æ•°ã€‘")
        
        # ç”ŸæˆåŸºäºæ¨¡å‹è´¨é‡çš„æ¨¡æ‹Ÿç³»æ•°
        coefficients = []
        
        # é¢„è®¾çš„æ¨¡å‹è´¨é‡è¯„åˆ†ï¼ˆåŸºäºçœŸå®æ¨¡å‹è¡¨ç°ï¼‰
        model_quality_scores = {
            "claude-3-5-sonnet-20241022": 0.85,  # é¡¶çº§æ¨¡å‹
            "gpt-4o-2024-08-06": 0.80,          # é¡¶çº§æ¨¡å‹
            "claude-3-opus-20240229": 0.78,      # é«˜è´¨é‡æ¨¡å‹
            "gpt-4-turbo-2024-04-09": 0.75,     # é«˜è´¨é‡æ¨¡å‹
            "gemini-1.5-pro-002": 0.70,         # ä¸­é«˜è´¨é‡æ¨¡å‹
            "claude-3-sonnet-20240229": 0.65,   # ä¸­ç­‰æ¨¡å‹
            "gpt-4o-mini-2024-07-18": 0.60,     # è½»é‡ä½†é«˜æ•ˆæ¨¡å‹
            "claude-3-haiku-20240307": 0.55,    # å¿«é€Ÿæ¨¡å‹
            "gemini-1.5-flash-002": 0.50,       # å¿«é€Ÿæ¨¡å‹
            "gpt-3.5-turbo-0125": 0.45          # åŸºç¡€æ¨¡å‹
        }
        
        for model_name in self.model_list:
            config = self.model_configs[model_name]
            
            # è·å–åŸºç¡€è´¨é‡è¯„åˆ†
            base_quality = model_quality_scores.get(model_name, 0.5)
            
            # æ ¹æ®æˆæœ¬æ•ˆç›Šè°ƒæ•´ï¼ˆæˆæœ¬è¶Šä½ç›¸å¯¹ä¼˜åŠ¿è¶Šå¤§ï¼‰
            cost_efficiency = max(0.1, min(2.0, 0.01 / config["cost_per_1k"]))
            cost_factor = min(1.2, 1.0 + (cost_efficiency - 1.0) * 0.1)  # è½»å¾®æˆæœ¬ä¼˜åŠ¿
            
            # æ ¹æ®å“åº”é€Ÿåº¦è°ƒæ•´ï¼ˆé€Ÿåº¦è¶Šå¿«ç›¸å¯¹ä¼˜åŠ¿è¶Šå¤§ï¼‰
            speed_efficiency = max(0.1, min(2.0, 3.0 / config["avg_response_time"]))
            speed_factor = min(1.2, 1.0 + (speed_efficiency - 1.0) * 0.1)  # è½»å¾®é€Ÿåº¦ä¼˜åŠ¿
            
            # è®¡ç®—æœ€ç»ˆç³»æ•°
            adjusted_quality = base_quality * cost_factor * speed_factor
            
            # è½¬æ¢ä¸ºBradley-Terryç³»æ•°èŒƒå›´ [0.1, 2.0]ï¼Œé¿å…è´Ÿæ•°
            # Bradley-Terryç³»æ•°åº”è¯¥æ˜¯æ­£æ•°ï¼Œè¡¨ç¤ºç›¸å¯¹å¼ºåº¦
            coef = max(0.1, min(2.0, adjusted_quality * 2.0))
            
            # æ·»åŠ å°é‡éšæœºå™ªå£°ï¼Œä½†ä¿æŒæ­£æ•°
            noise = np.random.normal(0, 0.05)
            coef = max(0.1, coef + noise)
            
            coefficients.append(coef)
            
            print(f"   {model_name}: åŸºç¡€è´¨é‡={base_quality:.2f}, "
                  f"æˆæœ¬å› å­={cost_factor:.2f}, é€Ÿåº¦å› å­={speed_factor:.2f}, "
                  f"æœ€ç»ˆç³»æ•°={coef:.3f}")
        
        coefficients = np.array(coefficients)
        print(f"âœ… æ¨¡æ‹Ÿç³»æ•°ç”Ÿæˆå®Œæˆ: èŒƒå›´[{coefficients.min():.3f}, {coefficients.max():.3f}]")
        
        logger.info("ğŸ”„ ç”Ÿæˆæ”¹è¿›çš„æ¨¡æ‹ŸP2Lç³»æ•°")
        return coefficients
    
    def _fallback_scoring(self, enabled_models: Optional[List[str]] = None) -> List[Dict]:
        """é™çº§è¯„åˆ†æ–¹æ³•"""
        scores = []
        
        models_to_score = self.model_list
        if enabled_models:
            models_to_score = [m for m in self.model_list if m in enabled_models]
        
        for model_name in models_to_score:
            config = self.model_configs[model_name]
            
            # åŸºäºP2Lç³»æ•°çš„åˆ†æ•°
            cost_factor = max(0.1, min(1.0, 0.05 / config["cost_per_1k"]))
            speed_factor = max(0.1, min(1.0, 5.0 / config["avg_response_time"]))
            score = (cost_factor * speed_factor) * 100
            
            scores.append({
                "model": model_name,
                "score": score,
                "p2l_coefficient": score / 100 - 0.5,  # è½¬æ¢ä¸ºç³»æ•°æ ¼å¼
                "config": config,
                "provider": config["provider"],
                "cost_per_1k": config["cost_per_1k"],
                "avg_response_time": config["avg_response_time"],
                "cost_per_1k": config["cost_per_1k"],
                "avg_response_time": config["avg_response_time"]
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x["score"], reverse=True)
        return scores
    
    def generate_recommendation_reasoning(
        self, 
        best_model: Dict, 
        routing_info: Dict, 
        priority: str
    ) -> str:
        """ç”ŸæˆP2Læ¨èç†ç”±"""
        model_name = best_model["model"]
        strategy = routing_info.get("strategy", "unknown")
        p2l_score = best_model.get("p2l_coefficient", 0)
        
        reasoning_parts = []
        
        # åŸºäºP2Lç³»æ•°çš„æ¨èç†ç”±
        if p2l_score > 0.3:
            reasoning_parts.append("P2Læ¨¡å‹é«˜åº¦æ¨è")
        elif p2l_score > 0:
            reasoning_parts.append("P2Læ¨¡å‹æ¨è")
        else:
            reasoning_parts.append("P2Læ¨¡å‹ä¸­ç­‰æ¨è")
        
        # åŸºäºè·¯ç”±ç­–ç•¥çš„ç†ç”±
        strategy_reasons = {
            "max_score": "æ€§èƒ½è¡¨ç°æœ€ä¼˜",
            "speed_weighted": "é€Ÿåº¦ä¸æ€§èƒ½å¹³è¡¡æœ€ä½³",
            "strict": "æˆæœ¬æ•ˆç›Šæœ€ä¼˜",
            "simple-lp": "ç»¼åˆä¼˜åŒ–æœ€ä½³",
            "optimal-lp": "Bradley-Terryæœ€ä¼˜",
            "fallback": "é™çº§é€‰æ‹©"
        }
        
        if strategy in strategy_reasons:
            reasoning_parts.append(strategy_reasons[strategy])
        
        # åŸºäºæ¨¡å‹ç‰¹æ€§çš„ç†ç”±
        config = best_model["config"]
        if priority == "cost" and config["cost_per_1k"] < 0.01:
            reasoning_parts.append("æˆæœ¬æ•ˆç›Šé«˜")
        elif priority == "speed" and config["avg_response_time"] < 2.0:
            reasoning_parts.append("å“åº”é€Ÿåº¦å¿«")
        elif priority == "performance" and config["cost_per_1k"] < 0.01:
            reasoning_parts.append("æ€§èƒ½è¡¨ç°ä¼˜ç§€")
        
        return "ï¼›".join(reasoning_parts) if reasoning_parts else "P2Læ™ºèƒ½æ¨è"