#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„ä¼˜å…ˆæ¨¡å¼è¯„åˆ†é€»è¾‘
éªŒè¯ä¸åŒä¼˜å…ˆæ¨¡å¼ä¸‹æ¨¡å‹è¯„åˆ†è®¡ç®—çš„å·®å¼‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from p2l_model_scorer import P2LModelScorer
from config import get_all_models

def test_mode_scoring_differences():
    """æµ‹è¯•ä¸åŒä¼˜å…ˆæ¨¡å¼ä¸‹çš„è¯„åˆ†å·®å¼‚"""
    print("ğŸ§ª æµ‹è¯•ä¼˜å…ˆæ¨¡å¼è¯„åˆ†å·®å¼‚")
    print("=" * 80)
    
    # è·å–æ¨¡å‹é…ç½®
    model_configs = get_all_models()
    
    # åˆ›å»ºè¯„åˆ†å™¨
    scorer = P2LModelScorer(model_configs)
    
    # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æç¤ºè¯
    test_prompt = "è¯·å¸®æˆ‘å†™ä¸€ä¸ªå¤æ‚çš„æœºå™¨å­¦ä¹ ç®—æ³•æ¥å¤„ç†å¤§è§„æ¨¡æ•°æ®"
    
    # æµ‹è¯•çš„æ¨¡å‹ï¼ˆé€‰æ‹©æœ‰ä»£è¡¨æ€§çš„å‡ ä¸ªï¼‰
    enabled_models = [
        "gpt-4o-2024-08-06",        # é«˜æ€§èƒ½é«˜æˆæœ¬
        "claude-3-5-sonnet-20241022", # é«˜æ€§èƒ½é«˜æˆæœ¬
        "gpt-4o-mini-2024-07-18",   # ä¸­æ€§èƒ½ä½æˆæœ¬
        "claude-3-haiku-20240307"    # ä½æ€§èƒ½ä½æˆæœ¬å¿«é€Ÿ
    ]
    
    # æµ‹è¯•æ‰€æœ‰ä¼˜å…ˆæ¨¡å¼
    modes = ['performance', 'cost', 'speed', 'balanced']
    
    all_results = {}
    
    for mode in modes:
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å¼: {mode.upper()}")
        print("-" * 60)
        
        try:
            rankings, routing_info = scorer.calculate_p2l_scores(
                prompt=test_prompt,
                priority=mode,
                enabled_models=enabled_models
            )
            
            all_results[mode] = {
                'rankings': rankings,
                'routing_info': routing_info
            }
            
            print(f"âœ… {mode}æ¨¡å¼æ’å:")
            for i, ranking in enumerate(rankings[:4], 1):
                model_name = ranking['model']
                adjusted_score = ranking['score']
                p2l_coef = ranking['p2l_coefficient']
                cost = ranking['cost_per_1k']
                response_time = ranking['avg_response_time']
                
                print(f"  {i}. {model_name}")
                print(f"     ç»¼åˆè¯„åˆ†: {adjusted_score:.4f}")
                print(f"     P2Lç³»æ•°: {p2l_coef:.4f}")
                print(f"     æˆæœ¬: ${cost:.4f}/1k")
                print(f"     å“åº”æ—¶é—´: {response_time:.1f}s")
                
        except Exception as e:
            print(f"âŒ {mode}æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
            all_results[mode] = {'error': str(e)}
    
    # åˆ†æè¯„åˆ†å·®å¼‚
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„åˆ†å·®å¼‚åˆ†æ")
    print("=" * 80)
    
    # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹åœ¨ä¸åŒæ¨¡å¼ä¸‹çš„æ’åå˜åŒ–
    model_rankings = {}
    
    for mode, result in all_results.items():
        if 'rankings' in result:
            for i, ranking in enumerate(result['rankings']):
                model_name = ranking['model']
                if model_name not in model_rankings:
                    model_rankings[model_name] = {}
                model_rankings[model_name][mode] = {
                    'rank': i + 1,
                    'score': ranking['score'],
                    'p2l_coef': ranking['p2l_coefficient']
                }
    
    print("ğŸ† å„æ¨¡å‹åœ¨ä¸åŒæ¨¡å¼ä¸‹çš„æ’å:")
    print(f"{'æ¨¡å‹':<30} {'æ€§èƒ½æ¨¡å¼':<12} {'æˆæœ¬æ¨¡å¼':<12} {'é€Ÿåº¦æ¨¡å¼':<12} {'å¹³è¡¡æ¨¡å¼':<12}")
    print("-" * 90)
    
    for model_name, mode_data in model_rankings.items():
        row = f"{model_name[:28]:<30}"
        for mode in modes:
            if mode in mode_data:
                rank = mode_data[mode]['rank']
                score = mode_data[mode]['score']
                row += f" #{rank}({score:.3f})"
                row += " " * (12 - len(f"#{rank}({score:.3f})"))
            else:
                row += " N/A        "
        print(row)
    
    # æ£€æŸ¥è¯„åˆ†å˜åŒ–ç¨‹åº¦
    print(f"\nğŸ“ˆ è¯„åˆ†å˜åŒ–åˆ†æ:")
    
    for model_name, mode_data in model_rankings.items():
        scores = [mode_data[mode]['score'] for mode in modes if mode in mode_data]
        if len(scores) >= 2:
            score_range = max(scores) - min(scores)
            score_std = np.std(scores)
            print(f"  {model_name[:30]}: è¯„åˆ†èŒƒå›´={score_range:.4f}, æ ‡å‡†å·®={score_std:.4f}")
    
    # æ£€æŸ¥æ¨èæ¨¡å‹çš„å¤šæ ·æ€§
    recommended_models = []
    for mode, result in all_results.items():
        if 'rankings' in result and result['rankings']:
            recommended_models.append((mode, result['rankings'][0]['model']))
    
    print(f"\nğŸ¯ æ¨èæ¨¡å‹å¤šæ ·æ€§:")
    unique_recommendations = set([model for _, model in recommended_models])
    
    for mode, model in recommended_models:
        print(f"  {mode:12}: {model}")
    
    print(f"\nğŸ“Š æ€»ç»“:")
    print(f"  æµ‹è¯•æ¨¡å¼æ•°: {len(modes)}")
    print(f"  æˆåŠŸæµ‹è¯•æ•°: {len([r for r in all_results.values() if 'rankings' in r])}")
    print(f"  æ¨èæ¨¡å‹ç§ç±»: {len(unique_recommendations)}")
    
    if len(unique_recommendations) == 1:
        print(f"  âš ï¸  æ‰€æœ‰æ¨¡å¼éƒ½æ¨èåŒä¸€æ¨¡å‹: {list(unique_recommendations)[0]}")
        print(f"  ğŸ’¡ å»ºè®®: å¢åŠ æƒé‡å·®å¼‚æˆ–è°ƒæ•´è¯„åˆ†ç®—æ³•")
    elif len(unique_recommendations) == len(modes):
        print(f"  âœ… ç†æƒ³æƒ…å†µ: æ¯ä¸ªæ¨¡å¼æ¨èä¸åŒæ¨¡å‹")
    else:
        print(f"  ğŸ”„ éƒ¨åˆ†å·®å¼‚: {len(unique_recommendations)}/{len(modes)} ä¸ªä¸åŒæ¨è")
    
    return all_results

def test_weight_sensitivity():
    """æµ‹è¯•æƒé‡æ•æ„Ÿæ€§"""
    print("\nğŸ§ª æµ‹è¯•æƒé‡æ•æ„Ÿæ€§")
    print("=" * 80)
    
    # æ¨¡æ‹Ÿä¸åŒçš„æ¨¡å‹æ•°æ®
    model_list = ["model_a", "model_b", "model_c"]
    p2l_coefficients = np.array([0.8, 0.5, 0.3])  # Aæœ€é«˜P2L
    
    model_configs = {
        "model_a": {"cost_per_1k": 0.03, "avg_response_time": 3.0},  # é«˜æˆæœ¬æ…¢é€Ÿ
        "model_b": {"cost_per_1k": 0.015, "avg_response_time": 2.0}, # ä¸­æˆæœ¬ä¸­é€Ÿ
        "model_c": {"cost_per_1k": 0.001, "avg_response_time": 1.0}  # ä½æˆæœ¬å¿«é€Ÿ
    }
    
    from p2l_router import P2LRouter
    router = P2LRouter()
    
    modes = ['performance', 'cost', 'speed', 'balanced']
    
    print("ğŸ” æƒé‡æ•æ„Ÿæ€§æµ‹è¯•:")
    print(f"{'æ¨¡å¼':<12} {'Model A':<15} {'Model B':<15} {'Model C':<15} {'æ¨è':<10}")
    print("-" * 70)
    
    for mode in modes:
        adjusted_scores = router._calculate_mode_adjusted_scores(
            p2l_coefficients, model_list, model_configs, mode
        )
        
        recommended_idx = np.argmax(adjusted_scores)
        recommended_model = model_list[recommended_idx]
        
        row = f"{mode:<12}"
        for i, score in enumerate(adjusted_scores):
            row += f" {score:.4f}        "
        row += f" {recommended_model}"
        
        print(row)
    
    print("\nğŸ’¡ æƒé‡æ•æ„Ÿæ€§åˆ†æ:")
    print("  - performanceæ¨¡å¼åº”è¯¥æ¨èModel A (æœ€é«˜P2L)")
    print("  - costæ¨¡å¼åº”è¯¥æ¨èModel C (æœ€ä½æˆæœ¬)")
    print("  - speedæ¨¡å¼åº”è¯¥æ¨èModel C (æœ€å¿«å“åº”)")
    print("  - balancedæ¨¡å¼åº”è¯¥ç»¼åˆè€ƒè™‘æ‰€æœ‰å› ç´ ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¼˜å…ˆæ¨¡å¼è¯„åˆ†å·®å¼‚æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•å®é™…è¯„åˆ†å·®å¼‚
    results = test_mode_scoring_differences()
    
    # æµ‹è¯•æƒé‡æ•æ„Ÿæ€§
    test_weight_sensitivity()
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ æµ‹è¯•å®Œæˆ")
    print("=" * 80)
    print("âœ… ä¼˜å…ˆæ¨¡å¼è¯„åˆ†å·®å¼‚æµ‹è¯•å®Œæˆ")
    print("ğŸ’¡ å¦‚æœä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œå¯èƒ½éœ€è¦:")
    print("   1. è°ƒæ•´æƒé‡è®¾ç½®ï¼Œå¢åŠ å·®å¼‚")
    print("   2. æ£€æŸ¥P2Lç³»æ•°çš„åˆ†å¸ƒèŒƒå›´")
    print("   3. ä¼˜åŒ–æ ‡å‡†åŒ–ç®—æ³•")

if __name__ == "__main__":
    main()